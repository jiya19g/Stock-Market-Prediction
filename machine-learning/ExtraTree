{"cells":[{"cell_type":"markdown","source":["## Initial Code"],"metadata":{"id":"5ebGneEwoKEe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WY7I5C1zqFJt"},"outputs":[],"source":["# Importing necessary libraries for data analysis and manipulation\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","# For handling warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23123,"status":"ok","timestamp":1734352249764,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"3RKq8vfwqVHB","outputId":"85e5beef-e692-48be-9f80-8336ef7801a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4yeLMDyqd2o"},"outputs":[],"source":["df_aapl = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stocks/AAPL.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHvcgRGPruCy"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import boxcox\n","\n","df_aapl['Close_log'] = np.log(df_aapl['Close'] + 1)\n","df_aapl['Close_sqrt'] = np.sqrt(df_aapl['Close'])\n","df_aapl['Close_boxcox'], _ = boxcox(df_aapl['Close'] + 1)\n"]},{"cell_type":"markdown","metadata":{"id":"lLz5cmQlryah"},"source":["This code calculates the skewness of the 'Close' column in the `df_aapl` DataFrame before and after applying various transformations:\n","\n","1. **Original Skewness**: Calculates the skewness of the original 'Close' data.\n","2. **Log Transformation Skewness**: Calculates the skewness of the 'Close_log' column after applying the log transformation.\n","3. **Square Root Transformation Skewness**: Calculates the skewness of the 'Close_sqrt' column after applying the square root transformation.\n","4. **Box-Cox Transformation Skewness**: Calculates the skewness of the 'Close_boxcox' column after applying the Box-Cox transformation.\n","\n","The printed results help assess how each transformation affects the distribution's symmetry and the success of skewness correction.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1734352251454,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"DIPGiQydr2K0","outputId":"4eb19f53-d9ef-4cad-8236-6793597b297e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Skewness: 2.5045276102319933\n","Log Transformation Skewness: 0.8535555176510303\n","Square Root Transformation Skewness: 1.6211545809555206\n","Box-Cox Transformation Skewness: 0.43527466713563334\n"]}],"source":["\n","skew_original = df_aapl['Close'].skew()\n","skew_log = df_aapl['Close_log'].skew()\n","skew_sqrt = df_aapl['Close_sqrt'].skew()\n","skew_boxcox = pd.Series(df_aapl['Close_boxcox']).skew()\n","\n","print(f\"Original Skewness: {skew_original}\")\n","print(f\"Log Transformation Skewness: {skew_log}\")\n","print(f\"Square Root Transformation Skewness: {skew_sqrt}\")\n","print(f\"Box-Cox Transformation Skewness: {skew_boxcox}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"De4W27wEr9-p"},"outputs":[],"source":["\n","df_aapl['Open_log'] = np.log(df_aapl['Open'])\n","df_aapl['High_log'] = np.log(df_aapl['High'])\n","df_aapl['Low_log'] = np.log(df_aapl['Low'])\n","df_aapl['Adj Close_log'] = np.log(df_aapl['Adj Close'])\n","df_aapl['Volume_log'] = np.log(df_aapl['Volume'])\n","\n","\n","df_aapl['Open_sqrt'] = np.sqrt(df_aapl['Open'])\n","df_aapl['High_sqrt'] = np.sqrt(df_aapl['High'])\n","df_aapl['Low_sqrt'] = np.sqrt(df_aapl['Low'])\n","df_aapl['Adj Close_sqrt'] = np.sqrt(df_aapl['Adj Close'])\n","df_aapl['Volume_sqrt'] = np.sqrt(df_aapl['Volume'])\n","\n","from scipy.stats import boxcox\n","df_aapl['Open_boxcox'], _ = boxcox(df_aapl['Open'])\n","df_aapl['High_boxcox'], _ = boxcox(df_aapl['High'])\n","df_aapl['Low_boxcox'], _ = boxcox(df_aapl['Low'])\n","df_aapl['Adj Close_boxcox'], _ = boxcox(df_aapl['Adj Close'])"]},{"cell_type":"markdown","source":["This helps compare how the transformations reduce skewness in the data, aiming for a more normal distribution."],"metadata":{"id":"2XrZQHaDAigS"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1734352251454,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"of1KONYmsC8t","outputId":"cc5bf130-b967-4196-d58f-a649df7f5eb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness Before Transformation:\n"," Open         2.504632\n","High         2.502208\n","Low          2.506714\n","Adj Close    2.550677\n","Volume       3.565699\n","dtype: float64\n","\n","Skewness After Transformation:\n"," Open_log            0.482872\n","High_log            0.481997\n","Low_log             0.484246\n","Adj Close_log       0.494009\n","Open_sqrt           1.620771\n","High_sqrt           1.621456\n","Low_sqrt            1.620661\n","Adj Close_sqrt      1.679402\n","Volume_sqrt         1.299776\n","Open_boxcox         0.181226\n","High_boxcox         0.179749\n","Low_boxcox          0.182882\n","Adj Close_boxcox    0.180085\n","dtype: float64\n"]}],"source":["\n","skewness_before = df_aapl[['Open', 'High', 'Low', 'Adj Close', 'Volume']].skew()\n","skewness_after = df_aapl[['Open_log', 'High_log', 'Low_log', 'Adj Close_log',\n","                          'Open_sqrt', 'High_sqrt', 'Low_sqrt', 'Adj Close_sqrt', 'Volume_sqrt',\n","                          'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox']].skew()\n","\n","print(\"Skewness Before Transformation:\\n\", skewness_before)\n","print(\"\\nSkewness After Transformation:\\n\", skewness_after)\n"]},{"cell_type":"markdown","source":["- Applied Box-Cox transformation to the 'Open', 'High', 'Low', 'Adj Close', and 'Close' columns.\n","- Recalculated skewness after the transformation to reduce skew and normalize the data for modeling."],"metadata":{"id":"zfEokf4iAmnv"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1734352251454,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"s9oEP05csI66","outputId":"ee206e51-f57f-437c-ede5-44c57d2cb993"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness After Box-Cox Transformation:\n","Open_boxcox         0.435237\n","High_boxcox         0.433381\n","Low_boxcox          0.437331\n","Adj Close_boxcox    0.458762\n","Close_boxcox        0.435275\n","dtype: float64\n"]}],"source":["from scipy import stats\n","\n","df_aapl['Open_boxcox'], _ = stats.boxcox(df_aapl['Open'] + 1)\n","df_aapl['High_boxcox'], _ = stats.boxcox(df_aapl['High'] + 1)\n","df_aapl['Low_boxcox'], _ = stats.boxcox(df_aapl['Low'] + 1)\n","df_aapl['Adj Close_boxcox'], _ = stats.boxcox(df_aapl['Adj Close'] + 1)\n","df_aapl['Close_boxcox'], _ = stats.boxcox(df_aapl['Close'] + 1)\n","\n","skewness_after_boxcox = df_aapl[['Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox', 'Close_boxcox']].skew()\n","\n","print(\"Skewness After Box-Cox Transformation:\")\n","print(skewness_after_boxcox)\n"]},{"cell_type":"markdown","source":["Feature Selection"],"metadata":{"id":"uvZe7IzRAwHu"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1734352251454,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"aczNHUI4rk8x","outputId":"ada290a4-fcf6-4e8f-b0a7-eaf389f55518"},"outputs":[{"output_type":"stream","name":"stdout","text":["         Date      Open      High       Low  Adj Close     Close     Volume  \\\n","0  1980-12-12  0.128348  0.128906  0.128348   0.098943  0.128348  469033600   \n","1  1980-12-15  0.122210  0.122210  0.121652   0.093781  0.121652  175884800   \n","2  1980-12-16  0.113281  0.113281  0.112723   0.086898  0.112723  105728000   \n","3  1980-12-17  0.115513  0.116071  0.115513   0.089049  0.115513   86441600   \n","4  1980-12-18  0.118862  0.119420  0.118862   0.091630  0.118862   73449600   \n","\n","   Open_boxcox  High_boxcox  Low_boxcox  Adj Close_boxcox  Close_boxcox  \n","0     0.117689     0.118173    0.117674          0.092374      0.117689  \n","1     0.112503     0.112516    0.112016          0.087857      0.112030  \n","2     0.104886     0.104897    0.104395          0.081785      0.104407  \n","3     0.106798     0.107287    0.106786          0.083688      0.106798  \n","4     0.109657     0.110145    0.109644          0.085966      0.109657  \n"]}],"source":["\n","df_aapl_cleaned = df_aapl[['Date', 'Open', 'High', 'Low', 'Adj Close', 'Close', 'Volume',\n","                           'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox',\n","                           'Close_boxcox']]\n","\n","print(df_aapl_cleaned.head())\n"]},{"cell_type":"markdown","source":["### Train Validation Test Split\n","\n","The code splits the data into training, validation, and test sets. The features `X` and target `Y` are split as follows:\n","\n","- 70% for training (`X_train`, `Y_train`)\n","- 15% for validation (`X_val`, `Y_val`)\n","- 15% for testing (`X_test`, `Y_test`)\n","\n","The split is done using a 30% test size, followed by splitting the remaining 70% into validation and test sets without shuffling (time series data)."],"metadata":{"id":"chw5ijVT_JRM"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X = df_aapl_cleaned[['Open_boxcox', 'High_boxcox', 'Low_boxcox']]\n","Y = df_aapl_cleaned['Close_boxcox']\n","\n","X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, shuffle=False)\n","X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, shuffle=False)\n","\n","print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSztwxnoy8-U","executionInfo":{"status":"ok","timestamp":1734352251455,"user_tz":-330,"elapsed":6,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"50860d60-c789-46ef-8463-a640833a55aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set: (7736, 3), Validation set: (1658, 3), Test set: (1658, 3)\n"]}]},{"cell_type":"code","source":["!pip install ConfigSpace\n","!pip install hpbandster"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zR-2_QoaXW7p","executionInfo":{"status":"ok","timestamp":1734352275114,"user_tz":-330,"elapsed":23664,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"74d34bb4-2942-4316-a7d4-117822f96f7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (1.26.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (3.2.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (1.13.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (10.5.0)\n","Building wheels for collected packages: ConfigSpace\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=ConfigSpace-1.2.1-py3-none-any.whl size=115941 sha256=4167f65464e4e64e1c17a31c5c44db2c72dd36f47ea9fed5fcf71c35c82453a7\n","  Stored in directory: /root/.cache/pip/wheels/75/e4/b7/23c23eb4a1c3b1adfeb8bd11366f48c805cbf3ba347237fea6\n","Successfully built ConfigSpace\n","Installing collected packages: ConfigSpace\n","Successfully installed ConfigSpace-1.2.1\n","Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: ConfigSpace in /usr/local/lib/python3.10/dist-packages (from hpbandster) (1.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from hpbandster) (1.13.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from ConfigSpace->hpbandster) (3.2.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from ConfigSpace->hpbandster) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.10/dist-packages (from ConfigSpace->hpbandster) (10.5.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=91152d9a2977919bfa0131df3d47f4aa5dd4b8e25f330b62c83714edc1ba60cb\n","  Stored in directory: /root/.cache/pip/wheels/79/51/18/33d6ba8c55cc8401bffbccb1b87b21e0c68f40edc4ce3c1f99\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp310-cp310-linux_x86_64.whl size=35008 sha256=46db46817d73aefa84c87f1a559b4ad82ad3b0d05a66f7b16f5284add8ebb0fe\n","  Stored in directory: /root/.cache/pip/wheels/48/65/b3/4c4cc6038b81ff21cc9df69f2b6774f5f52e23d3c275ed15aa\n","Successfully built hpbandster netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, hpbandster\n","Successfully installed Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"markdown","source":["##Extra Tree Regressor"],"metadata":{"id":"ZM1U_xwzcy1t"}},{"cell_type":"markdown","source":["###**Initial**"],"metadata":{"id":"aoU7aW1Qc3GK"}},{"cell_type":"code","source":["from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","\n","# Initialize the model\n","model = ExtraTreesRegressor(random_state=42)\n","\n","# Fit the model on training data\n","model.fit(X_train, Y_train)\n","\n","# Predictions on train, validate, and test sets\n","y_train_pred = model.predict(X_train)\n","y_val_pred = model.predict(X_val)\n","y_test_pred = model.predict(X_test)\n","\n","# Calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Print metrics for train, validate, and test sets\n","train_metrics = calculate_metrics(Y_train, y_train_pred)\n","val_metrics = calculate_metrics(Y_val, y_val_pred)\n","test_metrics = calculate_metrics(Y_test, y_test_pred)\n","\n","print(\"Train Metrics: MAE = {:.4f}, MSE = {:.4f}, RMSE = {:.4f}, R² = {:.4f}, MAPE = {:.4f}%\".format(*train_metrics))\n","print(\"Validation Metrics: MAE = {:.4f}, MSE = {:.4f}, RMSE = {:.4f}, R² = {:.4f}, MAPE = {:.4f}%\".format(*val_metrics))\n","print(\"Test Metrics: MAE = {:.4f}, MSE = {:.4f}, RMSE = {:.4f}, R² = {:.4f}, MAPE = {:.4f}%\".format(*test_metrics))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8_hV08wc5tt","executionInfo":{"status":"ok","timestamp":1734352338316,"user_tz":-330,"elapsed":4785,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"0e586994-7334-443c-b6f2-9ce75c24fffe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Metrics: MAE = 0.0001, MSE = 0.0000, RMSE = 0.0003, R² = 1.0000, MAPE = 0.0253%\n","Validation Metrics: MAE = 0.1356, MSE = 0.0240, RMSE = 0.1550, R² = -3.0870, MAPE = 7.5865%\n","Test Metrics: MAE = 0.4038, MSE = 0.1691, RMSE = 0.4112, R² = -27.0213, MAPE = 19.9132%\n"]}]},{"cell_type":"markdown","source":["###**GridSearchCV**"],"metadata":{"id":"aTcG8kCXeaVx"}},{"cell_type":"code","source":["from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","\n","# Define the parameter grid for tuning\n","param_grid = {\n","    'n_estimators': [100, 200, 300],  # Number of trees\n","    'max_depth': [None, 10, 20],       # Maximum depth of the trees\n","    'min_samples_split': [2, 5, 10],   # Minimum samples required to split an internal node\n","    'min_samples_leaf': [1, 2, 4],     # Minimum number of samples required to be at a leaf node\n","    'max_features': ['auto', 'sqrt', 'log2'],  # The number of features to consider for the best split\n","    'bootstrap': [True, False]         # Whether bootstrap samples are used when building trees\n","}\n","\n","# Initialize the model\n","model = ExtraTreesRegressor(random_state=42)\n","\n","# Setup GridSearchCV\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n","\n","# Fit GridSearchCV to the training data\n","grid_search.fit(X_train, Y_train)\n","\n","# Get best parameters\n","best_params = grid_search.best_params_\n","print(\"Best Hyperparameters:\", best_params)\n","\n","# Fit model with best parameters\n","best_model = grid_search.best_estimator_\n","\n","# Predictions on train, validate, and test sets\n","y_train_pred = best_model.predict(X_train)\n","y_val_pred = best_model.predict(X_val)\n","y_test_pred = best_model.predict(X_test)\n","\n","# Calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Print metrics for train, validate, and test sets\n","train_metrics = calculate_metrics(Y_train, y_train_pred)\n","val_metrics = calculate_metrics(Y_val, y_val_pred)\n","test_metrics = calculate_metrics(Y_test, y_test_pred)\n","\n","print(\"Train Metrics: MAE = {:.4f}, MSE = {:.4f}, RMSE = {:.4f}, R² = {:.4f}, MAPE = {:.4f}%\".format(*train_metrics))\n","print(\"Validation Metrics: MAE = {:.4f}, MSE = {:.4f}, RMSE = {:.4f}, R² = {:.4f}, MAPE = {:.4f}%\".format(*val_metrics))\n","print(\"Test Metrics: MAE = {:.4f}, MSE = {:.4f}, RMSE = {:.4f}, R² = {:.4f}, MAPE = {:.4f}%\".format(*test_metrics))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vg6xmWG8c6hr","executionInfo":{"status":"ok","timestamp":1734353296494,"user_tz":-330,"elapsed":685193,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"837d8095-4014-4516-965e-7af4e789a628"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n","Best Hyperparameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n","Train Metrics: MAE = 0.0017, MSE = 0.0000, RMSE = 0.0025, R² = 1.0000, MAPE = 0.4979%\n","Validation Metrics: MAE = 0.1394, MSE = 0.0251, RMSE = 0.1585, R² = -3.2736, MAPE = 7.8032%\n","Test Metrics: MAE = 0.4078, MSE = 0.1724, RMSE = 0.4152, R² = -27.5586, MAPE = 20.1117%\n"]}]},{"cell_type":"markdown","source":["###**Hyperparameter tuning using TimeSplit**"],"metadata":{"id":"l1UvLmGUedew"}},{"cell_type":"code","source":["from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# Create lag features for stock data\n","def create_lag_features(df, lags=5):\n","    for lag in range(1, lags+1):\n","        df[f'Close_lag_{lag}'] = df['Close_boxcox'].shift(lag)\n","    df = df.dropna()  # Drop missing values created by lagging\n","    return df\n","\n","df_aapl_lagged = create_lag_features(df_aapl_cleaned)\n","\n","# Redefine X and Y with lagged features\n","X = df_aapl_lagged[['Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Close_lag_1', 'Close_lag_2', 'Close_lag_3', 'Close_lag_4', 'Close_lag_5']]\n","Y = df_aapl_lagged['Close_boxcox']\n","\n","# Create a new dataset split for model training (keeping original split intact)\n","X_train_new, X_temp_new, Y_train_new, Y_temp_new = train_test_split(X, Y, test_size=0.3, shuffle=False)\n","X_val_new, X_test_new, Y_val_new, Y_test_new = train_test_split(X_temp_new, Y_temp_new, test_size=0.5, shuffle=False)\n","\n","# Initialize the model\n","model = ExtraTreesRegressor(random_state=42)\n","\n","# Fit the model on training data\n","model.fit(X_train_new, Y_train_new)\n","\n","# Predictions on train, validate, and test sets\n","y_train_pred_new = model.predict(X_train_new)\n","y_val_pred_new = model.predict(X_val_new)\n","y_test_pred_new = model.predict(X_test_new)\n","\n","# Calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Print metrics for train, validate, and test sets\n","train_metrics = calculate_metrics(Y_train_new, y_train_pred_new)\n","val_metrics = calculate_metrics(Y_val_new, y_val_pred_new)\n","test_metrics = calculate_metrics(Y_test_new, y_test_pred_new)\n","\n","print(\"Train Metrics: MAE = {:.4f}, MSE = {:.4f}, RMSE = {:.4f}, R² = {:.4f}, MAPE = {:.4f}%\".format(*train_metrics))\n","print(\"Validation Metrics: MAE = {:.4f}, MSE = {:.4f}, RMSE = {:.4f}, R² = {:.4f}, MAPE = {:.4f}%\".format(*val_metrics))\n","print(\"Test Metrics: MAE = {:.4f}, MSE = {:.4f}, RMSE = {:.4f}, R² = {:.4f}, MAPE = {:.4f}%\".format(*test_metrics))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60uNyBUIeKUG","executionInfo":{"status":"ok","timestamp":1734353314356,"user_tz":-330,"elapsed":5288,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"33792a3e-c057-4edb-e29d-78496404279a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Metrics: MAE = 0.0000, MSE = 0.0000, RMSE = 0.0000, R² = 1.0000, MAPE = 0.0000%\n","Validation Metrics: MAE = 0.1369, MSE = 0.0244, RMSE = 0.1563, R² = -3.1602, MAPE = 7.6643%\n","Test Metrics: MAE = 0.4052, MSE = 0.1702, RMSE = 0.4126, R² = -27.2030, MAPE = 19.9806%\n"]}]},{"cell_type":"code","source":["# Drop the lag features from the dataframe\n","df_aapl_cleaned.drop(columns=['Close_lag_1', 'Close_lag_2', 'Close_lag_3', 'Close_lag_4', 'Close_lag_5'], inplace=True)\n","\n","# Redefine X and Y without the lag features\n","X = df_aapl_cleaned[['Open_boxcox', 'High_boxcox', 'Low_boxcox']]\n","Y = df_aapl_cleaned['Close_boxcox']\n","\n","# Create a new dataset split without lag features\n","X_train_new, X_temp_new, Y_train_new, Y_temp_new = train_test_split(X, Y, test_size=0.3, shuffle=False)\n","X_val_new, X_test_new, Y_val_new, Y_test_new = train_test_split(X_temp_new, Y_temp_new, test_size=0.5, shuffle=False)\n"],"metadata":{"id":"KRPEzDZ2i6cm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**CrossValidation**"],"metadata":{"id":"4UfWkq2Vg-OZ"}},{"cell_type":"code","source":["from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","\n","# Initialize the model\n","model = ExtraTreesRegressor(random_state=42)\n","\n","# Set up KFold for cross-validation (no shuffle, sequential split)\n","kf = KFold(n_splits=5, shuffle=False)\n","\n","# Calculate the metrics for each fold\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Store results for all folds\n","train_metrics = []\n","val_metrics = []\n","test_metrics = []\n","\n","# Loop over each fold to get train, validation, and test predictions\n","for train_idx, val_test_idx in kf.split(X_train):\n","    X_train_fold, X_val_test_fold = X_train.iloc[train_idx], X_train.iloc[val_test_idx]\n","    Y_train_fold, Y_val_test_fold = Y_train.iloc[train_idx], Y_train.iloc[val_test_idx]\n","\n","    # Split validation/test set further\n","    X_val_fold, X_test_fold, Y_val_fold, Y_test_fold = train_test_split(X_val_test_fold, Y_val_test_fold, test_size=0.5, shuffle=False)\n","\n","    # Train the model\n","    model.fit(X_train_fold, Y_train_fold)\n","\n","    # Predictions\n","    y_train_pred = model.predict(X_train_fold)\n","    y_val_pred = model.predict(X_val_fold)\n","    y_test_pred = model.predict(X_test_fold)\n","\n","    # Calculate metrics\n","    train_metrics.append(calculate_metrics(Y_train_fold, y_train_pred))\n","    val_metrics.append(calculate_metrics(Y_val_fold, y_val_pred))\n","    test_metrics.append(calculate_metrics(Y_test_fold, y_test_pred))\n","\n","# Calculate average metrics across all folds\n","avg_train_metrics = np.mean(train_metrics, axis=0)\n","avg_val_metrics = np.mean(val_metrics, axis=0)\n","avg_test_metrics = np.mean(test_metrics, axis=0)\n","\n","# Print all metrics for each fold\n","for i, (train_metric, val_metric, test_metric) in enumerate(zip(train_metrics, val_metrics, test_metrics)):\n","    print(f\"Fold {i + 1}:\")\n","    print(f\"  Train Metrics: MAE = {train_metric[0]:.4f}, MSE = {train_metric[1]:.4f}, RMSE = {train_metric[2]:.4f}, R² = {train_metric[3]:.4f}, MAPE = {train_metric[4]:.4f}%\")\n","    print(f\"  Validation Metrics: MAE = {val_metric[0]:.4f}, MSE = {val_metric[1]:.4f}, RMSE = {val_metric[2]:.4f}, R² = {val_metric[3]:.4f}, MAPE = {val_metric[4]:.4f}%\")\n","    print(f\"  Test Metrics: MAE = {test_metric[0]:.4f}, MSE = {test_metric[1]:.4f}, RMSE = {test_metric[2]:.4f}, R² = {test_metric[3]:.4f}, MAPE = {test_metric[4]:.4f}%\")\n","    print()\n","\n","# Print average metrics across all folds\n","print(\"Average Metrics Across All Folds:\")\n","print(f\"  Avg Train Metrics: MAE = {avg_train_metrics[0]:.4f}, MSE = {avg_train_metrics[1]:.4f}, RMSE = {avg_train_metrics[2]:.4f}, R² = {avg_train_metrics[3]:.4f}, MAPE = {avg_train_metrics[4]:.4f}%\")\n","print(f\"  Avg Validation Metrics: MAE = {avg_val_metrics[0]:.4f}, MSE = {avg_val_metrics[1]:.4f}, RMSE = {avg_val_metrics[2]:.4f}, R² = {avg_val_metrics[3]:.4f}, MAPE = {avg_val_metrics[4]:.4f}%\")\n","print(f\"  Avg Test Metrics: MAE = {avg_test_metrics[0]:.4f}, MSE = {avg_test_metrics[1]:.4f}, RMSE = {avg_test_metrics[2]:.4f}, R² = {avg_test_metrics[3]:.4f}, MAPE = {avg_test_metrics[4]:.4f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHkeUbN2g0q7","executionInfo":{"status":"ok","timestamp":1734353956979,"user_tz":-330,"elapsed":11907,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"103c1d43-609e-46fb-e756-909a06bbf8b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1:\n","  Train Metrics: MAE = 0.0000, MSE = 0.0000, RMSE = 0.0000, R² = 1.0000, MAPE = 0.0000%\n","  Validation Metrics: MAE = 0.0149, MSE = 0.0005, RMSE = 0.0227, R² = 0.6627, MAPE = 20.6845%\n","  Test Metrics: MAE = 0.0096, MSE = 0.0003, RMSE = 0.0164, R² = 0.9612, MAPE = 11.6819%\n","\n","Fold 2:\n","  Train Metrics: MAE = 0.0000, MSE = 0.0000, RMSE = 0.0000, R² = 1.0000, MAPE = 0.0000%\n","  Validation Metrics: MAE = 0.0020, MSE = 0.0000, RMSE = 0.0026, R² = 0.9956, MAPE = 0.6693%\n","  Test Metrics: MAE = 0.0021, MSE = 0.0000, RMSE = 0.0027, R² = 0.9983, MAPE = 0.7156%\n","\n","Fold 3:\n","  Train Metrics: MAE = 0.0000, MSE = 0.0000, RMSE = 0.0000, R² = 1.0000, MAPE = 0.0000%\n","  Validation Metrics: MAE = 0.0017, MSE = 0.0000, RMSE = 0.0024, R² = 0.9985, MAPE = 0.8757%\n","  Test Metrics: MAE = 0.0056, MSE = 0.0001, RMSE = 0.0096, R² = 0.9951, MAPE = 1.3998%\n","\n","Fold 4:\n","  Train Metrics: MAE = 0.0000, MSE = 0.0000, RMSE = 0.0000, R² = 1.0000, MAPE = 0.0000%\n","  Validation Metrics: MAE = 0.0038, MSE = 0.0000, RMSE = 0.0068, R² = 0.9983, MAPE = 0.8449%\n","  Test Metrics: MAE = 0.0083, MSE = 0.0001, RMSE = 0.0119, R² = 0.9954, MAPE = 0.8374%\n","\n","Fold 5:\n","  Train Metrics: MAE = 0.0000, MSE = 0.0000, RMSE = 0.0000, R² = 1.0000, MAPE = 0.0000%\n","  Validation Metrics: MAE = 0.1273, MSE = 0.0282, RMSE = 0.1678, R² = 0.0308, MAPE = 7.9745%\n","  Test Metrics: MAE = 0.3477, MSE = 0.1246, RMSE = 0.3530, R² = -33.0563, MAPE = 20.0158%\n","\n","Average Metrics Across All Folds:\n","  Avg Train Metrics: MAE = 0.0000, MSE = 0.0000, RMSE = 0.0000, R² = 1.0000, MAPE = 0.0000%\n","  Avg Validation Metrics: MAE = 0.0299, MSE = 0.0057, RMSE = 0.0405, R² = 0.7372, MAPE = 6.2098%\n","  Avg Test Metrics: MAE = 0.0747, MSE = 0.0250, RMSE = 0.0787, R² = -5.8213, MAPE = 6.9301%\n"]}]},{"cell_type":"markdown","source":["###**OPTUNA**"],"metadata":{"id":"K0DSHNFSFAMr"}},{"cell_type":"code","source":["import numpy as np\n","import optuna\n","from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","from sklearn.model_selection import train_test_split\n","\n","# Remove specific unwanted lag features from the DataFrame\n","unwanted_features = ['Close_lag_1', 'Close_lag_2', 'Close_lag_3', 'Close_lag_4', 'Close_lag_5']\n","X_train_cleaned = X_train.drop(columns=unwanted_features, errors='ignore')\n","X_val_cleaned = X_val.drop(columns=unwanted_features, errors='ignore')\n","X_test_cleaned = X_test.drop(columns=unwanted_features, errors='ignore')\n","\n","# Define objective function for Optuna\n","def objective(trial):\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n","        'max_features': trial.suggest_float('max_features', 0.5, 1.0),\n","        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n","        'min_samples_leaf': trial.suggest_float('min_samples_leaf', 0.01, 0.1)\n","    }\n","\n","    model = ExtraTreesRegressor(\n","        n_estimators=params['n_estimators'],\n","        max_features=params['max_features'],\n","        min_samples_split=params['min_samples_split'],\n","        min_samples_leaf=params['min_samples_leaf'],\n","        random_state=42\n","    )\n","\n","    model.fit(X_train_cleaned, Y_train)\n","    Y_val_pred = model.predict(X_val_cleaned)\n","    mae = mean_absolute_error(Y_val, Y_val_pred)\n","    return mae\n","\n","# Run Optuna study\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=50)\n","\n","# Retrieve best hyperparameters\n","best_params = study.best_params\n","\n","# Build the ExtraTreesRegressor model with the best hyperparameters\n","best_model = ExtraTreesRegressor(\n","    n_estimators=best_params[\"n_estimators\"],\n","    max_features=best_params[\"max_features\"],\n","    min_samples_split=best_params[\"min_samples_split\"],\n","    min_samples_leaf=best_params[\"min_samples_leaf\"],\n","    random_state=42\n",")\n","\n","# Fit the best model\n","best_model.fit(X_train_cleaned, Y_train)\n","\n","# Predict and evaluate\n","Y_train_pred = best_model.predict(X_train_cleaned)\n","Y_val_pred = best_model.predict(X_val_cleaned)\n","Y_test_pred = best_model.predict(X_test_cleaned)\n","\n","# Performance metrics calculation\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred)\n","    return mae, mse, rmse, r2, mape\n","\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print the results\n","print(\"Best Parameters Found by Optuna:\")\n","print(best_params)\n","\n","print(\"\\nTraining set metrics:\")\n","print(f\"MAE: {train_metrics[0]}, MSE: {train_metrics[1]}, RMSE: {train_metrics[2]}, R²: {train_metrics[3]}, MAPE: {train_metrics[4]}\")\n","\n","print(\"\\nValidation set metrics:\")\n","print(f\"MAE: {val_metrics[0]}, MSE: {val_metrics[1]}, RMSE: {val_metrics[2]}, R²: {val_metrics[3]}, MAPE: {val_metrics[4]}\")\n","\n","print(\"\\nTest set metrics:\")\n","print(f\"MAE: {test_metrics[0]}, MSE: {test_metrics[1]}, RMSE: {test_metrics[2]}, R²: {test_metrics[3]}, MAPE: {test_metrics[4]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GgznSAvjC-bp","executionInfo":{"status":"ok","timestamp":1751189898586,"user_tz":-330,"elapsed":16498,"user":{"displayName":"ANOUSHKA SHRIVASTAVA (RA2211031010135)","userId":"06391903211264765596"}},"outputId":"b30c4701-dee7-47b2-9323-a8107996a2e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-06-29 09:35:31,281] A new study created in memory with name: no-name-bae6c30b-1042-4faf-89de-77d662e021d5\n","[I 2025-06-29 09:35:31,449] Trial 0 finished with value: 0.38854489569185807 and parameters: {'n_estimators': 144, 'max_features': 0.7217954741706752, 'min_samples_split': 5, 'min_samples_leaf': 0.04897741367197366}. Best is trial 0 with value: 0.38854489569185807.\n","[I 2025-06-29 09:35:31,556] Trial 1 finished with value: 0.48355349427160366 and parameters: {'n_estimators': 101, 'max_features': 0.7909353232224359, 'min_samples_split': 9, 'min_samples_leaf': 0.0792268734983443}. Best is trial 0 with value: 0.38854489569185807.\n","[I 2025-06-29 09:35:31,650] Trial 2 finished with value: 0.7132725462863723 and parameters: {'n_estimators': 99, 'max_features': 0.5191871103891429, 'min_samples_split': 4, 'min_samples_leaf': 0.09793926160062294}. Best is trial 0 with value: 0.38854489569185807.\n","[I 2025-06-29 09:35:31,799] Trial 3 finished with value: 0.5533365025603573 and parameters: {'n_estimators': 151, 'max_features': 0.811639103152529, 'min_samples_split': 8, 'min_samples_leaf': 0.09729125423079774}. Best is trial 0 with value: 0.38854489569185807.\n","[I 2025-06-29 09:35:31,930] Trial 4 finished with value: 0.6231272582583048 and parameters: {'n_estimators': 149, 'max_features': 0.6033161825080486, 'min_samples_split': 10, 'min_samples_leaf': 0.06818596853805915}. Best is trial 0 with value: 0.38854489569185807.\n","[I 2025-06-29 09:35:32,157] Trial 5 finished with value: 0.23165887351125997 and parameters: {'n_estimators': 167, 'max_features': 0.943015117100073, 'min_samples_split': 3, 'min_samples_leaf': 0.019464227998209373}. Best is trial 5 with value: 0.23165887351125997.\n","[I 2025-06-29 09:35:32,284] Trial 6 finished with value: 0.42436665869649876 and parameters: {'n_estimators': 118, 'max_features': 0.6853250086872194, 'min_samples_split': 10, 'min_samples_leaf': 0.06602603419762626}. Best is trial 5 with value: 0.23165887351125997.\n","[I 2025-06-29 09:35:32,408] Trial 7 finished with value: 0.42070184467480776 and parameters: {'n_estimators': 83, 'max_features': 0.7934394306434804, 'min_samples_split': 4, 'min_samples_leaf': 0.05657821610654494}. Best is trial 5 with value: 0.23165887351125997.\n","[I 2025-06-29 09:35:32,536] Trial 8 finished with value: 0.3718186420019454 and parameters: {'n_estimators': 109, 'max_features': 0.8420009536216202, 'min_samples_split': 6, 'min_samples_leaf': 0.045840618563657565}. Best is trial 5 with value: 0.23165887351125997.\n","[I 2025-06-29 09:35:32,668] Trial 9 finished with value: 0.305234394224534 and parameters: {'n_estimators': 99, 'max_features': 0.596405967126237, 'min_samples_split': 10, 'min_samples_leaf': 0.015322723646400548}. Best is trial 5 with value: 0.23165887351125997.\n","[I 2025-06-29 09:35:33,043] Trial 10 finished with value: 0.21138935531100012 and parameters: {'n_estimators': 199, 'max_features': 0.9999467072607462, 'min_samples_split': 2, 'min_samples_leaf': 0.013266302474238538}. Best is trial 10 with value: 0.21138935531100012.\n","[I 2025-06-29 09:35:33,414] Trial 11 finished with value: 0.20112935766386095 and parameters: {'n_estimators': 198, 'max_features': 0.9997541085511649, 'min_samples_split': 2, 'min_samples_leaf': 0.01163378115897045}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:33,691] Trial 12 finished with value: 0.2925970952277764 and parameters: {'n_estimators': 197, 'max_features': 0.9801734741878996, 'min_samples_split': 2, 'min_samples_leaf': 0.0315287952675926}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:33,961] Trial 13 finished with value: 0.28701094252300396 and parameters: {'n_estimators': 194, 'max_features': 0.8982759442517239, 'min_samples_split': 2, 'min_samples_leaf': 0.03029641349694962}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:34,244] Trial 14 finished with value: 0.20387947473277 and parameters: {'n_estimators': 176, 'max_features': 0.893697191352162, 'min_samples_split': 7, 'min_samples_leaf': 0.011360654857780705}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:34,467] Trial 15 finished with value: 0.2919576631747634 and parameters: {'n_estimators': 171, 'max_features': 0.893376174164187, 'min_samples_split': 7, 'min_samples_leaf': 0.03127306388172201}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:34,721] Trial 16 finished with value: 0.2511305046071949 and parameters: {'n_estimators': 177, 'max_features': 0.9029899858831176, 'min_samples_split': 7, 'min_samples_leaf': 0.02468046971631143}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:35,055] Trial 17 finished with value: 0.3255679704644659 and parameters: {'n_estimators': 182, 'max_features': 0.946068091583596, 'min_samples_split': 6, 'min_samples_leaf': 0.03926170046240683}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:35,275] Trial 18 finished with value: 0.20315667046393926 and parameters: {'n_estimators': 135, 'max_features': 0.8518552598148318, 'min_samples_split': 8, 'min_samples_leaf': 0.012243638351925455}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:35,356] Trial 19 finished with value: 0.26739690304553665 and parameters: {'n_estimators': 50, 'max_features': 0.8380373610832416, 'min_samples_split': 8, 'min_samples_leaf': 0.02497999939996407}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:35,533] Trial 20 finished with value: 0.3238150089055732 and parameters: {'n_estimators': 135, 'max_features': 0.732280750877549, 'min_samples_split': 5, 'min_samples_leaf': 0.04043302837403743}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:35,802] Trial 21 finished with value: 0.21320475336325448 and parameters: {'n_estimators': 163, 'max_features': 0.8703373517811028, 'min_samples_split': 8, 'min_samples_leaf': 0.013201125323549884}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:36,100] Trial 22 finished with value: 0.20285277651681544 and parameters: {'n_estimators': 186, 'max_features': 0.9475087993533451, 'min_samples_split': 7, 'min_samples_leaf': 0.01117806558652551}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:36,368] Trial 23 finished with value: 0.237254631550721 and parameters: {'n_estimators': 185, 'max_features': 0.9475373070792036, 'min_samples_split': 9, 'min_samples_leaf': 0.021520321590990252}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:36,562] Trial 24 finished with value: 0.23541220319325246 and parameters: {'n_estimators': 129, 'max_features': 0.9702887939979541, 'min_samples_split': 6, 'min_samples_leaf': 0.019802328611025863}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:36,826] Trial 25 finished with value: 0.20228201433676993 and parameters: {'n_estimators': 154, 'max_features': 0.9291161961343952, 'min_samples_split': 9, 'min_samples_leaf': 0.011208986348888953}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:37,045] Trial 26 finished with value: 0.3254914347732688 and parameters: {'n_estimators': 160, 'max_features': 0.9213082394134213, 'min_samples_split': 9, 'min_samples_leaf': 0.035826641651376834}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:37,300] Trial 27 finished with value: 0.2737964692857587 and parameters: {'n_estimators': 188, 'max_features': 0.9984446654373114, 'min_samples_split': 5, 'min_samples_leaf': 0.028879332982298524}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:37,526] Trial 28 finished with value: 0.2362411305007733 and parameters: {'n_estimators': 155, 'max_features': 0.9537873483959964, 'min_samples_split': 3, 'min_samples_leaf': 0.018693145039579553}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:37,697] Trial 29 finished with value: 0.40336618249210743 and parameters: {'n_estimators': 142, 'max_features': 0.6864481319124995, 'min_samples_split': 4, 'min_samples_leaf': 0.051848481736356915}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:37,943] Trial 30 finished with value: 0.36490711004590337 and parameters: {'n_estimators': 188, 'max_features': 0.770575778088614, 'min_samples_split': 7, 'min_samples_leaf': 0.04543690730887582}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:38,363] Trial 31 finished with value: 0.20820174989826765 and parameters: {'n_estimators': 140, 'max_features': 0.9336757312295418, 'min_samples_split': 8, 'min_samples_leaf': 0.012768843434587212}. Best is trial 11 with value: 0.20112935766386095.\n","[I 2025-06-29 09:35:38,988] Trial 32 finished with value: 0.19835674835518785 and parameters: {'n_estimators': 121, 'max_features': 0.8602957878821686, 'min_samples_split': 9, 'min_samples_leaf': 0.010693073479200508}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:40,045] Trial 33 finished with value: 0.20191943986871747 and parameters: {'n_estimators': 124, 'max_features': 0.8743462482034718, 'min_samples_split': 9, 'min_samples_leaf': 0.010001101653122547}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:40,543] Trial 34 finished with value: 0.49240037097302075 and parameters: {'n_estimators': 120, 'max_features': 0.8684485879619274, 'min_samples_split': 9, 'min_samples_leaf': 0.0860019590478131}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:41,065] Trial 35 finished with value: 0.24941441827859653 and parameters: {'n_estimators': 85, 'max_features': 0.8150861075086868, 'min_samples_split': 9, 'min_samples_leaf': 0.02432139152330136}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:41,792] Trial 36 finished with value: 0.22737307280126004 and parameters: {'n_estimators': 118, 'max_features': 0.912468325343623, 'min_samples_split': 10, 'min_samples_leaf': 0.01736047973464197}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:42,312] Trial 37 finished with value: 0.4280240079132985 and parameters: {'n_estimators': 107, 'max_features': 0.816567151792435, 'min_samples_split': 9, 'min_samples_leaf': 0.05921081678464758}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:42,712] Trial 38 finished with value: 0.20408846094598038 and parameters: {'n_estimators': 87, 'max_features': 0.8732130840232961, 'min_samples_split': 10, 'min_samples_leaf': 0.010022343050794283}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:43,267] Trial 39 finished with value: 0.23498744970021943 and parameters: {'n_estimators': 147, 'max_features': 0.7514693197495977, 'min_samples_split': 9, 'min_samples_leaf': 0.017615909703260123}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:43,559] Trial 40 finished with value: 0.4594129953496862 and parameters: {'n_estimators': 128, 'max_features': 0.9757874175363817, 'min_samples_split': 3, 'min_samples_leaf': 0.07720646119281674}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:44,028] Trial 41 finished with value: 0.24336200097893773 and parameters: {'n_estimators': 155, 'max_features': 0.9282984583656971, 'min_samples_split': 8, 'min_samples_leaf': 0.02241888016325052}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:44,579] Trial 42 finished with value: 0.2020853098002174 and parameters: {'n_estimators': 105, 'max_features': 0.9645024410557635, 'min_samples_split': 5, 'min_samples_leaf': 0.010032980219521877}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:44,853] Trial 43 finished with value: 0.30840368571572474 and parameters: {'n_estimators': 96, 'max_features': 0.5196008345340527, 'min_samples_split': 5, 'min_samples_leaf': 0.016303583023911203}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:45,237] Trial 44 finished with value: 0.25790901942259314 and parameters: {'n_estimators': 113, 'max_features': 0.9701480664892275, 'min_samples_split': 4, 'min_samples_leaf': 0.027112012719906035}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:45,638] Trial 45 finished with value: 0.2290452383415565 and parameters: {'n_estimators': 107, 'max_features': 0.9969733248726091, 'min_samples_split': 10, 'min_samples_leaf': 0.017097475842688413}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:45,837] Trial 46 finished with value: 0.24782677661987412 and parameters: {'n_estimators': 70, 'max_features': 0.7923953990809453, 'min_samples_split': 9, 'min_samples_leaf': 0.020699626114409064}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:46,243] Trial 47 finished with value: 0.20199382173068944 and parameters: {'n_estimators': 123, 'max_features': 0.8804364753976774, 'min_samples_split': 3, 'min_samples_leaf': 0.010490733103444493}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:46,499] Trial 48 finished with value: 0.32242112669685047 and parameters: {'n_estimators': 94, 'max_features': 0.8483992358399453, 'min_samples_split': 2, 'min_samples_leaf': 0.03418701664607847}. Best is trial 32 with value: 0.19835674835518785.\n","[I 2025-06-29 09:35:46,857] Trial 49 finished with value: 0.21846422926201942 and parameters: {'n_estimators': 125, 'max_features': 0.8843060399806754, 'min_samples_split': 3, 'min_samples_leaf': 0.014531389442713198}. Best is trial 32 with value: 0.19835674835518785.\n"]},{"output_type":"stream","name":"stdout","text":["Best Parameters Found by Optuna:\n","{'n_estimators': 121, 'max_features': 0.8602957878821686, 'min_samples_split': 9, 'min_samples_leaf': 0.010693073479200508}\n","\n","Training set metrics:\n","MAE: 0.0059072163048574985, MSE: 6.363482783047166e-05, RMSE: 0.007977144089865223, R²: 0.9996364160555595, MAPE: 0.025256706220828914\n","\n","Validation set metrics:\n","MAE: 0.19835674835518785, MSE: 0.04522694305360638, RMSE: 0.2126662715467744, R²: -6.689638536058293, MAPE: 0.11184049948849341\n","\n","Test set metrics:\n","MAE: 0.46739383438289556, MSE: 0.2244920668101436, RMSE: 0.4738059379219974, R²: -36.19792020073155, MAPE: 0.2307194973038975\n"]}]},{"cell_type":"markdown","source":["###**BOHB**"],"metadata":{"id":"I7nwi-Poj83O"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.model_selection import train_test_split\n","\n","# Sample Data Preparation (Replace with your dataset)\n","# X_train, Y_train, X_val, Y_val, X_test, Y_test = ...\n","\n","# Remove specific unwanted lag features from the DataFrame\n","unwanted_features = ['Close_lag_1', 'Close_lag_2', 'Close_lag_3', 'Close_lag_4', 'Close_lag_5']\n","X_train_cleaned = X_train.drop(columns=unwanted_features, errors='ignore')\n","X_val_cleaned = X_val.drop(columns=unwanted_features, errors='ignore')\n","X_test_cleaned = X_test.drop(columns=unwanted_features, errors='ignore')\n","\n","# Define configuration space for hyperparameter optimization\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","\n","    # Hyperparameters for ExtraTreesRegressor\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 200, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"max_features\", 0.5, 1.0, default_value=0.8))\n","\n","    # Adjusted range for min_samples_split and min_samples_leaf\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"min_samples_split\", 2, 10, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"min_samples_leaf\", 0.01, 0.1, default_value=0.01))  # Using float range (0.0, 1.0)\n","\n","    return cs\n","\n","# Define worker for BOHB\n","class ExtraTreesWorker(Worker):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","    def compute(self, config, budget, **kwargs):\n","        # Define ExtraTreesRegressor model with hyperparameters\n","        model = ExtraTreesRegressor(\n","            n_estimators=config[\"n_estimators\"],\n","            max_features=config[\"max_features\"],\n","            min_samples_split=config[\"min_samples_split\"],\n","            min_samples_leaf=config[\"min_samples_leaf\"],  # Now this will be valid\n","            random_state=42\n","        )\n","\n","        # Fit and evaluate on the validation set\n","        model.fit(X_train_cleaned, Y_train)\n","        Y_val_pred = model.predict(X_val_cleaned)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Set up BOHB\n","NS = hpns.NameServer(run_id=\"extratree_bohb\", host=\"127.0.0.1\", port=None)\n","NS.start()\n","\n","worker = ExtraTreesWorker(nameserver=\"127.0.0.1\", run_id=\"extratree_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(\n","    configspace=get_config_space(),\n","    run_id=\"extratree_bohb\",\n","    nameserver=\"127.0.0.1\",\n","    min_budget=3,  # Increase the budget for better exploration\n","    max_budget=5   # Increase the budget for better exploration\n",")\n","\n","# Perform optimization\n","res = bohb.run(n_iterations=50)\n","\n","# Shutdown BOHB and NameServer\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Retrieve the best configuration\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","# Build the ExtraTreesRegressor model with the best hyperparameters\n","best_model = ExtraTreesRegressor(\n","    n_estimators=best_params[\"n_estimators\"],\n","    max_features=best_params[\"max_features\"],\n","    min_samples_split=best_params[\"min_samples_split\"],\n","    min_samples_leaf=best_params[\"min_samples_leaf\"],\n","    random_state=42\n",")\n","\n","# Fit the best model\n","best_model.fit(X_train_cleaned, Y_train)\n","\n","# Predict and evaluate\n","Y_train_pred = best_model.predict(X_train_cleaned)\n","Y_val_pred = best_model.predict(X_val_cleaned)\n","Y_test_pred = best_model.predict(X_test_cleaned)\n","\n","# Performance metrics calculation\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred)\n","    return mae, mse, rmse, r2, mape\n","\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print the results\n","print(\"Best Parameters Found by BOHB:\")\n","print(best_params)\n","\n","print(\"\\nTraining set metrics:\")\n","print(f\"MAE: {train_metrics[0]}, MSE: {train_metrics[1]}, RMSE: {train_metrics[2]}, R²: {train_metrics[3]}, MAPE: {train_metrics[4]}\")\n","\n","print(\"\\nValidation set metrics:\")\n","print(f\"MAE: {val_metrics[0]}, MSE: {val_metrics[1]}, RMSE: {val_metrics[2]}, R²: {val_metrics[3]}, MAPE: {val_metrics[4]}\")\n","\n","print(\"\\nTest set metrics:\")\n","print(f\"MAE: {test_metrics[0]}, MSE: {test_metrics[1]}, RMSE: {test_metrics[2]}, R²: {test_metrics[3]}, MAPE: {test_metrics[4]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5G3Vlgb6idpu","executionInfo":{"status":"ok","timestamp":1734354619843,"user_tz":-330,"elapsed":11954,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"2685e555-3fef-462a-fa71-42923a0d2b7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters Found by BOHB:\n","{'max_features': 0.9641281616965, 'min_samples_leaf': 0.0118924185015, 'min_samples_split': 8, 'n_estimators': 51}\n","\n","Training set metrics:\n","MAE: 0.0051538865309437385, MSE: 5.039741437283039e-05, RMSE: 0.007099113632900265, R²: 0.9998460406938728, MAPE: 0.01830676823355505\n","\n","Validation set metrics:\n","MAE: 0.014588237297779234, MSE: 0.0006089868604010204, RMSE: 0.02467765913536007, R²: 0.8964579846985827, MAPE: 0.008035764466533127\n","\n","Test set metrics:\n","MAE: 0.16946141734882475, MSE: 0.03976191291023111, RMSE: 0.19940389391942953, R²: -2.1271975584161718, MAPE: 0.0833971224863345\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5rw0KCnukDvF"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1CQdutdgyqbrTk1lZTfK9Vhf0m1anSoli","timestamp":1734111847141},{"file_id":"1NhOLn-oZnqGNDPvM_Gy3KWNMAh2DypGX","timestamp":1734019638652},{"file_id":"18Gu3lcn4Li66MaqYzsvOVlNEV8a_ArEE","timestamp":1733939530379}],"gpuType":"T4","collapsed_sections":["5ebGneEwoKEe","aoU7aW1Qc3GK","aTcG8kCXeaVx","l1UvLmGUedew","4UfWkq2Vg-OZ","K0DSHNFSFAMr","I7nwi-Poj83O"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}