{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aGt34YSxd1U72wCoL1YOqcuEVxd5oZBn","timestamp":1741271287873}],"collapsed_sections":["embxDhWA_wMJ","JrpYo1dbamCt","94koI_eFao0r","ehknf8a-arXK","qCWOF10XateG"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Initial"],"metadata":{"id":"uK1dS__d_tUI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKbPTNe79jrU"},"outputs":[],"source":["# Importing necessary libraries for data analysis and manipulation\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","# For handling warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"clUQy9PG_IaB","outputId":"0354070d-31a6-49ab-ebfd-64449b95f3b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["df_aapl = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stocks/AAPL.csv')"],"metadata":{"id":"rIHlO5kO_NDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import boxcox\n","\n","df_aapl['Close_log'] = np.log(df_aapl['Close'] + 1)\n","df_aapl['Close_sqrt'] = np.sqrt(df_aapl['Close'])\n","df_aapl['Close_boxcox'], _ = boxcox(df_aapl['Close'] + 1)\n"],"metadata":{"id":"htDFq3P8_Xwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","skew_original = df_aapl['Close'].skew()\n","skew_log = df_aapl['Close_log'].skew()\n","skew_sqrt = df_aapl['Close_sqrt'].skew()\n","skew_boxcox = pd.Series(df_aapl['Close_boxcox']).skew()\n","\n","print(f\"Original Skewness: {skew_original}\")\n","print(f\"Log Transformation Skewness: {skew_log}\")\n","print(f\"Square Root Transformation Skewness: {skew_sqrt}\")\n","print(f\"Box-Cox Transformation Skewness: {skew_boxcox}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"liGFwwlA_aHf","outputId":"27a8da3c-491c-48ca-e161-d141ff35d675"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Skewness: 2.5045276102319933\n","Log Transformation Skewness: 0.8535555176510303\n","Square Root Transformation Skewness: 1.6211545809555206\n","Box-Cox Transformation Skewness: 0.43527466713563334\n"]}]},{"cell_type":"code","source":["\n","df_aapl['Open_log'] = np.log(df_aapl['Open'])\n","df_aapl['High_log'] = np.log(df_aapl['High'])\n","df_aapl['Low_log'] = np.log(df_aapl['Low'])\n","df_aapl['Adj Close_log'] = np.log(df_aapl['Adj Close'])\n","df_aapl['Volume_log'] = np.log(df_aapl['Volume'])\n","\n","\n","df_aapl['Open_sqrt'] = np.sqrt(df_aapl['Open'])\n","df_aapl['High_sqrt'] = np.sqrt(df_aapl['High'])\n","df_aapl['Low_sqrt'] = np.sqrt(df_aapl['Low'])\n","df_aapl['Adj Close_sqrt'] = np.sqrt(df_aapl['Adj Close'])\n","df_aapl['Volume_sqrt'] = np.sqrt(df_aapl['Volume'])\n","\n","from scipy.stats import boxcox\n","df_aapl['Open_boxcox'], _ = boxcox(df_aapl['Open'])\n","df_aapl['High_boxcox'], _ = boxcox(df_aapl['High'])\n","df_aapl['Low_boxcox'], _ = boxcox(df_aapl['Low'])\n","df_aapl['Adj Close_boxcox'], _ = boxcox(df_aapl['Adj Close'])"],"metadata":{"id":"8rlsDOcr_bpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","skewness_before = df_aapl[['Open', 'High', 'Low', 'Adj Close', 'Volume']].skew()\n","skewness_after = df_aapl[['Open_log', 'High_log', 'Low_log', 'Adj Close_log',\n","                          'Open_sqrt', 'High_sqrt', 'Low_sqrt', 'Adj Close_sqrt', 'Volume_sqrt',\n","                          'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox']].skew()\n","\n","print(\"Skewness Before Transformation:\\n\", skewness_before)\n","print(\"\\nSkewness After Transformation:\\n\", skewness_after)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYyf057d_dII","outputId":"13d63c2f-7b73-49be-eee9-750006c0ec80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness Before Transformation:\n"," Open         2.504632\n","High         2.502208\n","Low          2.506714\n","Adj Close    2.550677\n","Volume       3.565699\n","dtype: float64\n","\n","Skewness After Transformation:\n"," Open_log            0.482872\n","High_log            0.481997\n","Low_log             0.484246\n","Adj Close_log       0.494009\n","Open_sqrt           1.620771\n","High_sqrt           1.621456\n","Low_sqrt            1.620661\n","Adj Close_sqrt      1.679402\n","Volume_sqrt         1.299776\n","Open_boxcox         0.181226\n","High_boxcox         0.179749\n","Low_boxcox          0.182882\n","Adj Close_boxcox    0.180085\n","dtype: float64\n"]}]},{"cell_type":"code","source":["from scipy import stats\n","\n","df_aapl['Open_boxcox'], _ = stats.boxcox(df_aapl['Open'] + 1)\n","df_aapl['High_boxcox'], _ = stats.boxcox(df_aapl['High'] + 1)\n","df_aapl['Low_boxcox'], _ = stats.boxcox(df_aapl['Low'] + 1)\n","df_aapl['Adj Close_boxcox'], _ = stats.boxcox(df_aapl['Adj Close'] + 1)\n","df_aapl['Close_boxcox'], _ = stats.boxcox(df_aapl['Close'] + 1)\n","\n","skewness_after_boxcox = df_aapl[['Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox', 'Close_boxcox']].skew()\n","\n","print(\"Skewness After Box-Cox Transformation:\")\n","print(skewness_after_boxcox)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOGjmLU0_e1r","outputId":"ff2d7a8f-3321-470f-b46d-6ae3e09dc0b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness After Box-Cox Transformation:\n","Open_boxcox         0.435237\n","High_boxcox         0.433381\n","Low_boxcox          0.437331\n","Adj Close_boxcox    0.458762\n","Close_boxcox        0.435275\n","dtype: float64\n"]}]},{"cell_type":"code","source":["\n","df_aapl_cleaned = df_aapl[['Date', 'Open', 'High', 'Low', 'Adj Close', 'Close', 'Volume',\n","                           'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox',\n","                           'Close_boxcox']]\n","\n","print(df_aapl_cleaned.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7UaPkC4_jFu","outputId":"e67e9d7e-cb0b-4b56-ecf9-a3447401cda2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["         Date      Open      High       Low  Adj Close     Close     Volume  \\\n","0  1980-12-12  0.128348  0.128906  0.128348   0.098943  0.128348  469033600   \n","1  1980-12-15  0.122210  0.122210  0.121652   0.093781  0.121652  175884800   \n","2  1980-12-16  0.113281  0.113281  0.112723   0.086898  0.112723  105728000   \n","3  1980-12-17  0.115513  0.116071  0.115513   0.089049  0.115513   86441600   \n","4  1980-12-18  0.118862  0.119420  0.118862   0.091630  0.118862   73449600   \n","\n","   Open_boxcox  High_boxcox  Low_boxcox  Adj Close_boxcox  Close_boxcox  \n","0     0.117689     0.118173    0.117674          0.092374      0.117689  \n","1     0.112503     0.112516    0.112016          0.087857      0.112030  \n","2     0.104886     0.104897    0.104395          0.081785      0.104407  \n","3     0.106798     0.107287    0.106786          0.083688      0.106798  \n","4     0.109657     0.110145    0.109644          0.085966      0.109657  \n"]}]},{"cell_type":"markdown","source":["# Train, Validation and Test"],"metadata":{"id":"BtR5pMPo_m33"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X = df_aapl_cleaned[['Open_boxcox', 'High_boxcox', 'Low_boxcox']]\n","Y = df_aapl_cleaned['Close_boxcox']\n","\n","X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, shuffle=False)\n","X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, shuffle=False)\n","\n","print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0gWlfXZ_ky_","outputId":"6917282b-fba4-4995-a2db-10c25c90ddd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set: (7736, 3), Validation set: (1658, 3), Test set: (1658, 3)\n"]}]},{"cell_type":"markdown","source":["# XG Bosst"],"metadata":{"id":"embxDhWA_wMJ"}},{"cell_type":"markdown","source":["## Initial"],"metadata":{"id":"7TorJvjo_yPQ"}},{"cell_type":"code","source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import GRU, Dense\n","import xgboost as xgb\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Enable GPU for TensorFlow\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train a GRU model on GPU\n","def train_gru(X_train, Y_train, X_val, Y_val, layers):\n","    with tf.device('/GPU:0'):  # Force GPU usage\n","        model = Sequential()\n","        model.add(GRU(64, return_sequences=(layers > 1), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(GRU(64, return_sequences=(_ < layers - 2)))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        start_time = time.time()\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","        train_time = time.time() - start_time\n","        return model, train_time\n","\n","# Reshaping input for GRU (adding time step dimension)\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train 2, 3, and 5-layer GRU models\n","gru_models = {}\n","gru_predictions = {}\n","times = {}\n","\n","for layers in [2, 3, 5]:\n","    model, train_time = train_gru(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    Y_train_pred = model.predict(X_train_r)\n","    Y_val_pred = model.predict(X_val_r)\n","    Y_test_pred = model.predict(X_test_r)\n","\n","    gru_models[layers] = model\n","    gru_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","    times[f'GRU-{layers}'] = train_time\n","\n","# Prepare input for XGBoost (using GRU predictions as features)\n","X_train_xgb = np.column_stack([gru_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_xgb = np.column_stack([gru_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_xgb = np.column_stack([gru_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train XGBoost model with GPU\n","xgb_model = xgb.XGBRegressor(objective='reg:squarederror',\n","                             n_estimators=100,\n","                             learning_rate=0.05,\n","                             max_depth=3,\n","                             tree_method='gpu_hist')  # Enable GPU acceleration\n","\n","start_time = time.time()\n","xgb_model.fit(X_train_xgb, Y_train)\n","times['XGBoost'] = time.time() - start_time\n","\n","# Predictions from XGBoost\n","start_time = time.time()\n","Y_train_pred_xgb = xgb_model.predict(X_train_xgb)\n","times['XGBoost Train'] = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_xgb = xgb_model.predict(X_val_xgb)\n","times['XGBoost Validate'] = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_xgb = xgb_model.predict(X_test_xgb)\n","times['XGBoost Test'] = time.time() - start_time\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_xgb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_xgb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_xgb)\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print training times\n","print(\"Training Times:\")\n","for model, t in times.items():\n","    print(f\"{model}: {t:.2f} seconds\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvEXi7NT_z7D","outputId":"7f1d0a8a-3f55-47f6-deeb-eac549055a4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU activated for TensorFlow!\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","Train Metrics: MAE=0.0042, MSE=0.0000, RMSE=0.0060, R²=0.9998, MAPE=1.50%\n","Validation Metrics: MAE=0.1711, MSE=0.0352, RMSE=0.1875, R²=-4.9786, MAPE=9.62%\n","Test Metrics: MAE=0.4402, MSE=0.1998, RMSE=0.4470, R²=-32.1021, MAPE=21.72%\n","Training Times:\n","GRU-2: 80.26 seconds\n","GRU-3: 91.85 seconds\n","GRU-5: 136.17 seconds\n","XGBoost: 0.75 seconds\n","XGBoost Train: 0.06 seconds\n","XGBoost Validate: 0.00 seconds\n","XGBoost Test: 0.00 seconds\n"]}]},{"cell_type":"markdown","source":["## Optuna"],"metadata":{"id":"Pw5lgQ5fGUX1"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUKq_V0TGaoJ","outputId":"e2555bba-2fa9-4b2b-a83d-d236056aea88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import GRU, Dense\n","import xgboost as xgb\n","import optuna\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import time  # For tracking training time\n","\n","# Function to compute metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Function to train GRU model\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","def train_gru(X_train, Y_train, X_val, Y_val, units, layers, learning_rate, batch_size, epochs):\n","    model = keras.Sequential()\n","\n","    for _ in range(layers - 1):  # All except last layer have return_sequences=True\n","        model.add(GRU(units, return_sequences=True))\n","\n","    model.add(GRU(units))  # Last GRU layer\n","    model.add(Dense(1))  # Output layer\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n","\n","    # Early Stopping Callback\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","\n","    start_time = time.time()  # Start time tracking\n","    history = model.fit(X_train, Y_train,\n","                        validation_data=(X_val, Y_val),\n","                        epochs=epochs,\n","                        batch_size=batch_size,\n","                        verbose=0,\n","                        callbacks=[early_stopping])  # Add early stopping\n","    gru_train_time = time.time() - start_time  # End time tracking\n","\n","    return model, history, gru_train_time\n","\n","# Objective function for Optuna\n","def objective(trial):\n","    units = trial.suggest_int(\"gru_units\", 32, 128, step=16)\n","    layers = trial.suggest_categorical(\"gru_layers\", [2, 3, 5])\n","    learning_rate = trial.suggest_loguniform(\"gru_learning_rate\", 1e-4, 1e-2)\n","    batch_size = trial.suggest_categorical(\"gru_batch_size\", [16, 32, 64])\n","    epochs = trial.suggest_int(\"gru_epochs\", 10, 50, step=10)\n","\n","    # Reshape input for GRU\n","    X_train_r = np.expand_dims(X_train, axis=-1)\n","    X_val_r = np.expand_dims(X_val, axis=-1)\n","    X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","    # Train GRU\n","    model, _, gru_train_time = train_gru(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","    Y_train_pred_gru = model.predict(X_train_r).flatten()\n","    Y_val_pred_gru = model.predict(X_val_r).flatten()\n","    Y_test_pred_gru = model.predict(X_test_r).flatten()\n","\n","    # Compute metrics for GRU\n","    metrics_train = compute_metrics(Y_train, Y_train_pred_gru)\n","    metrics_val = compute_metrics(Y_val, Y_val_pred_gru)\n","    metrics_test = compute_metrics(Y_test, Y_test_pred_gru)\n","\n","    print(f\"GRU Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","    print(f\"GRU Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","    print(f\"GRU Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","    # Prepare data for XGBoost\n","    X_train_xgb = np.column_stack([Y_train_pred_gru])\n","    X_val_xgb = np.column_stack([Y_val_pred_gru])\n","    X_test_xgb = np.column_stack([Y_test_pred_gru])\n","\n","    xgb_params = {\n","        \"objective\": \"reg:squarederror\",\n","        \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 50, 200, step=50),\n","        \"learning_rate\": trial.suggest_loguniform(\"xgb_learning_rate\", 0.01, 0.3),\n","        \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 10),\n","    }\n","\n","    if tf.config.list_physical_devices(\"GPU\"):\n","        xgb_params[\"tree_method\"] = \"gpu_hist\"\n","\n","    # Train XGBoost\n","    start_time = time.time()\n","    xgb_model = xgb.XGBRegressor(**xgb_params)\n","    xgb_model.fit(X_train_xgb, Y_train)\n","    xgb_train_time = time.time() - start_time\n","\n","    # Predict and evaluate\n","    Y_train_pred_xgb = xgb_model.predict(X_train_xgb)\n","    Y_val_pred_xgb = xgb_model.predict(X_val_xgb)\n","    Y_test_pred_xgb = xgb_model.predict(X_test_xgb)\n","\n","    # Compute metrics for XGBoost\n","    metrics_train_xgb = compute_metrics(Y_train, Y_train_pred_xgb)\n","    metrics_val_xgb = compute_metrics(Y_val, Y_val_pred_xgb)\n","    metrics_test_xgb = compute_metrics(Y_test, Y_test_pred_xgb)\n","\n","    print(f\"XGBoost Train Metrics: MAE={metrics_train_xgb[0]:.4f}, MSE={metrics_train_xgb[1]:.4f}, RMSE={metrics_train_xgb[2]:.4f}, R²={metrics_train_xgb[3]:.4f}, MAPE={metrics_train_xgb[4]:.2f}%\")\n","    print(f\"XGBoost Validation Metrics: MAE={metrics_val_xgb[0]:.4f}, MSE={metrics_val_xgb[1]:.4f}, RMSE={metrics_val_xgb[2]:.4f}, R²={metrics_val_xgb[3]:.4f}, MAPE={metrics_val_xgb[4]:.2f}%\")\n","    print(f\"XGBoost Test Metrics: MAE={metrics_test_xgb[0]:.4f}, MSE={metrics_test_xgb[1]:.4f}, RMSE={metrics_test_xgb[2]:.4f}, R²={metrics_test_xgb[3]:.4f}, MAPE={metrics_test_xgb[4]:.2f}%\")\n","\n","    print(f\"GRU Training Time: {gru_train_time:.2f} seconds\")\n","    print(f\"XGBoost Training Time: {xgb_train_time:.2f} seconds\")\n","\n","    return metrics_val_xgb[2]  # Return RMSE for Optuna optimization\n","\n","# Run Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=25)\n","\n","# Best hyperparameters\n","print(\"Best hyperparameters:\", study.best_params)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cYc_tj8GbFi","outputId":"f5558311-5256-4ccf-8609-49897a3bfc4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:30:02,121] A new study created in memory with name: no-name-ebeaaa2a-21ea-4337-ab63-04cb566ef668\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","GRU Train Metrics: MAE=0.0051, MSE=0.0000, RMSE=0.0066, R²=0.9998, MAPE=1.80%\n","GRU Validation Metrics: MAE=0.0089, MSE=0.0001, RMSE=0.0110, R²=0.9794, MAPE=0.50%\n","GRU Test Metrics: MAE=0.0491, MSE=0.0027, RMSE=0.0515, R²=0.5606, MAPE=2.41%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:32:14,793] Trial 0 finished with value: 0.3591757086977534 and parameters: {'gru_units': 64, 'gru_layers': 5, 'gru_learning_rate': 0.0012134453037837014, 'gru_batch_size': 16, 'gru_epochs': 20, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.0127195303424236, 'xgb_max_depth': 9}. Best is trial 0 with value: 0.3591757086977534.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0485, MSE=0.0039, RMSE=0.0623, R²=0.9778, MAPE=18.56%\n","XGBoost Validation Metrics: MAE=0.3509, MSE=0.1290, RMSE=0.3592, R²=-20.9342, MAPE=19.93%\n","XGBoost Test Metrics: MAE=0.6199, MSE=0.3903, RMSE=0.6248, R²=-63.6799, MAPE=30.65%\n","GRU Training Time: 128.50 seconds\n","XGBoost Training Time: 0.42 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0043, MSE=0.0000, RMSE=0.0056, R²=0.9998, MAPE=1.85%\n","GRU Validation Metrics: MAE=0.0019, MSE=0.0000, RMSE=0.0026, R²=0.9989, MAPE=0.11%\n","GRU Test Metrics: MAE=0.0063, MSE=0.0000, RMSE=0.0069, R²=0.9920, MAPE=0.31%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:32:55,579] Trial 1 finished with value: 0.17571755644511275 and parameters: {'gru_units': 80, 'gru_layers': 2, 'gru_learning_rate': 0.00016046438077414166, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.16114892492022576, 'xgb_max_depth': 9}. Best is trial 1 with value: 0.17571755644511275.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0038, MSE=0.0000, RMSE=0.0057, R²=0.9998, MAPE=1.08%\n","XGBoost Validation Metrics: MAE=0.1582, MSE=0.0309, RMSE=0.1757, R²=-4.2498, MAPE=8.88%\n","XGBoost Test Metrics: MAE=0.4271, MSE=0.1885, RMSE=0.4341, R²=-30.2308, MAPE=21.07%\n","GRU Training Time: 38.76 seconds\n","XGBoost Training Time: 0.26 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","GRU Train Metrics: MAE=0.0068, MSE=0.0001, RMSE=0.0084, R²=0.9996, MAPE=3.46%\n","GRU Validation Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0043, R²=0.9968, MAPE=0.20%\n","GRU Test Metrics: MAE=0.0209, MSE=0.0005, RMSE=0.0223, R²=0.9173, MAPE=1.02%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:33:20,549] Trial 2 finished with value: 0.4622699078418917 and parameters: {'gru_units': 48, 'gru_layers': 3, 'gru_learning_rate': 0.000214841664179632, 'gru_batch_size': 32, 'gru_epochs': 20, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.01428102715890757, 'xgb_max_depth': 5}. Best is trial 1 with value: 0.17571755644511275.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0784, MSE=0.0101, RMSE=0.1003, R²=0.9425, MAPE=29.97%\n","XGBoost Validation Metrics: MAE=0.4559, MSE=0.2137, RMSE=0.4623, R²=-35.3329, MAPE=25.96%\n","XGBoost Test Metrics: MAE=0.7249, MSE=0.5315, RMSE=0.7291, R²=-87.0713, MAPE=35.87%\n","GRU Training Time: 22.05 seconds\n","XGBoost Training Time: 0.21 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0047, MSE=0.0000, RMSE=0.0066, R²=0.9998, MAPE=1.46%\n","GRU Validation Metrics: MAE=0.0041, MSE=0.0000, RMSE=0.0051, R²=0.9955, MAPE=0.24%\n","GRU Test Metrics: MAE=0.0176, MSE=0.0004, RMSE=0.0191, R²=0.9394, MAPE=0.86%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:33:38,526] Trial 3 finished with value: 0.18452306686567352 and parameters: {'gru_units': 64, 'gru_layers': 3, 'gru_learning_rate': 0.00788047451728766, 'gru_batch_size': 64, 'gru_epochs': 10, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.09676616226381016, 'xgb_max_depth': 10}. Best is trial 1 with value: 0.17571755644511275.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0042, MSE=0.0000, RMSE=0.0061, R²=0.9998, MAPE=1.36%\n","XGBoost Validation Metrics: MAE=0.1678, MSE=0.0340, RMSE=0.1845, R²=-4.7891, MAPE=9.43%\n","XGBoost Test Metrics: MAE=0.4369, MSE=0.1969, RMSE=0.4437, R²=-31.6241, MAPE=21.56%\n","GRU Training Time: 15.87 seconds\n","XGBoost Training Time: 0.17 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","GRU Train Metrics: MAE=0.0094, MSE=0.0001, RMSE=0.0111, R²=0.9993, MAPE=3.32%\n","GRU Validation Metrics: MAE=0.0068, MSE=0.0001, RMSE=0.0082, R²=0.9885, MAPE=0.39%\n","GRU Test Metrics: MAE=0.0407, MSE=0.0019, RMSE=0.0430, R²=0.6930, MAPE=2.00%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:34:00,985] Trial 4 finished with value: 0.17571337346388857 and parameters: {'gru_units': 64, 'gru_layers': 5, 'gru_learning_rate': 0.009224010990911385, 'gru_batch_size': 64, 'gru_epochs': 10, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.11581117601755432, 'xgb_max_depth': 7}. Best is trial 4 with value: 0.17571337346388857.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0038, MSE=0.0000, RMSE=0.0057, R²=0.9998, MAPE=1.08%\n","XGBoost Validation Metrics: MAE=0.1581, MSE=0.0309, RMSE=0.1757, R²=-4.2495, MAPE=8.88%\n","XGBoost Test Metrics: MAE=0.4271, MSE=0.1885, RMSE=0.4341, R²=-30.2301, MAPE=21.07%\n","GRU Training Time: 18.94 seconds\n","XGBoost Training Time: 0.22 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","GRU Train Metrics: MAE=0.0037, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=1.26%\n","GRU Validation Metrics: MAE=0.0041, MSE=0.0000, RMSE=0.0049, R²=0.9959, MAPE=0.23%\n","GRU Test Metrics: MAE=0.0171, MSE=0.0003, RMSE=0.0179, R²=0.9470, MAPE=0.84%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:34:36,979] Trial 5 finished with value: 0.17591407567976342 and parameters: {'gru_units': 96, 'gru_layers': 3, 'gru_learning_rate': 0.0008571239173815505, 'gru_batch_size': 16, 'gru_epochs': 20, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.08290398854070592, 'xgb_max_depth': 9}. Best is trial 4 with value: 0.17571337346388857.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0038, MSE=0.0000, RMSE=0.0058, R²=0.9998, MAPE=1.10%\n","XGBoost Validation Metrics: MAE=0.1584, MSE=0.0309, RMSE=0.1759, R²=-4.2615, MAPE=8.89%\n","XGBoost Test Metrics: MAE=0.4274, MSE=0.1887, RMSE=0.4344, R²=-30.2617, MAPE=21.08%\n","GRU Training Time: 33.46 seconds\n","XGBoost Training Time: 0.35 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0047, MSE=0.0000, RMSE=0.0066, R²=0.9998, MAPE=1.51%\n","GRU Validation Metrics: MAE=0.0031, MSE=0.0000, RMSE=0.0039, R²=0.9975, MAPE=0.18%\n","GRU Test Metrics: MAE=0.0140, MSE=0.0002, RMSE=0.0150, R²=0.9626, MAPE=0.69%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:35:22,775] Trial 6 finished with value: 0.24207136959177597 and parameters: {'gru_units': 112, 'gru_layers': 3, 'gru_learning_rate': 0.00035590289703528184, 'gru_batch_size': 16, 'gru_epochs': 10, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.029596520826583116, 'xgb_max_depth': 4}. Best is trial 4 with value: 0.17571337346388857.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0170, MSE=0.0005, RMSE=0.0220, R²=0.9972, MAPE=6.55%\n","XGBoost Validation Metrics: MAE=0.2296, MSE=0.0586, RMSE=0.2421, R²=-8.9631, MAPE=12.98%\n","XGBoost Test Metrics: MAE=0.4986, MSE=0.2547, RMSE=0.5047, R²=-41.1993, MAPE=24.62%\n","GRU Training Time: 43.90 seconds\n","XGBoost Training Time: 0.18 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0050, MSE=0.0000, RMSE=0.0069, R²=0.9997, MAPE=1.31%\n","GRU Validation Metrics: MAE=0.0081, MSE=0.0001, RMSE=0.0099, R²=0.9834, MAPE=0.45%\n","GRU Test Metrics: MAE=0.0400, MSE=0.0017, RMSE=0.0416, R²=0.7128, MAPE=1.96%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:06,468] Trial 7 finished with value: 0.17554103358149703 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.007983139606085564, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.1422206693482803, 'xgb_max_depth': 4}. Best is trial 7 with value: 0.17554103358149703.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=1.00%\n","XGBoost Validation Metrics: MAE=0.1580, MSE=0.0308, RMSE=0.1755, R²=-4.2392, MAPE=8.87%\n","XGBoost Test Metrics: MAE=0.4269, MSE=0.1883, RMSE=0.4339, R²=-30.2030, MAPE=21.06%\n","GRU Training Time: 41.74 seconds\n","XGBoost Training Time: 0.17 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","GRU Train Metrics: MAE=0.0056, MSE=0.0000, RMSE=0.0070, R²=0.9997, MAPE=2.84%\n","GRU Validation Metrics: MAE=0.0140, MSE=0.0002, RMSE=0.0157, R²=0.9580, MAPE=0.79%\n","GRU Test Metrics: MAE=0.0474, MSE=0.0024, RMSE=0.0488, R²=0.6048, MAPE=2.33%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:24,091] Trial 8 finished with value: 0.5121562252988904 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.00014280984405440345, 'gru_batch_size': 64, 'gru_epochs': 10, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.025002679607915433, 'xgb_max_depth': 10}. Best is trial 7 with value: 0.17554103358149703.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0929, MSE=0.0141, RMSE=0.1189, R²=0.9192, MAPE=35.54%\n","XGBoost Validation Metrics: MAE=0.5064, MSE=0.2623, RMSE=0.5122, R²=-43.5978, MAPE=28.86%\n","XGBoost Test Metrics: MAE=0.7754, MSE=0.6073, RMSE=0.7793, R²=-99.6300, MAPE=38.38%\n","GRU Training Time: 15.65 seconds\n","XGBoost Training Time: 0.23 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0085, MSE=0.0001, RMSE=0.0104, R²=0.9994, MAPE=4.63%\n","GRU Validation Metrics: MAE=0.0039, MSE=0.0000, RMSE=0.0048, R²=0.9961, MAPE=0.23%\n","GRU Test Metrics: MAE=0.0203, MSE=0.0005, RMSE=0.0221, R²=0.9193, MAPE=0.99%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:36,188] Trial 9 finished with value: 0.24797639924109363 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.00020426980306198237, 'gru_batch_size': 64, 'gru_epochs': 50, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.028689534808618564, 'xgb_max_depth': 5}. Best is trial 7 with value: 0.17554103358149703.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0185, MSE=0.0006, RMSE=0.0239, R²=0.9967, MAPE=7.03%\n","XGBoost Validation Metrics: MAE=0.2358, MSE=0.0615, RMSE=0.2480, R²=-9.4551, MAPE=13.33%\n","XGBoost Test Metrics: MAE=0.5049, MSE=0.2609, RMSE=0.5108, R²=-42.2331, MAPE=24.93%\n","GRU Training Time: 10.51 seconds\n","XGBoost Training Time: 0.22 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0050, R²=0.9999, MAPE=1.03%\n","GRU Validation Metrics: MAE=0.0024, MSE=0.0000, RMSE=0.0030, R²=0.9985, MAPE=0.14%\n","GRU Test Metrics: MAE=0.0103, MSE=0.0001, RMSE=0.0110, R²=0.9799, MAPE=0.50%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:37:08,056] Trial 10 finished with value: 0.1754788422976193 and parameters: {'gru_units': 128, 'gru_layers': 2, 'gru_learning_rate': 0.003115069244876269, 'gru_batch_size': 32, 'gru_epochs': 40, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.2814978277277694, 'xgb_max_depth': 3}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=0.99%\n","XGBoost Validation Metrics: MAE=0.1579, MSE=0.0308, RMSE=0.1755, R²=-4.2355, MAPE=8.86%\n","XGBoost Test Metrics: MAE=0.4269, MSE=0.1883, RMSE=0.4339, R²=-30.1932, MAPE=21.06%\n","GRU Training Time: 29.61 seconds\n","XGBoost Training Time: 0.27 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0032, MSE=0.0000, RMSE=0.0046, R²=0.9999, MAPE=1.01%\n","GRU Validation Metrics: MAE=0.0074, MSE=0.0001, RMSE=0.0083, R²=0.9882, MAPE=0.42%\n","GRU Test Metrics: MAE=0.0235, MSE=0.0006, RMSE=0.0242, R²=0.9030, MAPE=1.16%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:37:24,875] Trial 11 finished with value: 0.17563883452452533 and parameters: {'gru_units': 128, 'gru_layers': 2, 'gru_learning_rate': 0.003549193915117188, 'gru_batch_size': 32, 'gru_epochs': 40, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.2845573868226529, 'xgb_max_depth': 3}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=1.01%\n","XGBoost Validation Metrics: MAE=0.1581, MSE=0.0308, RMSE=0.1756, R²=-4.2451, MAPE=8.87%\n","XGBoost Test Metrics: MAE=0.4270, MSE=0.1884, RMSE=0.4341, R²=-30.2184, MAPE=21.07%\n","GRU Training Time: 15.02 seconds\n","XGBoost Training Time: 0.27 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","GRU Train Metrics: MAE=0.0033, MSE=0.0000, RMSE=0.0045, R²=0.9999, MAPE=1.11%\n","GRU Validation Metrics: MAE=0.0045, MSE=0.0000, RMSE=0.0052, R²=0.9953, MAPE=0.25%\n","GRU Test Metrics: MAE=0.0169, MSE=0.0003, RMSE=0.0175, R²=0.9490, MAPE=0.83%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:37:45,160] Trial 12 finished with value: 0.17560483866192467 and parameters: {'gru_units': 128, 'gru_layers': 2, 'gru_learning_rate': 0.003449240714295065, 'gru_batch_size': 32, 'gru_epochs': 40, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.28327708508268734, 'xgb_max_depth': 3}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=1.02%\n","XGBoost Validation Metrics: MAE=0.1580, MSE=0.0308, RMSE=0.1756, R²=-4.2430, MAPE=8.87%\n","XGBoost Test Metrics: MAE=0.4270, MSE=0.1884, RMSE=0.4340, R²=-30.2130, MAPE=21.07%\n","GRU Training Time: 17.87 seconds\n","XGBoost Training Time: 0.23 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0032, MSE=0.0000, RMSE=0.0046, R²=0.9999, MAPE=1.01%\n","GRU Validation Metrics: MAE=0.0036, MSE=0.0000, RMSE=0.0042, R²=0.9969, MAPE=0.20%\n","GRU Test Metrics: MAE=0.0148, MSE=0.0002, RMSE=0.0154, R²=0.9606, MAPE=0.73%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:38:07,547] Trial 13 finished with value: 0.17564065769029655 and parameters: {'gru_units': 96, 'gru_layers': 2, 'gru_learning_rate': 0.0036151644550709116, 'gru_batch_size': 32, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.19192292563723468, 'xgb_max_depth': 6}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=1.00%\n","XGBoost Validation Metrics: MAE=0.1581, MSE=0.0308, RMSE=0.1756, R²=-4.2452, MAPE=8.87%\n","XGBoost Test Metrics: MAE=0.4271, MSE=0.1884, RMSE=0.4341, R²=-30.2187, MAPE=21.07%\n","GRU Training Time: 20.38 seconds\n","XGBoost Training Time: 0.20 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","GRU Train Metrics: MAE=0.0041, MSE=0.0000, RMSE=0.0058, R²=0.9998, MAPE=1.13%\n","GRU Validation Metrics: MAE=0.0043, MSE=0.0000, RMSE=0.0052, R²=0.9953, MAPE=0.24%\n","GRU Test Metrics: MAE=0.0220, MSE=0.0005, RMSE=0.0230, R²=0.9122, MAPE=1.08%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:39:16,126] Trial 14 finished with value: 0.17565245475302702 and parameters: {'gru_units': 96, 'gru_layers': 2, 'gru_learning_rate': 0.0017702038080399246, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.057816909248756314, 'xgb_max_depth': 4}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=0.99%\n","XGBoost Validation Metrics: MAE=0.1581, MSE=0.0309, RMSE=0.1757, R²=-4.2459, MAPE=8.87%\n","XGBoost Test Metrics: MAE=0.4271, MSE=0.1884, RMSE=0.4341, R²=-30.2205, MAPE=21.07%\n","GRU Training Time: 66.08 seconds\n","XGBoost Training Time: 0.35 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0040, MSE=0.0000, RMSE=0.0056, R²=0.9998, MAPE=1.11%\n","GRU Validation Metrics: MAE=0.0063, MSE=0.0001, RMSE=0.0076, R²=0.9903, MAPE=0.35%\n","GRU Test Metrics: MAE=0.0274, MSE=0.0008, RMSE=0.0284, R²=0.8660, MAPE=1.35%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:39:37,901] Trial 15 finished with value: 0.1758854312912012 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.00570456046381695, 'gru_batch_size': 32, 'gru_epochs': 50, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.17046426307575036, 'xgb_max_depth': 3}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0037, MSE=0.0000, RMSE=0.0055, R²=0.9998, MAPE=1.09%\n","XGBoost Validation Metrics: MAE=0.1583, MSE=0.0309, RMSE=0.1759, R²=-4.2598, MAPE=8.89%\n","XGBoost Test Metrics: MAE=0.4273, MSE=0.1886, RMSE=0.4343, R²=-30.2572, MAPE=21.08%\n","GRU Training Time: 20.13 seconds\n","XGBoost Training Time: 0.21 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","GRU Train Metrics: MAE=0.0102, MSE=0.0001, RMSE=0.0122, R²=0.9991, MAPE=3.42%\n","GRU Validation Metrics: MAE=0.0109, MSE=0.0002, RMSE=0.0133, R²=0.9699, MAPE=0.61%\n","GRU Test Metrics: MAE=0.0582, MSE=0.0037, RMSE=0.0609, R²=0.3862, MAPE=2.86%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:41:02,636] Trial 16 finished with value: 0.22337440805402572 and parameters: {'gru_units': 80, 'gru_layers': 5, 'gru_learning_rate': 0.00216878516497274, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.06458720836583118, 'xgb_max_depth': 7}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0124, MSE=0.0003, RMSE=0.0163, R²=0.9985, MAPE=4.68%\n","XGBoost Validation Metrics: MAE=0.2098, MSE=0.0499, RMSE=0.2234, R²=-7.4835, MAPE=11.84%\n","XGBoost Test Metrics: MAE=0.4788, MSE=0.2353, RMSE=0.4851, R²=-37.9915, MAPE=23.64%\n","GRU Training Time: 81.39 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0034, MSE=0.0000, RMSE=0.0047, R²=0.9999, MAPE=1.26%\n","GRU Validation Metrics: MAE=0.0019, MSE=0.0000, RMSE=0.0025, R²=0.9990, MAPE=0.11%\n","GRU Test Metrics: MAE=0.0068, MSE=0.0001, RMSE=0.0073, R²=0.9911, MAPE=0.33%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:41:21,217] Trial 17 finished with value: 0.1756554576715194 and parameters: {'gru_units': 112, 'gru_layers': 2, 'gru_learning_rate': 0.0006729853210713665, 'gru_batch_size': 32, 'gru_epochs': 30, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.13372284326310924, 'xgb_max_depth': 4}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0036, MSE=0.0000, RMSE=0.0055, R²=0.9998, MAPE=1.06%\n","XGBoost Validation Metrics: MAE=0.1581, MSE=0.0309, RMSE=0.1757, R²=-4.2460, MAPE=8.87%\n","XGBoost Test Metrics: MAE=0.4271, MSE=0.1884, RMSE=0.4341, R²=-30.2210, MAPE=21.07%\n","GRU Training Time: 15.17 seconds\n","XGBoost Training Time: 0.27 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0056, MSE=0.0000, RMSE=0.0071, R²=0.9997, MAPE=1.97%\n","GRU Validation Metrics: MAE=0.0034, MSE=0.0000, RMSE=0.0042, R²=0.9970, MAPE=0.20%\n","GRU Test Metrics: MAE=0.0137, MSE=0.0002, RMSE=0.0149, R²=0.9635, MAPE=0.67%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:41:53,091] Trial 18 finished with value: 0.17549556909712402 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.005459038524355611, 'gru_batch_size': 32, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.21148247821713953, 'xgb_max_depth': 5}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=1.00%\n","XGBoost Validation Metrics: MAE=0.1579, MSE=0.0308, RMSE=0.1755, R²=-4.2365, MAPE=8.86%\n","XGBoost Test Metrics: MAE=0.4269, MSE=0.1883, RMSE=0.4339, R²=-30.1958, MAPE=21.06%\n","GRU Training Time: 30.32 seconds\n","XGBoost Training Time: 0.18 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","GRU Train Metrics: MAE=0.0045, MSE=0.0000, RMSE=0.0065, R²=0.9998, MAPE=1.31%\n","GRU Validation Metrics: MAE=0.0082, MSE=0.0001, RMSE=0.0100, R²=0.9830, MAPE=0.46%\n","GRU Test Metrics: MAE=0.0417, MSE=0.0019, RMSE=0.0436, R²=0.6855, MAPE=2.05%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:42:43,886] Trial 19 finished with value: 0.17569846508038425 and parameters: {'gru_units': 48, 'gru_layers': 5, 'gru_learning_rate': 0.0020078229881276242, 'gru_batch_size': 32, 'gru_epochs': 50, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.21179682440521538, 'xgb_max_depth': 6}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0039, MSE=0.0000, RMSE=0.0060, R²=0.9998, MAPE=1.14%\n","XGBoost Validation Metrics: MAE=0.1581, MSE=0.0309, RMSE=0.1757, R²=-4.2486, MAPE=8.88%\n","XGBoost Test Metrics: MAE=0.4271, MSE=0.1885, RMSE=0.4341, R²=-30.2278, MAPE=21.07%\n","GRU Training Time: 47.30 seconds\n","XGBoost Training Time: 0.21 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0068, MSE=0.0001, RMSE=0.0077, R²=0.9997, MAPE=2.77%\n","GRU Validation Metrics: MAE=0.0031, MSE=0.0000, RMSE=0.0039, R²=0.9975, MAPE=0.18%\n","GRU Test Metrics: MAE=0.0156, MSE=0.0003, RMSE=0.0167, R²=0.9537, MAPE=0.76%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:43:10,351] Trial 20 finished with value: 0.17728641450218638 and parameters: {'gru_units': 128, 'gru_layers': 2, 'gru_learning_rate': 0.005905663590645893, 'gru_batch_size': 32, 'gru_epochs': 40, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.04367889215012573, 'xgb_max_depth': 5}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=1.03%\n","XGBoost Validation Metrics: MAE=0.1599, MSE=0.0314, RMSE=0.1773, R²=-4.3439, MAPE=8.98%\n","XGBoost Test Metrics: MAE=0.4289, MSE=0.1900, RMSE=0.4359, R²=-30.4778, MAPE=21.16%\n","GRU Training Time: 24.22 seconds\n","XGBoost Training Time: 0.30 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0043, MSE=0.0000, RMSE=0.0054, R²=0.9998, MAPE=1.50%\n","GRU Validation Metrics: MAE=0.0026, MSE=0.0000, RMSE=0.0032, R²=0.9983, MAPE=0.15%\n","GRU Test Metrics: MAE=0.0116, MSE=0.0002, RMSE=0.0125, R²=0.9739, MAPE=0.57%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:43:54,061] Trial 21 finished with value: 0.17550393263800476 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.005737762592275413, 'gru_batch_size': 32, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.2981324448743009, 'xgb_max_depth': 4}. Best is trial 10 with value: 0.1754788422976193.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0034, MSE=0.0000, RMSE=0.0052, R²=0.9998, MAPE=0.98%\n","XGBoost Validation Metrics: MAE=0.1579, MSE=0.0308, RMSE=0.1755, R²=-4.2370, MAPE=8.86%\n","XGBoost Test Metrics: MAE=0.4269, MSE=0.1883, RMSE=0.4339, R²=-30.1972, MAPE=21.06%\n","GRU Training Time: 41.95 seconds\n","XGBoost Training Time: 0.22 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0060, MSE=0.0001, RMSE=0.0074, R²=0.9997, MAPE=2.19%\n","GRU Validation Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0043, R²=0.9968, MAPE=0.20%\n","GRU Test Metrics: MAE=0.0171, MSE=0.0003, RMSE=0.0183, R²=0.9444, MAPE=0.84%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:44:27,487] Trial 22 finished with value: 0.17547058625969064 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.0047841388058104005, 'gru_batch_size': 32, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.28330166726415457, 'xgb_max_depth': 5}. Best is trial 22 with value: 0.17547058625969064.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=1.00%\n","XGBoost Validation Metrics: MAE=0.1579, MSE=0.0308, RMSE=0.1755, R²=-4.2350, MAPE=8.86%\n","XGBoost Test Metrics: MAE=0.4269, MSE=0.1882, RMSE=0.4339, R²=-30.1919, MAPE=21.06%\n","GRU Training Time: 31.84 seconds\n","XGBoost Training Time: 0.19 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0060, MSE=0.0000, RMSE=0.0069, R²=0.9997, MAPE=2.56%\n","GRU Validation Metrics: MAE=0.0045, MSE=0.0000, RMSE=0.0054, R²=0.9950, MAPE=0.25%\n","GRU Test Metrics: MAE=0.0201, MSE=0.0004, RMSE=0.0209, R²=0.9276, MAPE=0.99%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:45:10,253] Trial 23 finished with value: 0.17562896805770062 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.002916134904063282, 'gru_batch_size': 32, 'gru_epochs': 20, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.21419831571727604, 'xgb_max_depth': 6}. Best is trial 22 with value: 0.17547058625969064.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=1.01%\n","XGBoost Validation Metrics: MAE=0.1581, MSE=0.0308, RMSE=0.1756, R²=-4.2445, MAPE=8.87%\n","XGBoost Test Metrics: MAE=0.4270, MSE=0.1884, RMSE=0.4340, R²=-30.2168, MAPE=21.07%\n","GRU Training Time: 40.59 seconds\n","XGBoost Training Time: 0.22 seconds\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","GRU Train Metrics: MAE=0.0033, MSE=0.0000, RMSE=0.0048, R²=0.9999, MAPE=0.96%\n","GRU Validation Metrics: MAE=0.0066, MSE=0.0001, RMSE=0.0076, R²=0.9901, MAPE=0.37%\n","GRU Test Metrics: MAE=0.0240, MSE=0.0006, RMSE=0.0248, R²=0.8981, MAPE=1.18%\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:45:44,869] Trial 24 finished with value: 0.17564033595483652 and parameters: {'gru_units': 80, 'gru_layers': 2, 'gru_learning_rate': 0.004795867617834519, 'gru_batch_size': 32, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.23089289002867996, 'xgb_max_depth': 5}. Best is trial 22 with value: 0.17547058625969064.\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Train Metrics: MAE=0.0035, MSE=0.0000, RMSE=0.0053, R²=0.9998, MAPE=1.00%\n","XGBoost Validation Metrics: MAE=0.1581, MSE=0.0308, RMSE=0.1756, R²=-4.2451, MAPE=8.87%\n","XGBoost Test Metrics: MAE=0.4270, MSE=0.1884, RMSE=0.4341, R²=-30.2186, MAPE=21.07%\n","GRU Training Time: 32.66 seconds\n","XGBoost Training Time: 0.17 seconds\n","Best hyperparameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.0047841388058104005, 'gru_batch_size': 32, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.28330166726415457, 'xgb_max_depth': 5}\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import time\n","\n","# Retrieve best hyperparameters from Optuna study\n","best_params = study.best_params\n","units = best_params[\"gru_units\"]\n","layers = best_params[\"gru_layers\"]\n","learning_rate = best_params[\"gru_learning_rate\"]\n","batch_size = best_params[\"gru_batch_size\"]\n","epochs = best_params[\"gru_epochs\"]\n","n_estimators = best_params[\"xgb_n_estimators\"]\n","xgb_learning_rate = best_params[\"xgb_learning_rate\"]\n","max_depth = best_params[\"xgb_max_depth\"]\n","\n","# Reshape input for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train final GRU model\n","final_gru, gru_train_time = train_gru(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","\n","\n","# Predictions with timing\n","start_time = time.time()\n","Y_train_pred_gru = final_gru.predict(X_train_r).flatten()\n","gru_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_gru = final_gru.predict(X_val_r).flatten()\n","gru_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_gru = final_gru.predict(X_test_r).flatten()\n","gru_test_pred_time = time.time() - start_time\n","\n","# Prepare data for XGBoost\n","X_train_xgb = np.column_stack([Y_train_pred_gru])\n","X_val_xgb = np.column_stack([Y_val_pred_gru])\n","X_test_xgb = np.column_stack([Y_test_pred_gru])\n","\n","xgb_params = {\n","    \"objective\": \"reg:squarederror\",\n","    \"n_estimators\": n_estimators,\n","    \"learning_rate\": xgb_learning_rate,\n","    \"max_depth\": max_depth,\n","}\n","\n","if tf.config.list_physical_devices(\"GPU\"):\n","    xgb_params[\"tree_method\"] = \"gpu_hist\"\n","\n","# Train final XGBoost model\n","start_time = time.time()\n","final_xgb = xgb.XGBRegressor(**xgb_params)\n","if hasattr(final_xgb, \"fit\") and \"early_stopping_rounds\" in final_xgb.fit.__code__.co_varnames:\n","    final_xgb.fit(X_train_xgb, Y_train, eval_set=[(X_val_xgb, Y_val)], early_stopping_rounds=10, verbose=False)\n","else:\n","    final_xgb.fit(X_train_xgb, Y_train)\n","xgb_train_time = time.time() - start_time\n","\n","# XGBoost Predictions with timing\n","start_time = time.time()\n","Y_train_pred_xgb = final_xgb.predict(X_train_xgb)\n","xgb_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_xgb = final_xgb.predict(X_val_xgb)\n","xgb_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_xgb = final_xgb.predict(X_test_xgb)\n","xgb_test_pred_time = time.time() - start_time\n","\n","# Compute Metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","metrics_train = compute_metrics(Y_train, Y_train_pred_xgb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_xgb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_xgb)\n","\n","# Print results\n","print(\"\\nFinal Model Performance:\")\n","print(f\"GRU Training Time: {gru_train_time:.2f} seconds\")\n","print(f\"XGBoost Training Time: {xgb_train_time:.2f} seconds\\n\")\n","\n","print(f\"GRU Train Prediction Time: {gru_train_pred_time:.4f} seconds\")\n","print(f\"GRU Validation Prediction Time: {gru_val_pred_time:.4f} seconds\")\n","print(f\"GRU Test Prediction Time: {gru_test_pred_time:.4f} seconds\\n\")\n","\n","print(f\"XGBoost Train Prediction Time: {xgb_train_pred_time:.4f} seconds\")\n","print(f\"XGBoost Validation Prediction Time: {xgb_val_pred_time:.4f} seconds\")\n","print(f\"XGBoost Test Prediction Time: {xgb_test_pred_time:.4f} seconds\\n\")\n","\n","print(\"Train Set Metrics:\")\n","print(f\"MAE: {metrics_train[0]:.4f}, MSE: {metrics_train[1]:.4f}, RMSE: {metrics_train[2]:.4f}, R²: {metrics_train[3]:.4f}, MAPE: {metrics_train[4]:.2f}%\")\n","\n","print(\"\\nValidation Set Metrics:\")\n","print(f\"MAE: {metrics_val[0]:.4f}, MSE: {metrics_val[1]:.4f}, RMSE: {metrics_val[2]:.4f}, R²: {metrics_val[3]:.4f}, MAPE: {metrics_val[4]:.2f}%\")\n","\n","print(\"\\nTest Set Metrics:\")\n","print(f\"MAE: {metrics_test[0]:.4f}, MSE: {metrics_test[1]:.4f}, RMSE: {metrics_test[2]:.4f}, R²: {metrics_test[3]:.4f}, MAPE: {metrics_test[4]:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLFUueFyJ3iL","outputId":"f0c1fe2c-7353-4f06-a2bc-53e91213673f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\n","Final Model Performance:\n","GRU Training Time: 137.08 seconds\n","XGBoost Training Time: 0.27 seconds\n","\n","GRU Train Prediction Time: 1.2607 seconds\n","GRU Validation Prediction Time: 0.2136 seconds\n","GRU Test Prediction Time: 0.3495 seconds\n","\n","XGBoost Train Prediction Time: 0.0036 seconds\n","XGBoost Validation Prediction Time: 0.0025 seconds\n","XGBoost Test Prediction Time: 0.0024 seconds\n","\n","Train Set Metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0055, R²: 0.9998, MAPE: 1.03%\n","\n","Validation Set Metrics:\n","MAE: 0.1581, MSE: 0.0309, RMSE: 0.1756, R²: -4.2457, MAPE: 8.87%\n","\n","Test Set Metrics:\n","MAE: 0.4271, MSE: 0.1884, RMSE: 0.4341, R²: -30.2200, MAPE: 21.07%\n"]}]},{"cell_type":"markdown","source":["## BOHB"],"metadata":{"id":"I2QN1olCOV7S"}},{"cell_type":"code","source":["!pip install hpbandster"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CfIgZ-TxN_lJ","outputId":"be52dd9e-6c19-4b53-e2e3-91a2e5932d4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Collecting ConfigSpace (from hpbandster)\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.13.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, ConfigSpace, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=57946f7b24bbd74507c248134e90b0ca8b3329cef06c2394fd0fa59896c92a9b\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=configspace-1.2.1-py3-none-any.whl size=115950 sha256=4077e093225f30338336077e2244f3e013bf90a7b2d2bd628b0551e2dac8198c\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35185 sha256=bf9cbce7fbfab95f8dff04f5872cbba78641574ec005c54e53aa7df2e0ee1736\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster ConfigSpace netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, ConfigSpace, hpbandster\n","Successfully installed ConfigSpace-1.2.1 Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"code","source":["!pip install configSpace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oY0pNeQ_OVN8","outputId":"88818f33-907c-4fb1-de42-fcf36606a050"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: configSpace in /usr/local/lib/python3.11/dist-packages (1.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from configSpace) (1.26.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from configSpace) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from configSpace) (1.13.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from configSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from configSpace) (10.6.0)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import xgboost as xgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)  # Single directional GRU\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# GRU Configurations\n","gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store GRU feature representations\n","gru_features = []\n","\n","for num_layers in gru_layers:\n","    print(f\"Training GRU with {num_layers} layers...\")\n","\n","    gru_model = GRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        gru_model.train()\n","        optimizer.zero_grad()\n","        outputs = gru_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    # Extract Feature Representations\n","    gru_model.eval()\n","    with torch.no_grad():\n","        val_start = time.time()\n","        train_features = gru_model(X_train_torch).numpy()\n","        val_features = gru_model(X_val_torch).numpy()\n","        val_time = time.time() - val_start\n","\n","        test_start = time.time()\n","        test_features = gru_model(X_test_torch).numpy()\n","        test_time = time.time() - test_start\n","\n","    gru_features.append((train_features, val_features, test_features, train_time, val_time, test_time))\n","\n","# Concatenate Features from All Layers\n","final_train_features = np.hstack([feat[0] for feat in gru_features])\n","final_val_features = np.hstack([feat[1] for feat in gru_features])\n","final_test_features = np.hstack([feat[2] for feat in gru_features])\n","\n","# Record Time for Each Stage\n","total_train_time = sum([feat[3] for feat in gru_features])\n","total_val_time = sum([feat[4] for feat in gru_features])\n","total_test_time = sum([feat[5] for feat in gru_features])\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"subsample\", 0.5, 1.0, default_value=0.8))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"colsample_bytree\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for XGBoost\n","class XGBoostWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = xgb.XGBRegressor(\n","            n_estimators=config[\"n_estimators\"],\n","            learning_rate=config[\"learning_rate\"],\n","            max_depth=config[\"max_depth\"],\n","            subsample=config[\"subsample\"],\n","            colsample_bytree=config[\"colsample_bytree\"],\n","            random_state=42\n","        )\n","        model.fit(final_train_features, Y_train)\n","        Y_val_pred = model.predict(final_val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"gru_xgb_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = XGBoostWorker(nameserver=\"127.0.0.2\", run_id=\"gru_xgb_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"gru_xgb_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best XGBoost Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_xgb_model = xgb.XGBRegressor(\n","    n_estimators=best_params[\"n_estimators\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    max_depth=best_params[\"max_depth\"],\n","    subsample=best_params[\"subsample\"],\n","    colsample_bytree=best_params[\"colsample_bytree\"],\n","    random_state=42\n",")\n","\n","best_xgb_model.fit(final_train_features, Y_train)\n","\n","# Predictions\n","Y_train_pred = best_xgb_model.predict(final_train_features)\n","Y_val_pred = best_xgb_model.predict(final_val_features)\n","Y_test_pred = best_xgb_model.predict(final_test_features)\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", total_train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", total_val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", total_test_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ycY6nawdOfSo","outputId":"8c8c3654-865d-442d-b35e-278ff64b240f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training GRU with 2 layers...\n","Training GRU with 3 layers...\n","Training GRU with 5 layers...\n","Train Metrics: (0.0037438697845475276, 3.132773494055797e-05, 0.005597118449752334, 0.9998210058575091, 1.1398412168449807) Time: 31.070932626724243\n","Validation Metrics: (0.15688695301470543, 0.030473581678214477, 0.17456684014501286, -4.18122190409313, 8.80536625751223) Time: 0.13595008850097656\n","Test Metrics: (0.4258555742319095, 0.1873880404953875, 0.43288340288741434, -30.04985167611241, 21.00789560106746) Time: 0.023193836212158203\n"]}]},{"cell_type":"markdown","source":["# Catboost"],"metadata":{"id":"1_BK4_slShJE"}},{"cell_type":"markdown","source":["## Initial"],"metadata":{"id":"jHB3kPV6SjYn"}},{"cell_type":"code","source":["!pip install catboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LEPdDFYURXhm","outputId":"3e6fc861-4056-43d0-85da-d21c4b96e0f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n","Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.7\n"]}]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import GRU, Dense\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Enable GPU for TensorFlow\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train a GRU model on GPU\n","def train_gru(X_train, Y_train, X_val, Y_val, layers):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(GRU(64, return_sequences=(layers > 1), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(GRU(64, return_sequences=(_ < layers - 2)))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        start_time = time.time()\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","        train_time = time.time() - start_time\n","        return model, train_time\n","\n","# Reshaping input for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train 2, 3, and 5-layer GRU models\n","gru_models = {}\n","gru_predictions = {}\n","times = {}\n","\n","for layers in [2, 3, 5]:\n","    model, train_time = train_gru(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    Y_train_pred = model.predict(X_train_r)\n","    Y_val_pred = model.predict(X_val_r)\n","    Y_test_pred = model.predict(X_test_r)\n","\n","    gru_models[layers] = model\n","    gru_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","    times[f'GRU-{layers}'] = train_time\n","\n","# Prepare input for CatBoost\n","X_train_cat = np.column_stack([gru_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_cat = np.column_stack([gru_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_cat = np.column_stack([gru_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train CatBoost model\n","cat_model = CatBoostRegressor(iterations=100, learning_rate=0.05, depth=3, loss_function='RMSE', task_type='GPU', verbose=0)\n","\n","start_time = time.time()\n","cat_model.fit(X_train_cat, Y_train, eval_set=(X_val_cat, Y_val), verbose=0)\n","times['CatBoost'] = time.time() - start_time\n","\n","# Predictions from CatBoost\n","start_time = time.time()\n","Y_train_pred_cat = cat_model.predict(X_train_cat)\n","times['CatBoost Train'] = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_cat = cat_model.predict(X_val_cat)\n","times['CatBoost Validate'] = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_cat = cat_model.predict(X_test_cat)\n","times['CatBoost Test'] = time.time() - start_time\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_cat)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_cat)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_cat)\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print training times\n","print(\"Training Times:\")\n","for model, t in times.items():\n","    print(f\"{model}: {t:.2f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuwKMxYtSOYX","outputId":"7c21644a-70cd-4c1c-f0b1-beb259c7256a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU activated for TensorFlow!\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","Train Metrics: MAE=0.0063, MSE=0.0001, RMSE=0.0092, R²=0.9995, MAPE=2.51%\n","Validation Metrics: MAE=0.1951, MSE=0.0439, RMSE=0.2096, R²=-6.4690, MAPE=10.99%\n","Test Metrics: MAE=0.4641, MSE=0.2214, RMSE=0.4706, R²=-35.6888, MAPE=22.91%\n","Training Times:\n","GRU-2: 63.18 seconds\n","GRU-3: 89.09 seconds\n","GRU-5: 103.86 seconds\n","CatBoost: 0.47 seconds\n","CatBoost Train: 0.01 seconds\n","CatBoost Validate: 0.01 seconds\n","CatBoost Test: 0.01 seconds\n"]}]},{"cell_type":"markdown","source":["## Optuna"],"metadata":{"id":"-QtH4Vwxaduj"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jm9_WkF4aJnw","outputId":"6f297124-8abb-44d4-d39b-bdd29a570bbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import GRU, Dense, Bidirectional, Lambda\n","import catboost as cb\n","import optuna\n","import time\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# Use GPU if available\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","# Function to train Bi-GRU and extract features\n","def train_bi_gru_extract_features(X_train, Y_train, X_val, Y_val, units, layers, learning_rate, batch_size, epochs):\n","    inputs = keras.Input(shape=(X_train.shape[1], 1))\n","    x = inputs\n","    hidden_states = []  # Store outputs from each GRU layer\n","\n","    for _ in range(layers - 1):\n","        x = Bidirectional(GRU(units, return_sequences=True))(x)\n","        # Wrap tf.reduce_mean in a Lambda layer\n","        hidden_states.append(Lambda(lambda x: tf.reduce_mean(x, axis=1))(x))\n","\n","    x = Bidirectional(GRU(units, return_sequences=False))(x)  # Last layer (no return_sequences)\n","    hidden_states.append(x)\n","\n","    model = keras.Model(inputs, Dense(1)(x))\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n","\n","    # Train model\n","    start_time = time.time()\n","    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=0)\n","    train_time = time.time() - start_time\n","\n","    # Extract features\n","    feature_extractor = keras.Model(inputs, hidden_states)  # Extract hidden states\n","    train_features = np.column_stack(feature_extractor.predict(X_train, verbose=0))\n","    val_features = np.column_stack(feature_extractor.predict(X_val, verbose=0))\n","\n","    return model, train_features, val_features, train_time\n","\n","\n","# Optuna objective function\n","def objective(trial):\n","    units = trial.suggest_int(\"gru_units\", 32, 128, step=16)\n","    layers = trial.suggest_categorical(\"gru_layers\", [2, 3, 5])\n","    learning_rate = trial.suggest_loguniform(\"gru_learning_rate\", 1e-4, 1e-2)\n","    batch_size = trial.suggest_categorical(\"gru_batch_size\", [16, 32, 64])\n","    epochs = trial.suggest_int(\"gru_epochs\", 10, 50, step=10)\n","\n","    X_train_r = np.expand_dims(X_train, axis=-1)\n","    X_val_r = np.expand_dims(X_val, axis=-1)\n","\n","    # Train Bi-GRU and extract features\n","    _, train_features, val_features, gru_train_time = train_bi_gru_extract_features(\n","        X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs\n","    )\n","\n","    # Optuna tuning for CatBoost\n","    cat_params = {\n","        \"depth\": trial.suggest_int(\"cat_depth\", 3, 10),\n","        \"learning_rate\": trial.suggest_loguniform(\"cat_learning_rate\", 0.01, 0.3),\n","        \"iterations\": trial.suggest_int(\"cat_iterations\", 50, 200, step=50),\n","        \"loss_function\": \"RMSE\",\n","        \"task_type\": \"CPU\",\n","        \"verbose\": 0\n","    }\n","\n","    # Train CatBoost\n","    start_time = time.time()\n","    cat_model = cb.CatBoostRegressor(**cat_params)\n","    cat_model.fit(train_features, Y_train)\n","    cat_train_time = time.time() - start_time\n","\n","    # Validation Predictions\n","    start_time = time.time()\n","    Y_val_pred = cat_model.predict(val_features)\n","    val_time = time.time() - start_time\n","\n","    # RMSE for Optuna\n","    rmse = np.sqrt(mean_squared_error(Y_val, Y_val_pred))\n","\n","    # Total training time (GRU + CatBoost)\n","    total_train_time = gru_train_time + cat_train_time\n","    print(f\"Total Training Time: {total_train_time:.2f} seconds | Validation Time: {val_time:.2f} seconds\")\n","\n","    return rmse\n","\n","\n","# Run Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=25)\n","\n","# Best hyperparameters\n","print(\"Best hyperparameters:\", study.best_params)\n","\n","# Extract best parameters\n","best_params = study.best_params\n","units = best_params[\"gru_units\"]\n","layers = best_params[\"gru_layers\"]\n","learning_rate = best_params[\"gru_learning_rate\"]\n","batch_size = best_params[\"gru_batch_size\"]\n","epochs = best_params[\"gru_epochs\"]\n","cat_depth = best_params[\"cat_depth\"]\n","cat_learning_rate = best_params[\"cat_learning_rate\"]\n","cat_iterations = best_params[\"cat_iterations\"]\n","\n","# Reshape input\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train final Bi-GRU model\n","final_gru, train_features, val_features, gru_train_time = train_bi_gru_extract_features(\n","    X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs\n",")\n","\n","# Extract test set features correctly\n","feature_extractor = keras.Model(final_gru.input, final_gru.output)\n","test_features = np.column_stack(feature_extractor.predict(X_test_r, verbose=0))\n","\n","# Train final CatBoost model\n","start_time = time.time()\n","final_cb = cb.CatBoostRegressor(\n","    iterations=cat_iterations, learning_rate=cat_learning_rate, depth=cat_depth,\n","    loss_function='RMSE', task_type=\"GPU\", verbose=0\n",")\n","final_cb.fit(train_features, Y_train)\n","cat_train_time = time.time() - start_time\n","\n","# Validation Predictions\n","start_time = time.time()\n","Y_val_pred = final_cb.predict(val_features)\n","val_time = time.time() - start_time\n","\n","# Test Predictions\n","start_time = time.time()\n","Y_test_pred = final_cb.predict(test_features)\n","test_time = time.time() - start_time\n","\n","# Compute Metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","metrics_train = compute_metrics(Y_train, final_cb.predict(train_features))\n","metrics_val = compute_metrics(Y_val, Y_val_pred)\n","metrics_test = compute_metrics(Y_test, Y_test_pred)\n","\n","# Print results\n","print(\"\\nFinal Model Performance:\")\n","print(f\"Training: MAE={metrics_train[0]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation: MAE={metrics_val[0]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Testing: MAE={metrics_test[0]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print training, validation, and test times\n","total_train_time = gru_train_time + cat_train_time\n","print(f\"\\nTotal Training Time: {total_train_time:.2f} seconds\")\n","print(f\"Total Validation Time: {val_time:.2f} seconds\")\n","print(f\"Total Test Time: {test_time:.2f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FnJKxClTdNhl","outputId":"e2c20ac5-55f9-4e62-85dd-b4a39083c955"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-03-06 12:48:50,860] A new study created in memory with name: no-name-516a68b3-4724-47c6-9796-fb6b05d05911\n","[I 2025-03-06 12:57:25,533] Trial 0 finished with value: 0.1664803448006816 and parameters: {'gru_units': 112, 'gru_layers': 3, 'gru_learning_rate': 0.0003135425366613965, 'gru_batch_size': 64, 'gru_epochs': 40, 'cat_depth': 9, 'cat_learning_rate': 0.1409968472824663, 'cat_iterations': 100}. Best is trial 0 with value: 0.1664803448006816.\n"]},{"output_type":"stream","name":"stdout","text":["Total Training Time: 508.90 seconds | Validation Time: 0.02 seconds\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-06 13:02:54,585] Trial 1 finished with value: 0.1656233171239945 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.000887586366166336, 'gru_batch_size': 64, 'gru_epochs': 50, 'cat_depth': 5, 'cat_learning_rate': 0.25139613104222147, 'cat_iterations': 100}. Best is trial 1 with value: 0.1656233171239945.\n"]},{"output_type":"stream","name":"stdout","text":["Total Training Time: 317.86 seconds | Validation Time: 0.01 seconds\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UVydjBG8gx8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import GRU, Dense\n","import catboost as cb\n","import optuna\n","from sklearn.metrics import mean_squared_error\n","import time  # For tracking training time\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Function to train GRU model\n","def train_gru(X_train, Y_train, X_val, Y_val, units, layers, learning_rate, batch_size, epochs):\n","    model = keras.Sequential()\n","\n","    for _ in range(layers - 1):  # All except last layer have return_sequences=True\n","        model.add(GRU(units, return_sequences=True))\n","\n","    model.add(GRU(units))  # Last GRU layer\n","    model.add(Dense(1))  # Output layer\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n","\n","    # Early Stopping Callback\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","\n","    start_time = time.time()  # Start time tracking\n","    history = model.fit(X_train, Y_train,\n","                        validation_data=(X_val, Y_val),\n","                        epochs=epochs,\n","                        batch_size=batch_size,\n","                        verbose=0,\n","                        callbacks=[early_stopping])  # Add early stopping\n","    gru_train_time = time.time() - start_time  # End time tracking\n","\n","    return model, history, gru_train_time\n","\n","\n","# Objective function for Optuna\n","def objective(trial):\n","    units = trial.suggest_int(\"gru_units\", 32, 128, step=16)\n","    layers = trial.suggest_categorical(\"gru_layers\", [2, 3, 5])\n","    learning_rate = trial.suggest_loguniform(\"gru_learning_rate\", 1e-4, 1e-2)\n","    batch_size = trial.suggest_categorical(\"gru_batch_size\", [16, 32, 64])\n","    epochs = trial.suggest_int(\"gru_epochs\", 10, 50, step=10)\n","\n","    # Reshape input for GRU\n","    X_train_r = np.expand_dims(X_train, axis=-1)\n","    X_val_r = np.expand_dims(X_val, axis=-1)\n","\n","    # Train GRU\n","    model, _, gru_train_time = train_gru(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","    Y_val_pred_gru = model.predict(X_val_r).flatten()\n","\n","    # Prepare data for CatBoost\n","    X_val_cat = np.column_stack([Y_val_pred_gru])\n","\n","    cat_params = {\n","        \"depth\": trial.suggest_int(\"cat_depth\", 3, 10),\n","        \"learning_rate\": trial.suggest_loguniform(\"cat_learning_rate\", 0.01, 0.3),\n","        \"iterations\": trial.suggest_int(\"cat_iterations\", 50, 200, step=50),\n","        \"loss_function\": \"RMSE\",\n","        \"verbose\": 0\n","    }\n","\n","    # Train CatBoost\n","    start_time = time.time()  # Start time tracking\n","    cat_model = cb.CatBoostRegressor(**cat_params)\n","    cat_model.fit(X_val_cat, Y_val)\n","    cat_train_time = time.time() - start_time  # End time tracking\n","\n","    # Predict and evaluate\n","    Y_val_pred_cat = cat_model.predict(X_val_cat)\n","    rmse = np.sqrt(mean_squared_error(Y_val, Y_val_pred_cat))\n","\n","    # Print training times\n","    print(f\"GRU Training Time: {gru_train_time:.2f} seconds\")\n","    print(f\"CatBoost Training Time: {cat_train_time:.2f} seconds\")\n","\n","    return rmse\n","\n","# Run Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=25)\n","\n","# Best hyperparameters\n","print(\"Best hyperparameters:\", study.best_params)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xLMCpvdmSorL","outputId":"4dd384a1-428e-4af9-b8f0-0a7211caff16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:41:56,901] A new study created in memory with name: no-name-06ae218d-0fbf-400d-860d-613507dae2ff\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:42:39,068] Trial 0 finished with value: 0.00267030949505933 and parameters: {'gru_units': 32, 'gru_layers': 5, 'gru_learning_rate': 0.009917502789856971, 'gru_batch_size': 64, 'gru_epochs': 50, 'cat_depth': 5, 'cat_learning_rate': 0.0759324952629042, 'cat_iterations': 150}. Best is trial 0 with value: 0.00267030949505933.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 36.72 seconds\n","CatBoost Training Time: 0.17 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:43:47,032] Trial 1 finished with value: 0.0031157414905727708 and parameters: {'gru_units': 32, 'gru_layers': 5, 'gru_learning_rate': 0.007534006617014981, 'gru_batch_size': 32, 'gru_epochs': 10, 'cat_depth': 3, 'cat_learning_rate': 0.053093953225131274, 'cat_iterations': 150}. Best is trial 0 with value: 0.00267030949505933.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 62.68 seconds\n","CatBoost Training Time: 0.08 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:44:26,061] Trial 2 finished with value: 0.02887098483474414 and parameters: {'gru_units': 96, 'gru_layers': 3, 'gru_learning_rate': 0.0005142809580733927, 'gru_batch_size': 32, 'gru_epochs': 50, 'cat_depth': 9, 'cat_learning_rate': 0.020554307850102812, 'cat_iterations': 50}. Best is trial 0 with value: 0.00267030949505933.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 36.10 seconds\n","CatBoost Training Time: 0.27 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:45:01,839] Trial 3 finished with value: 0.04273363021803867 and parameters: {'gru_units': 128, 'gru_layers': 2, 'gru_learning_rate': 0.005098962738243617, 'gru_batch_size': 64, 'gru_epochs': 10, 'cat_depth': 6, 'cat_learning_rate': 0.01245989204294033, 'cat_iterations': 50}. Best is trial 0 with value: 0.00267030949505933.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 34.65 seconds\n","CatBoost Training Time: 0.06 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:46:11,016] Trial 4 finished with value: 0.012903054086998454 and parameters: {'gru_units': 96, 'gru_layers': 5, 'gru_learning_rate': 0.002426499505861083, 'gru_batch_size': 32, 'gru_epochs': 10, 'cat_depth': 4, 'cat_learning_rate': 0.010058662639669851, 'cat_iterations': 200}. Best is trial 0 with value: 0.00267030949505933.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 66.40 seconds\n","CatBoost Training Time: 0.12 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:48:01,859] Trial 5 finished with value: 0.0023376083729354184 and parameters: {'gru_units': 128, 'gru_layers': 3, 'gru_learning_rate': 0.005193966065451501, 'gru_batch_size': 64, 'gru_epochs': 40, 'cat_depth': 6, 'cat_learning_rate': 0.09308910836804565, 'cat_iterations': 150}. Best is trial 5 with value: 0.0023376083729354184.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 108.04 seconds\n","CatBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:48:46,660] Trial 6 finished with value: 0.003111455439617337 and parameters: {'gru_units': 48, 'gru_layers': 5, 'gru_learning_rate': 0.00013189280925000548, 'gru_batch_size': 64, 'gru_epochs': 10, 'cat_depth': 5, 'cat_learning_rate': 0.19944622326165648, 'cat_iterations': 50}. Best is trial 5 with value: 0.0023376083729354184.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 42.42 seconds\n","CatBoost Training Time: 0.06 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:49:37,247] Trial 7 finished with value: 0.0021850019825751877 and parameters: {'gru_units': 80, 'gru_layers': 2, 'gru_learning_rate': 0.00010003438085095368, 'gru_batch_size': 32, 'gru_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.19306514339415773, 'cat_iterations': 150}. Best is trial 7 with value: 0.0021850019825751877.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 48.54 seconds\n","CatBoost Training Time: 0.69 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:50:21,165] Trial 8 finished with value: 0.0024955321261921457 and parameters: {'gru_units': 80, 'gru_layers': 5, 'gru_learning_rate': 0.00032167024278712675, 'gru_batch_size': 32, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.18540356532135419, 'cat_iterations': 100}. Best is trial 7 with value: 0.0021850019825751877.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 40.80 seconds\n","CatBoost Training Time: 0.47 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:51:00,867] Trial 9 finished with value: 0.02060231216254547 and parameters: {'gru_units': 112, 'gru_layers': 3, 'gru_learning_rate': 0.007428529195532369, 'gru_batch_size': 32, 'gru_epochs': 10, 'cat_depth': 7, 'cat_learning_rate': 0.014026822633569589, 'cat_iterations': 100}. Best is trial 7 with value: 0.0021850019825751877.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 36.91 seconds\n","CatBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:51:50,856] Trial 10 finished with value: 0.0021206801466013836 and parameters: {'gru_units': 64, 'gru_layers': 2, 'gru_learning_rate': 0.00010465908850158905, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 8, 'cat_learning_rate': 0.27552817273575436, 'cat_iterations': 200}. Best is trial 10 with value: 0.0021206801466013836.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 47.82 seconds\n","CatBoost Training Time: 0.75 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:52:44,351] Trial 11 finished with value: 0.0020940187132597507 and parameters: {'gru_units': 64, 'gru_layers': 2, 'gru_learning_rate': 0.00010855460070454656, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 8, 'cat_learning_rate': 0.2635109658504371, 'cat_iterations': 200}. Best is trial 11 with value: 0.0020940187132597507.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 51.55 seconds\n","CatBoost Training Time: 0.54 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:53:22,768] Trial 12 finished with value: 0.0020959312714240366 and parameters: {'gru_units': 64, 'gru_layers': 2, 'gru_learning_rate': 0.00024325763871251162, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 8, 'cat_learning_rate': 0.29259570868011936, 'cat_iterations': 200}. Best is trial 11 with value: 0.0020940187132597507.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 36.98 seconds\n","CatBoost Training Time: 0.35 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:53:57,597] Trial 13 finished with value: 0.0021786115974392942 and parameters: {'gru_units': 64, 'gru_layers': 2, 'gru_learning_rate': 0.0003001362715520743, 'gru_batch_size': 16, 'gru_epochs': 40, 'cat_depth': 8, 'cat_learning_rate': 0.12228584949264133, 'cat_iterations': 200}. Best is trial 11 with value: 0.0020940187132597507.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 33.11 seconds\n","CatBoost Training Time: 0.34 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:54:49,934] Trial 14 finished with value: 0.002290923237364905 and parameters: {'gru_units': 64, 'gru_layers': 2, 'gru_learning_rate': 0.0008082102359123332, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 8, 'cat_learning_rate': 0.042374305298652264, 'cat_iterations': 200}. Best is trial 11 with value: 0.0020940187132597507.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 50.60 seconds\n","CatBoost Training Time: 0.35 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:55:32,208] Trial 15 finished with value: 0.002120771856898413 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.0002002523862273772, 'gru_batch_size': 16, 'gru_epochs': 40, 'cat_depth': 7, 'cat_learning_rate': 0.2553022680818554, 'cat_iterations': 200}. Best is trial 11 with value: 0.0020940187132597507.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 40.85 seconds\n","CatBoost Training Time: 0.26 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:56:02,960] Trial 16 finished with value: 0.002092298206171118 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.0010550066385959064, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.1348510337959459, 'cat_iterations': 200}. Best is trial 16 with value: 0.002092298206171118.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 29.15 seconds\n","CatBoost Training Time: 0.53 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:56:40,672] Trial 17 finished with value: 0.0022934792874812174 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.001693065993635321, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.1265167286207441, 'cat_iterations': 100}. Best is trial 16 with value: 0.002092298206171118.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 36.40 seconds\n","CatBoost Training Time: 0.28 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:57:17,401] Trial 18 finished with value: 0.002155200569534757 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.0012544138722481559, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.13433437358824032, 'cat_iterations': 150}. Best is trial 16 with value: 0.002092298206171118.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 33.37 seconds\n","CatBoost Training Time: 0.68 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:57:46,535] Trial 19 finished with value: 0.002392906799336695 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.0006140021670106548, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.03246730658064094, 'cat_iterations': 200}. Best is trial 16 with value: 0.002092298206171118.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 27.27 seconds\n","CatBoost Training Time: 0.82 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:58:38,514] Trial 20 finished with value: 0.002393742635510003 and parameters: {'gru_units': 96, 'gru_layers': 3, 'gru_learning_rate': 0.0026759719162641905, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 7, 'cat_learning_rate': 0.07668819418296255, 'cat_iterations': 150}. Best is trial 16 with value: 0.002092298206171118.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 49.64 seconds\n","CatBoost Training Time: 0.32 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:59:14,156] Trial 21 finished with value: 0.0021174660681564966 and parameters: {'gru_units': 64, 'gru_layers': 2, 'gru_learning_rate': 0.00019202612119867805, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 8, 'cat_learning_rate': 0.25232572495613353, 'cat_iterations': 200}. Best is trial 16 with value: 0.002092298206171118.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 34.29 seconds\n","CatBoost Training Time: 0.33 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 17:59:50,222] Trial 22 finished with value: 0.002076332534611752 and parameters: {'gru_units': 80, 'gru_layers': 2, 'gru_learning_rate': 0.0003900250721681151, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.29891420635308885, 'cat_iterations': 200}. Best is trial 22 with value: 0.002076332534611752.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 34.48 seconds\n","CatBoost Training Time: 0.50 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 18:00:28,501] Trial 23 finished with value: 0.0021561272459764843 and parameters: {'gru_units': 80, 'gru_layers': 2, 'gru_learning_rate': 0.00040180342196885976, 'gru_batch_size': 16, 'gru_epochs': 40, 'cat_depth': 9, 'cat_learning_rate': 0.156804405791138, 'cat_iterations': 200}. Best is trial 22 with value: 0.002076332534611752.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 34.79 seconds\n","CatBoost Training Time: 0.81 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 18:01:22,909] Trial 24 finished with value: 0.002009502470202212 and parameters: {'gru_units': 80, 'gru_layers': 2, 'gru_learning_rate': 0.0010559159791994307, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.20463567886022785, 'cat_iterations': 200}. Best is trial 24 with value: 0.002009502470202212.\n"]},{"output_type":"stream","name":"stdout","text":["GRU Training Time: 52.06 seconds\n","CatBoost Training Time: 0.85 seconds\n","Best hyperparameters: {'gru_units': 80, 'gru_layers': 2, 'gru_learning_rate': 0.0010559159791994307, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.20463567886022785, 'cat_iterations': 200}\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import time\n","from catboost import CatBoostRegressor\n","\n","# Retrieve best hyperparameters from Optuna study\n","best_params = study.best_params\n","units = best_params[\"gru_units\"]\n","layers = best_params[\"gru_layers\"]\n","learning_rate = best_params[\"gru_learning_rate\"]\n","batch_size = best_params[\"gru_batch_size\"]\n","epochs = best_params[\"gru_epochs\"]\n","n_estimators = best_params[\"cat_iterations\"]  # Changed from 'catboost_n_estimators'\n","catboost_learning_rate = best_params[\"cat_learning_rate\"]  # Changed from 'catboost_learning_rate'\n","depth = best_params[\"cat_depth\"]  # Changed from 'catboost_depth'\n","\n","# Reshape input for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train final GRU model\n","final_gru, _, gru_train_time = train_gru(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","\n","# Predictions with timing\n","start_time = time.time()\n","Y_train_pred_gru = final_gru.predict(X_train_r).flatten()\n","gru_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_gru = final_gru.predict(X_val_r).flatten()\n","gru_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_gru = final_gru.predict(X_test_r).flatten()\n","gru_test_pred_time = time.time() - start_time\n","\n","# Prepare data for CatBoost\n","X_train_cb = np.column_stack([Y_train_pred_gru])\n","X_val_cb = np.column_stack([Y_val_pred_gru])\n","X_test_cb = np.column_stack([Y_test_pred_gru])\n","\n","# Train final CatBoost model\n","start_time = time.time()\n","final_cb = CatBoostRegressor(iterations=n_estimators, learning_rate=catboost_learning_rate, depth=depth, loss_function='RMSE', verbose=0)\n","final_cb.fit(X_train_cb, Y_train, eval_set=(X_val_cb, Y_val), early_stopping_rounds=10)\n","cb_train_time = time.time() - start_time\n","\n","# CatBoost Predictions with timing\n","start_time = time.time()\n","Y_train_pred_cb = final_cb.predict(X_train_cb)\n","cb_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_cb = final_cb.predict(X_val_cb)\n","cb_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_cb = final_cb.predict(X_test_cb)\n","cb_test_pred_time = time.time() - start_time\n","\n","# Compute Metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","metrics_train = compute_metrics(Y_train, Y_train_pred_cb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_cb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_cb)\n","\n","# Print results\n","print(\"\\nFinal Model Performance:\")\n","print(f\"GRU Training Time: {gru_train_time:.2f} seconds\")\n","print(f\"CatBoost Training Time: {cb_train_time:.2f} seconds\\n\")\n","\n","print(f\"GRU Train Prediction Time: {gru_train_pred_time:.4f} seconds\")\n","print(f\"GRU Validation Prediction Time: {gru_val_pred_time:.4f} seconds\")\n","print(f\"GRU Test Prediction Time: {gru_test_pred_time:.4f} seconds\\n\")\n","\n","print(f\"CatBoost Train Prediction Time: {cb_train_pred_time:.4f} seconds\")\n","print(f\"CatBoost Validation Prediction Time: {cb_val_pred_time:.4f} seconds\")\n","print(f\"CatBoost Test Prediction Time: {cb_test_pred_time:.4f} seconds\\n\")\n","\n","print(\"Train Set Metrics:\")\n","print(f\"MAE: {metrics_train[0]:.4f}, MSE: {metrics_train[1]:.4f}, RMSE: {metrics_train[2]:.4f}, R²: {metrics_train[3]:.4f}, MAPE: {metrics_train[4]:.2f}%\")\n","\n","print(\"\\nValidation Set Metrics:\")\n","print(f\"MAE: {metrics_val[0]:.4f}, MSE: {metrics_val[1]:.4f}, RMSE: {metrics_val[2]:.4f}, R²: {metrics_val[3]:.4f}, MAPE: {metrics_val[4]:.2f}%\")\n","\n","print(\"\\nTest Set Metrics:\")\n","print(f\"MAE: {metrics_test[0]:.4f}, MSE: {metrics_test[1]:.4f}, RMSE: {metrics_test[2]:.4f}, R²: {metrics_test[3]:.4f}, MAPE: {metrics_test[4]:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oorf-zN1Vk6r","outputId":"7da96ece-5fae-446f-f1c1-900c7163ebe7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\n","Final Model Performance:\n","GRU Training Time: 59.43 seconds\n","CatBoost Training Time: 0.60 seconds\n","\n","GRU Train Prediction Time: 1.4048 seconds\n","GRU Validation Prediction Time: 0.3603 seconds\n","GRU Test Prediction Time: 0.3608 seconds\n","\n","CatBoost Train Prediction Time: 0.0054 seconds\n","CatBoost Validation Prediction Time: 0.0020 seconds\n","CatBoost Test Prediction Time: 0.0019 seconds\n","\n","Train Set Metrics:\n","MAE: 0.0037, MSE: 0.0000, RMSE: 0.0056, R²: 0.9998, MAPE: 1.07%\n","\n","Validation Set Metrics:\n","MAE: 0.1578, MSE: 0.0308, RMSE: 0.1754, R²: -4.2294, MAPE: 8.86%\n","\n","Test Set Metrics:\n","MAE: 0.4268, MSE: 0.1882, RMSE: 0.4338, R²: -30.1771, MAPE: 21.05%\n"]}]},{"cell_type":"markdown","source":["## BOHB"],"metadata":{"id":"gOv3usTlaj_-"}},{"cell_type":"code","source":["!pip install ConfigSpace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RBVfbJ1Ogujp","outputId":"19dacf51-6c46-4624-d33b-74a6329608bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.26.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.13.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (10.6.0)\n","Building wheels for collected packages: ConfigSpace\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=configspace-1.2.1-py3-none-any.whl size=115950 sha256=d4d8f65b1292c5e26d5c6de6814b231247303a21ebd7e26ed82b204ada3dc8f6\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","Successfully built ConfigSpace\n","Installing collected packages: ConfigSpace\n","Successfully installed ConfigSpace-1.2.1\n"]}]},{"cell_type":"code","source":["!pip install hpbandster"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sYnNb0-gtlR","outputId":"15274900-96b3-41a4-9ada-ca0b24f1b5ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: ConfigSpace in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.13.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=ec1aa65b47de06d1aeff5e2f1a39d99806ce473191a4a833a9e7036230d0f30c\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35184 sha256=7f9f0212601c19d4fa4c68914941c9311418929c053380dbc816b0294c55eee3\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, hpbandster\n","Successfully installed Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# GRU Configurations\n","gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","gru_features = []\n","\n","for num_layers in gru_layers:\n","    print(f\"Training GRU with {num_layers} layers...\")\n","\n","    gru_model = GRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        gru_model.train()\n","        optimizer.zero_grad()\n","        outputs = gru_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    # Extract Feature Representations\n","    gru_model.eval()\n","    with torch.no_grad():\n","        val_start = time.time()\n","        train_features = gru_model(X_train_torch).numpy()\n","        val_features = gru_model(X_val_torch).numpy()\n","        val_time = time.time() - val_start\n","\n","        test_start = time.time()\n","        test_features = gru_model(X_test_torch).numpy()\n","        test_time = time.time() - test_start\n","\n","    gru_features.append((train_features, val_features, test_features, train_time, val_time, test_time))\n","\n","# Concatenate Features from All Layers\n","final_train_features = np.hstack([feat[0] for feat in gru_features])\n","final_val_features = np.hstack([feat[1] for feat in gru_features])\n","final_test_features = np.hstack([feat[2] for feat in gru_features])\n","\n","total_train_time = sum([feat[3] for feat in gru_features])\n","total_val_time = sum([feat[4] for feat in gru_features])\n","total_test_time = sum([feat[5] for feat in gru_features])\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"subsample\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for CatBoost\n","class CatBoostWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = CatBoostRegressor(\n","            iterations=config[\"n_estimators\"],\n","            learning_rate=config[\"learning_rate\"],\n","            depth=config[\"depth\"],\n","            subsample=config[\"subsample\"],\n","            loss_function='RMSE',\n","            verbose=False,\n","            random_seed=42\n","        )\n","        model.fit(final_train_features, Y_train)\n","        Y_val_pred = model.predict(final_val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"gru_catboost_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = CatBoostWorker(nameserver=\"127.0.0.2\", run_id=\"gru_catboost_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"gru_catboost_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best CatBoost Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_catboost_model = CatBoostRegressor(\n","    iterations=best_params[\"n_estimators\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    depth=best_params[\"depth\"],\n","    subsample=best_params[\"subsample\"],\n","    loss_function='RMSE',\n","    verbose=False,\n","    random_seed=42\n",")\n","\n","best_catboost_model.fit(final_train_features, Y_train)\n","\n","# Predictions\n","Y_train_pred = best_catboost_model.predict(final_train_features)\n","Y_val_pred = best_catboost_model.predict(final_val_features)\n","Y_test_pred = best_catboost_model.predict(final_test_features)\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", total_train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", total_val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", total_test_time)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f44E-CTgZnyt","outputId":"9138cdec-9d2d-4060-fc49-8f2fb8319e70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training GRU with 2 layers...\n","Training GRU with 3 layers...\n","Training GRU with 5 layers...\n","Train Metrics: (0.0034434628552621536, 2.7629727449693964e-05, 0.005256398714870663, 0.9998421347926526, 0.9821947264178991) Time: 42.73035764694214\n","Validation Metrics: (0.15749539305215066, 0.03066682252424607, 0.17511945215836552, -4.214077369354706, 8.840234429250694) Time: 0.22542214393615723\n","Test Metrics: (0.42647049762982914, 0.18791215573963216, 0.4334883570981257, -30.136696602565372, 21.038451410917933) Time: 0.0288846492767334\n"]}]},{"cell_type":"markdown","source":["# LightBoost"],"metadata":{"id":"JrpYo1dbamCt"}},{"cell_type":"markdown","source":["## Initial"],"metadata":{"id":"94koI_eFao0r"}},{"cell_type":"code","source":["!pip install lightgbm"],"metadata":{"id":"pFqcK9vhaqnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import lightgbm as lgb\n","import pandas as pd\n","import time\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h0)\n","        return out[:, -1, :]\n","\n","# Set Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).to(device)\n","\n","# Store embeddings for LGBM\n","train_embeddings, val_embeddings, test_embeddings = [], [], []\n","train_time, val_time, test_time = 0, 0, 0\n","\n","# Train multiple GRU models\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining GRU with {num_layers} layers...\")\n","    model = GRUModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Train GRU\n","    start_time = time.time()\n","    model.train()\n","    for epoch in range(num_epochs):\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time += time.time() - start_time\n","\n","    # Extract embeddings\n","    model.eval()\n","    with torch.no_grad():\n","        start_time = time.time()\n","        train_embed = model(X_train_torch).cpu().numpy()\n","        val_embed = model(X_val_torch).cpu().numpy()\n","        test_embed = model(X_test_torch).cpu().numpy()\n","        val_time += time.time() - start_time\n","        test_time += time.time() - start_time\n","\n","    train_embeddings.append(train_embed)\n","    val_embeddings.append(val_embed)\n","    test_embeddings.append(test_embed)\n","\n","# Concatenate embeddings from all GRU models\n","X_train_lgb = np.concatenate(train_embeddings, axis=1)\n","X_val_lgb = np.concatenate(val_embeddings, axis=1)\n","X_test_lgb = np.concatenate(test_embeddings, axis=1)\n","\n","# Ensure correct label shape\n","Y_train_lgb = Y_train.values.flatten()\n","Y_val_lgb = Y_val.values.flatten()\n","Y_test_lgb = Y_test.values.flatten()\n","\n","# Train LightGBM on GRU embeddings\n","print(\"\\nTraining LightGBM on Combined GRU Embeddings...\")\n","start_time = time.time()\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train_lgb)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val_lgb, reference=lgb_train)\n","\n","lgb_params = {\n","    \"objective\": \"regression\",\n","    \"metric\": \"rmse\",\n","    \"boosting_type\": \"gbdt\",\n","    \"learning_rate\": 0.05,\n","    \"num_leaves\": 31\n","}\n","\n","lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","train_time += time.time() - start_time\n","\n","# Predictions\n","start_time = time.time()\n","train_pred_lgb = lgb_model.predict(X_train_lgb)\n","val_pred_lgb = lgb_model.predict(X_val_lgb)\n","test_pred_lgb = lgb_model.predict(X_test_lgb)\n","test_time += time.time() - start_time\n","\n","# Compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100  # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# Store results\n","results_df = pd.DataFrame([\n","    [\"GRU(2,3,5) + LGBM\", \"Train\", *compute_metrics(Y_train_lgb, train_pred_lgb), train_time],\n","    [\"GRU(2,3,5) + LGBM\", \"Validation\", *compute_metrics(Y_val_lgb, val_pred_lgb), val_time],\n","    [\"GRU(2,3,5) + LGBM\", \"Test\", *compute_metrics(Y_test_lgb, test_pred_lgb), test_time]\n","], columns=[\"Model\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\", \"Time (s)\"])\n","\n","print(\"\\nFinal Model Performance\\n\")\n","print(results_df.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82DjHevvhlS6","outputId":"2ac4d574-98d1-4150-a2e7-84147190f13d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training GRU with 2 layers...\n","\n","Training GRU with 3 layers...\n","\n","Training GRU with 5 layers...\n","\n","Training LightGBM on Combined GRU Embeddings...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022234 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 48960\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 192\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0325761\tvalid_1's rmse: 0.25933\n","[100]\ttraining's rmse: 0.00511984\tvalid_1's rmse: 0.180597\n","[150]\ttraining's rmse: 0.00442671\tvalid_1's rmse: 0.174659\n","[200]\ttraining's rmse: 0.00439171\tvalid_1's rmse: 0.174065\n","\n","Final Model Performance\n","\n","            Model    Dataset      MAE      MSE     RMSE         R²      MAPE  Time (s)\n","GRU(2,3,5) + LGBM      Train 0.002933 0.000019 0.004392   0.999890  0.873499 45.982399\n","GRU(2,3,5) + LGBM Validation 0.156337 0.030299 0.174065  -4.151482  8.773841  0.236133\n","GRU(2,3,5) + LGBM       Test 0.425297 0.186912 0.432334 -29.971055 20.980133  0.344324\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import lightgbm as lgb\n","import pandas as pd\n","import time\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h0)\n","        return out[:, -1, :]\n","\n","# Set Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).to(device)\n","\n","# Store embeddings for LGBM\n","train_embeddings, val_embeddings, test_embeddings = [], [], []\n","train_time, val_time, test_time = 0, 0, 0\n","\n","# Train multiple GRU models\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining GRU with {num_layers} layers...\")\n","    model = GRUModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Train GRU\n","    start_time = time.time()\n","    model.train()\n","    for epoch in range(num_epochs):\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}\")\n","\n","    train_time += time.time() - start_time\n","\n","    # Extract embeddings\n","    model.eval()\n","    with torch.no_grad():\n","        start_time = time.time()\n","        train_embed = model(X_train_torch).cpu().numpy()\n","        val_embed = model(X_val_torch).cpu().numpy()\n","        test_embed = model(X_test_torch).cpu().numpy()\n","        val_time += time.time() - start_time\n","        test_time += time.time() - start_time\n","\n","    train_embeddings.append(train_embed)\n","    val_embeddings.append(val_embed)\n","    test_embeddings.append(test_embed)\n","\n","# Concatenate embeddings from all GRU models\n","X_train_lgb = np.concatenate(train_embeddings, axis=1)\n","X_val_lgb = np.concatenate(val_embeddings, axis=1)\n","X_test_lgb = np.concatenate(test_embeddings, axis=1)\n","\n","# Ensure correct label shape\n","Y_train_lgb = Y_train.values.flatten()\n","Y_val_lgb = Y_val.values.flatten()\n","Y_test_lgb = Y_test.values.flatten()\n","\n","# Train LightGBM on GRU embeddings\n","print(\"\\nTraining LightGBM on Combined GRU Embeddings...\")\n","start_time = time.time()\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train_lgb)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val_lgb, reference=lgb_train)\n","\n","lgb_params = {\n","    \"objective\": \"regression\",\n","    \"metric\": \"rmse\",\n","    \"boosting_type\": \"gbdt\",\n","    \"learning_rate\": 0.05,\n","    \"num_leaves\": 31\n","}\n","\n","lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","train_time += time.time() - start_time\n","\n","# Predictions\n","start_time = time.time()\n","train_pred_lgb = lgb_model.predict(X_train_lgb)\n","val_pred_lgb = lgb_model.predict(X_val_lgb)\n","test_pred_lgb = lgb_model.predict(X_test_lgb)\n","test_time += time.time() - start_time\n","\n","# Compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100  # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# Store results\n","results_df = pd.DataFrame([\n","    [\"GRU(2,3,5) + LGBM\", \"Train\", *compute_metrics(Y_train_lgb, train_pred_lgb), train_time],\n","    [\"GRU(2,3,5) + LGBM\", \"Validation\", *compute_metrics(Y_val_lgb, val_pred_lgb), val_time],\n","    [\"GRU(2,3,5) + LGBM\", \"Test\", *compute_metrics(Y_test_lgb, test_pred_lgb), test_time]\n","], columns=[\"Model\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\", \"Time (s)\"])\n","\n","print(\"\\nFinal Model Performance\\n\")\n","print(results_df.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-x0RMlMPodks","outputId":"80668a42-65f5-4ff7-a54a-b10bf082ce89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training GRU with 2 layers...\n","Epoch [1/100], Loss: 0.139056\n","Epoch [2/100], Loss: 0.137580\n","Epoch [3/100], Loss: 0.136117\n","Epoch [4/100], Loss: 0.134659\n","Epoch [5/100], Loss: 0.133197\n","Epoch [6/100], Loss: 0.131725\n","Epoch [7/100], Loss: 0.130238\n","Epoch [8/100], Loss: 0.128728\n","Epoch [9/100], Loss: 0.127193\n","Epoch [10/100], Loss: 0.125627\n","Epoch [11/100], Loss: 0.124027\n","Epoch [12/100], Loss: 0.122389\n","Epoch [13/100], Loss: 0.120709\n","Epoch [14/100], Loss: 0.118984\n","Epoch [15/100], Loss: 0.117212\n","Epoch [16/100], Loss: 0.115389\n","Epoch [17/100], Loss: 0.113513\n","Epoch [18/100], Loss: 0.111583\n","Epoch [19/100], Loss: 0.109598\n","Epoch [20/100], Loss: 0.107555\n","Epoch [21/100], Loss: 0.105455\n","Epoch [22/100], Loss: 0.103297\n","Epoch [23/100], Loss: 0.101081\n","Epoch [24/100], Loss: 0.098809\n","Epoch [25/100], Loss: 0.096482\n","Epoch [26/100], Loss: 0.094102\n","Epoch [27/100], Loss: 0.091672\n","Epoch [28/100], Loss: 0.089194\n","Epoch [29/100], Loss: 0.086673\n","Epoch [30/100], Loss: 0.084113\n","Epoch [31/100], Loss: 0.081520\n","Epoch [32/100], Loss: 0.078901\n","Epoch [33/100], Loss: 0.076261\n","Epoch [34/100], Loss: 0.073609\n","Epoch [35/100], Loss: 0.070954\n","Epoch [36/100], Loss: 0.068303\n","Epoch [37/100], Loss: 0.065667\n","Epoch [38/100], Loss: 0.063056\n","Epoch [39/100], Loss: 0.060479\n","Epoch [40/100], Loss: 0.057948\n","Epoch [41/100], Loss: 0.055472\n","Epoch [42/100], Loss: 0.053063\n","Epoch [43/100], Loss: 0.050729\n","Epoch [44/100], Loss: 0.048481\n","Epoch [45/100], Loss: 0.046326\n","Epoch [46/100], Loss: 0.044272\n","Epoch [47/100], Loss: 0.042326\n","Epoch [48/100], Loss: 0.040491\n","Epoch [49/100], Loss: 0.038770\n","Epoch [50/100], Loss: 0.037166\n","Epoch [51/100], Loss: 0.035677\n","Epoch [52/100], Loss: 0.034300\n","Epoch [53/100], Loss: 0.033032\n","Epoch [54/100], Loss: 0.031866\n","Epoch [55/100], Loss: 0.030795\n","Epoch [56/100], Loss: 0.029810\n","Epoch [57/100], Loss: 0.028901\n","Epoch [58/100], Loss: 0.028059\n","Epoch [59/100], Loss: 0.027274\n","Epoch [60/100], Loss: 0.026535\n","Epoch [61/100], Loss: 0.025834\n","Epoch [62/100], Loss: 0.025162\n","Epoch [63/100], Loss: 0.024511\n","Epoch [64/100], Loss: 0.023875\n","Epoch [65/100], Loss: 0.023251\n","Epoch [66/100], Loss: 0.022633\n","Epoch [67/100], Loss: 0.022021\n","Epoch [68/100], Loss: 0.021411\n","Epoch [69/100], Loss: 0.020804\n","Epoch [70/100], Loss: 0.020201\n","Epoch [71/100], Loss: 0.019601\n","Epoch [72/100], Loss: 0.019005\n","Epoch [73/100], Loss: 0.018416\n","Epoch [74/100], Loss: 0.017835\n","Epoch [75/100], Loss: 0.017263\n","Epoch [76/100], Loss: 0.016701\n","Epoch [77/100], Loss: 0.016151\n","Epoch [78/100], Loss: 0.015614\n","Epoch [79/100], Loss: 0.015090\n","Epoch [80/100], Loss: 0.014581\n","Epoch [81/100], Loss: 0.014085\n","Epoch [82/100], Loss: 0.013604\n","Epoch [83/100], Loss: 0.013138\n","Epoch [84/100], Loss: 0.012685\n","Epoch [85/100], Loss: 0.012246\n","Epoch [86/100], Loss: 0.011820\n","Epoch [87/100], Loss: 0.011406\n","Epoch [88/100], Loss: 0.011005\n","Epoch [89/100], Loss: 0.010615\n","Epoch [90/100], Loss: 0.010235\n","Epoch [91/100], Loss: 0.009866\n","Epoch [92/100], Loss: 0.009507\n","Epoch [93/100], Loss: 0.009158\n","Epoch [94/100], Loss: 0.008817\n","Epoch [95/100], Loss: 0.008486\n","Epoch [96/100], Loss: 0.008163\n","Epoch [97/100], Loss: 0.007849\n","Epoch [98/100], Loss: 0.007544\n","Epoch [99/100], Loss: 0.007246\n","Epoch [100/100], Loss: 0.006958\n","\n","Training GRU with 3 layers...\n","Epoch [1/100], Loss: 0.142752\n","Epoch [2/100], Loss: 0.141253\n","Epoch [3/100], Loss: 0.139781\n","Epoch [4/100], Loss: 0.138323\n","Epoch [5/100], Loss: 0.136867\n","Epoch [6/100], Loss: 0.135402\n","Epoch [7/100], Loss: 0.133917\n","Epoch [8/100], Loss: 0.132403\n","Epoch [9/100], Loss: 0.130851\n","Epoch [10/100], Loss: 0.129248\n","Epoch [11/100], Loss: 0.127582\n","Epoch [12/100], Loss: 0.125842\n","Epoch [13/100], Loss: 0.124016\n","Epoch [14/100], Loss: 0.122092\n","Epoch [15/100], Loss: 0.120059\n","Epoch [16/100], Loss: 0.117907\n","Epoch [17/100], Loss: 0.115628\n","Epoch [18/100], Loss: 0.113214\n","Epoch [19/100], Loss: 0.110655\n","Epoch [20/100], Loss: 0.107946\n","Epoch [21/100], Loss: 0.105080\n","Epoch [22/100], Loss: 0.102053\n","Epoch [23/100], Loss: 0.098861\n","Epoch [24/100], Loss: 0.095505\n","Epoch [25/100], Loss: 0.091987\n","Epoch [26/100], Loss: 0.088315\n","Epoch [27/100], Loss: 0.084501\n","Epoch [28/100], Loss: 0.080564\n","Epoch [29/100], Loss: 0.076531\n","Epoch [30/100], Loss: 0.072435\n","Epoch [31/100], Loss: 0.068319\n","Epoch [32/100], Loss: 0.064233\n","Epoch [33/100], Loss: 0.060238\n","Epoch [34/100], Loss: 0.056398\n","Epoch [35/100], Loss: 0.052784\n","Epoch [36/100], Loss: 0.049464\n","Epoch [37/100], Loss: 0.046502\n","Epoch [38/100], Loss: 0.043945\n","Epoch [39/100], Loss: 0.041820\n","Epoch [40/100], Loss: 0.040122\n","Epoch [41/100], Loss: 0.038810\n","Epoch [42/100], Loss: 0.037812\n","Epoch [43/100], Loss: 0.037031\n","Epoch [44/100], Loss: 0.036363\n","Epoch [45/100], Loss: 0.035710\n","Epoch [46/100], Loss: 0.034994\n","Epoch [47/100], Loss: 0.034161\n","Epoch [48/100], Loss: 0.033185\n","Epoch [49/100], Loss: 0.032063\n","Epoch [50/100], Loss: 0.030810\n","Epoch [51/100], Loss: 0.029457\n","Epoch [52/100], Loss: 0.028041\n","Epoch [53/100], Loss: 0.026603\n","Epoch [54/100], Loss: 0.025179\n","Epoch [55/100], Loss: 0.023800\n","Epoch [56/100], Loss: 0.022490\n","Epoch [57/100], Loss: 0.021261\n","Epoch [58/100], Loss: 0.020119\n","Epoch [59/100], Loss: 0.019059\n","Epoch [60/100], Loss: 0.018072\n","Epoch [61/100], Loss: 0.017147\n","Epoch [62/100], Loss: 0.016267\n","Epoch [63/100], Loss: 0.015418\n","Epoch [64/100], Loss: 0.014589\n","Epoch [65/100], Loss: 0.013771\n","Epoch [66/100], Loss: 0.012957\n","Epoch [67/100], Loss: 0.012146\n","Epoch [68/100], Loss: 0.011341\n","Epoch [69/100], Loss: 0.010545\n","Epoch [70/100], Loss: 0.009767\n","Epoch [71/100], Loss: 0.009012\n","Epoch [72/100], Loss: 0.008288\n","Epoch [73/100], Loss: 0.007603\n","Epoch [74/100], Loss: 0.006960\n","Epoch [75/100], Loss: 0.006362\n","Epoch [76/100], Loss: 0.005810\n","Epoch [77/100], Loss: 0.005304\n","Epoch [78/100], Loss: 0.004840\n","Epoch [79/100], Loss: 0.004416\n","Epoch [80/100], Loss: 0.004027\n","Epoch [81/100], Loss: 0.003669\n","Epoch [82/100], Loss: 0.003341\n","Epoch [83/100], Loss: 0.003038\n","Epoch [84/100], Loss: 0.002760\n","Epoch [85/100], Loss: 0.002506\n","Epoch [86/100], Loss: 0.002275\n","Epoch [87/100], Loss: 0.002067\n","Epoch [88/100], Loss: 0.001883\n","Epoch [89/100], Loss: 0.001720\n","Epoch [90/100], Loss: 0.001580\n","Epoch [91/100], Loss: 0.001460\n","Epoch [92/100], Loss: 0.001359\n","Epoch [93/100], Loss: 0.001276\n","Epoch [94/100], Loss: 0.001208\n","Epoch [95/100], Loss: 0.001153\n","Epoch [96/100], Loss: 0.001108\n","Epoch [97/100], Loss: 0.001073\n","Epoch [98/100], Loss: 0.001044\n","Epoch [99/100], Loss: 0.001021\n","Epoch [100/100], Loss: 0.001002\n","\n","Training GRU with 5 layers...\n","Epoch [1/100], Loss: 0.142282\n","Epoch [2/100], Loss: 0.140876\n","Epoch [3/100], Loss: 0.139490\n","Epoch [4/100], Loss: 0.138114\n","Epoch [5/100], Loss: 0.136736\n","Epoch [6/100], Loss: 0.135342\n","Epoch [7/100], Loss: 0.133920\n","Epoch [8/100], Loss: 0.132457\n","Epoch [9/100], Loss: 0.130940\n","Epoch [10/100], Loss: 0.129355\n","Epoch [11/100], Loss: 0.127686\n","Epoch [12/100], Loss: 0.125917\n","Epoch [13/100], Loss: 0.124029\n","Epoch [14/100], Loss: 0.122000\n","Epoch [15/100], Loss: 0.119808\n","Epoch [16/100], Loss: 0.117430\n","Epoch [17/100], Loss: 0.114836\n","Epoch [18/100], Loss: 0.111998\n","Epoch [19/100], Loss: 0.108887\n","Epoch [20/100], Loss: 0.105477\n","Epoch [21/100], Loss: 0.101746\n","Epoch [22/100], Loss: 0.097684\n","Epoch [23/100], Loss: 0.093296\n","Epoch [24/100], Loss: 0.088613\n","Epoch [25/100], Loss: 0.083705\n","Epoch [26/100], Loss: 0.078688\n","Epoch [27/100], Loss: 0.073740\n","Epoch [28/100], Loss: 0.069108\n","Epoch [29/100], Loss: 0.065095\n","Epoch [30/100], Loss: 0.062015\n","Epoch [31/100], Loss: 0.060084\n","Epoch [32/100], Loss: 0.059269\n","Epoch [33/100], Loss: 0.059207\n","Epoch [34/100], Loss: 0.059326\n","Epoch [35/100], Loss: 0.059086\n","Epoch [36/100], Loss: 0.058160\n","Epoch [37/100], Loss: 0.056484\n","Epoch [38/100], Loss: 0.054202\n","Epoch [39/100], Loss: 0.051566\n","Epoch [40/100], Loss: 0.048846\n","Epoch [41/100], Loss: 0.046270\n","Epoch [42/100], Loss: 0.043975\n","Epoch [43/100], Loss: 0.042007\n","Epoch [44/100], Loss: 0.040325\n","Epoch [45/100], Loss: 0.038835\n","Epoch [46/100], Loss: 0.037424\n","Epoch [47/100], Loss: 0.035979\n","Epoch [48/100], Loss: 0.034416\n","Epoch [49/100], Loss: 0.032685\n","Epoch [50/100], Loss: 0.030773\n","Epoch [51/100], Loss: 0.028708\n","Epoch [52/100], Loss: 0.026545\n","Epoch [53/100], Loss: 0.024356\n","Epoch [54/100], Loss: 0.022216\n","Epoch [55/100], Loss: 0.020182\n","Epoch [56/100], Loss: 0.018281\n","Epoch [57/100], Loss: 0.016509\n","Epoch [58/100], Loss: 0.014836\n","Epoch [59/100], Loss: 0.013227\n","Epoch [60/100], Loss: 0.011657\n","Epoch [61/100], Loss: 0.010129\n","Epoch [62/100], Loss: 0.008671\n","Epoch [63/100], Loss: 0.007324\n","Epoch [64/100], Loss: 0.006130\n","Epoch [65/100], Loss: 0.005117\n","Epoch [66/100], Loss: 0.004292\n","Epoch [67/100], Loss: 0.003644\n","Epoch [68/100], Loss: 0.003149\n","Epoch [69/100], Loss: 0.002778\n","Epoch [70/100], Loss: 0.002504\n","Epoch [71/100], Loss: 0.002308\n","Epoch [72/100], Loss: 0.002178\n","Epoch [73/100], Loss: 0.002104\n","Epoch [74/100], Loss: 0.002081\n","Epoch [75/100], Loss: 0.002101\n","Epoch [76/100], Loss: 0.002155\n","Epoch [77/100], Loss: 0.002230\n","Epoch [78/100], Loss: 0.002313\n","Epoch [79/100], Loss: 0.002393\n","Epoch [80/100], Loss: 0.002460\n","Epoch [81/100], Loss: 0.002506\n","Epoch [82/100], Loss: 0.002527\n","Epoch [83/100], Loss: 0.002525\n","Epoch [84/100], Loss: 0.002500\n","Epoch [85/100], Loss: 0.002457\n","Epoch [86/100], Loss: 0.002400\n","Epoch [87/100], Loss: 0.002334\n","Epoch [88/100], Loss: 0.002263\n","Epoch [89/100], Loss: 0.002190\n","Epoch [90/100], Loss: 0.002117\n","Epoch [91/100], Loss: 0.002045\n","Epoch [92/100], Loss: 0.001975\n","Epoch [93/100], Loss: 0.001908\n","Epoch [94/100], Loss: 0.001844\n","Epoch [95/100], Loss: 0.001786\n","Epoch [96/100], Loss: 0.001734\n","Epoch [97/100], Loss: 0.001689\n","Epoch [98/100], Loss: 0.001651\n","Epoch [99/100], Loss: 0.001621\n","Epoch [100/100], Loss: 0.001598\n","\n","Training LightGBM on Combined GRU Embeddings...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021397 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 48960\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 192\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.032543\tvalid_1's rmse: 0.259324\n","[100]\ttraining's rmse: 0.00489494\tvalid_1's rmse: 0.180593\n","[150]\ttraining's rmse: 0.00415222\tvalid_1's rmse: 0.174704\n","[200]\ttraining's rmse: 0.00410743\tvalid_1's rmse: 0.174099\n","\n","Final Model Performance\n","\n","            Model    Dataset      MAE      MSE     RMSE         R²      MAPE  Time (s)\n","GRU(2,3,5) + LGBM      Train 0.002792 0.000017 0.004107   0.999904  0.844790 46.521070\n","GRU(2,3,5) + LGBM Validation 0.156374 0.030311 0.174099  -4.153508  8.775991  0.221381\n","GRU(2,3,5) + LGBM       Test 0.425335 0.186945 0.432371 -29.976429 20.982028  0.322031\n"]}]},{"cell_type":"markdown","source":["## Optuna"],"metadata":{"id":"ehknf8a-arXK"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import lightgbm as lgb\n","import pandas as pd\n","import optuna\n","import time\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h0)\n","        return self.fc(out[:, -1, :])\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# ----------- OPTUNA OPTIMIZATION FUNCTION -----------\n","def objective(trial):\n","    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128, step=16)\n","    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n","\n","    model = GRUModel(input_size=3, hidden_size=hidden_size, num_layers=num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    start_time = time.time()\n","    num_epochs = 50\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    model.eval()\n","    with torch.no_grad():\n","        val_pred = model(X_val_torch).cpu().numpy()\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    _, _, _, _, mape = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","    return mape\n","\n","# Run Optuna for GRU\n","study_gru = optuna.create_study(direction=\"minimize\")\n","study_gru.optimize(objective, n_trials=20)\n","best_gru_params = study_gru.best_params\n","print(\"\\nBest GRU Model:\", best_gru_params)\n","\n","# ----------- Train Best GRU and Get Embeddings -----------\n","best_gru = GRUModel(input_size=3, hidden_size=best_gru_params[\"hidden_size\"], num_layers=best_gru_params[\"num_layers\"]).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(best_gru.parameters(), lr=best_gru_params[\"learning_rate\"])\n","\n","start_train_time = time.time()\n","for epoch in range(50):\n","    best_gru.train()\n","    outputs = best_gru(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_train_time\n","\n","best_gru.eval()\n","with torch.no_grad():\n","    start_val_time = time.time()\n","    train_pred = best_gru(X_train_torch).cpu().numpy()\n","    val_pred = best_gru(X_val_torch).cpu().numpy()\n","    test_pred = best_gru(X_test_torch).cpu().numpy()\n","    val_time = time.time() - start_val_time\n","\n","test_time_start = time.time()\n","test_pred = best_gru(X_test_torch).cpu().detach().numpy()\n","test_time = time.time() - test_time_start\n","\n","X_train_lgb = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","X_val_lgb = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","X_test_lgb = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","# ----------- Print Timing Information -----------\n","print(\"Train Time:\", train_time)\n","print(\"Validation Time:\", val_time)\n","print(\"Test Time:\", test_time)\n","\n","# ----------- Print Metrics -----------\n","metrics_train = compute_metrics(Y_train.values.flatten(), X_train_lgb.flatten())\n","metrics_val = compute_metrics(Y_val.values.flatten(), X_val_lgb.flatten())\n","metrics_test = compute_metrics(Y_test.values.flatten(), X_test_lgb.flatten())\n","\n","print(\"Train Metrics:\", metrics_train)\n","print(\"Validation Metrics:\", metrics_val)\n","print(\"Test Metrics:\", metrics_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACc5KrKjpxek","outputId":"0e131605-e963-4017-afa2-0cfcc55381d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 19:05:10,288] A new study created in memory with name: no-name-224fee4b-4ede-403d-bc72-69222adf3eb3\n","[I 2025-03-05 19:05:45,734] Trial 0 finished with value: 64.56326662072686 and parameters: {'hidden_size': 80, 'num_layers': 2, 'learning_rate': 0.00042035182513254873}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:06:17,759] Trial 1 finished with value: 71.62521113250705 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.004274655269207101}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:06:58,829] Trial 2 finished with value: 73.18961925134037 and parameters: {'hidden_size': 96, 'num_layers': 4, 'learning_rate': 0.002326159023512487}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:07:53,237] Trial 3 finished with value: 72.2029674101477 and parameters: {'hidden_size': 128, 'num_layers': 5, 'learning_rate': 0.0002889986540999462}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:08:43,430] Trial 4 finished with value: 75.12668719314586 and parameters: {'hidden_size': 112, 'num_layers': 5, 'learning_rate': 0.0004070129642083176}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:09:15,203] Trial 5 finished with value: 97.15633359821551 and parameters: {'hidden_size': 32, 'num_layers': 5, 'learning_rate': 0.00013650646251779985}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:10:02,443] Trial 6 finished with value: 74.53106050691285 and parameters: {'hidden_size': 96, 'num_layers': 5, 'learning_rate': 0.002631780658845599}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:10:40,414] Trial 7 finished with value: 73.11109649228501 and parameters: {'hidden_size': 96, 'num_layers': 3, 'learning_rate': 0.0011001421139743754}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:11:15,990] Trial 8 finished with value: 73.61190343208565 and parameters: {'hidden_size': 64, 'num_layers': 5, 'learning_rate': 0.0025126038527480362}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:11:51,362] Trial 9 finished with value: 73.33902071337948 and parameters: {'hidden_size': 80, 'num_layers': 3, 'learning_rate': 0.003166424974233421}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:12:19,882] Trial 10 finished with value: 68.14461826575643 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.000680492039688258}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:12:48,259] Trial 11 finished with value: 69.96403581044532 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0006975999934916595}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:13:17,445] Trial 12 finished with value: 73.92962524588063 and parameters: {'hidden_size': 48, 'num_layers': 2, 'learning_rate': 0.009484207933406302}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:13:51,966] Trial 13 finished with value: 73.14300119926352 and parameters: {'hidden_size': 64, 'num_layers': 3, 'learning_rate': 0.0002081228864314695}. Best is trial 0 with value: 64.56326662072686.\n","[I 2025-03-05 19:14:44,726] Trial 14 finished with value: 63.971535057535455 and parameters: {'hidden_size': 48, 'num_layers': 2, 'learning_rate': 0.0008030772702469081}. Best is trial 14 with value: 63.971535057535455.\n","[I 2025-03-05 19:15:17,770] Trial 15 finished with value: 73.76089910119342 and parameters: {'hidden_size': 48, 'num_layers': 4, 'learning_rate': 0.001315145925911179}. Best is trial 14 with value: 63.971535057535455.\n","[I 2025-03-05 19:15:57,919] Trial 16 finished with value: 65.22957021416993 and parameters: {'hidden_size': 80, 'num_layers': 3, 'learning_rate': 0.00044351204280664264}. Best is trial 14 with value: 63.971535057535455.\n","[I 2025-03-05 19:16:28,369] Trial 17 finished with value: 75.15837959675909 and parameters: {'hidden_size': 48, 'num_layers': 2, 'learning_rate': 0.00012542642841776672}. Best is trial 14 with value: 63.971535057535455.\n","[I 2025-03-05 19:17:05,533] Trial 18 finished with value: 74.45161196043654 and parameters: {'hidden_size': 112, 'num_layers': 2, 'learning_rate': 0.0007680080183537951}. Best is trial 14 with value: 63.971535057535455.\n","[I 2025-03-05 19:17:43,472] Trial 19 finished with value: 73.32474155218586 and parameters: {'hidden_size': 64, 'num_layers': 4, 'learning_rate': 0.0015808217654954978}. Best is trial 14 with value: 63.971535057535455.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Best GRU Model: {'hidden_size': 48, 'num_layers': 2, 'learning_rate': 0.0008030772702469081}\n","Train Time: 30.373690605163574\n","Validation Time: 0.025646448135375977\n","Test Time: 0.004586219787597656\n","Train Metrics: (0.2565726685994142, 0.1035254439122539, 0.321753700697061, 0.40849703643668944, 99.62007153782372)\n","Validation Metrics: (0.9986792914865059, 1.0012501993480294, 1.0006249044212467, -169.2359610734009, 57.1344744804048)\n","Test Metrics: (1.2194269558784696, 1.491119171960729, 1.221113906218715, -246.07568849319298, 60.467445837572875)\n"]}]},{"cell_type":"markdown","source":["## BOHB"],"metadata":{"id":"qCWOF10XateG"}},{"cell_type":"code","source":["!pip install ConfigSpace"],"metadata":{"id":"u1q5OH2ywHV-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install hpbandster"],"metadata":{"id":"vMu6f5SDwM_c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import lightgbm as lgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# GRU Training\n","gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","gru_features = []\n","train_time_start = time.time()\n","\n","for num_layers in gru_layers:\n","    gru_model = GRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        gru_model.train()\n","        optimizer.zero_grad()\n","        outputs = gru_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    gru_model.eval()\n","    with torch.no_grad():\n","        train_features = gru_model(X_train_torch).numpy()\n","        val_features = gru_model(X_val_torch).numpy()\n","        test_features = gru_model(X_test_torch).numpy()\n","\n","    gru_features.append((train_features, val_features, test_features))\n","\n","train_time = time.time() - train_time_start\n","\n","# Stack extracted features\n","train_features_stacked = np.hstack([feat[0] for feat in gru_features])\n","val_features_stacked = np.hstack([feat[1] for feat in gru_features])\n","test_features_stacked = np.hstack([feat[2] for feat in gru_features])\n","\n","# Define ConfigSpace for BOHB (LightGBM)\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_leaves\", 20, 300, default_value=50))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 12, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"feature_fraction\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Optimization\n","bohb_start_time = time.time()\n","\n","NS = hpns.NameServer(run_id=\"stacked_gru_lgb_bohb\", host=\"127.0.0.1\", port=None)\n","NS.start()\n","\n","class LightGBMWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = lgb.LGBMRegressor(\n","            num_leaves=config[\"num_leaves\"],\n","            max_depth=config[\"max_depth\"],\n","            learning_rate=config[\"learning_rate\"],\n","            feature_fraction=config[\"feature_fraction\"],\n","            random_state=42\n","        )\n","        model.fit(self.train_features, Y_train)\n","        val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","worker = LightGBMWorker(\n","    train_features=train_features_stacked,\n","    val_features=val_features_stacked,\n","    nameserver=\"127.0.0.1\",\n","    run_id=\"stacked_gru_lgb_bohb\"\n",")\n","worker.run(background=True)\n","\n","bohb = BOHB(\n","    configspace=get_config_space(),\n","    run_id=\"stacked_gru_lgb_bohb\",\n","    nameserver=\"127.0.0.1\",\n","    min_budget=1,\n","    max_budget=3\n",")\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","bohb_time = time.time() - bohb_start_time\n","\n","# Train final LightGBM model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","best_lgb_model = lgb.LGBMRegressor(**best_params, random_state=42)\n","best_lgb_model.fit(train_features_stacked, Y_train)\n","\n","# Predictions and metrics\n","val_time_start = time.time()\n","Y_val_pred = best_lgb_model.predict(val_features_stacked)\n","val_time = time.time() - val_time_start\n","\n","test_time_start = time.time()\n","Y_test_pred = best_lgb_model.predict(test_features_stacked)\n","test_time = time.time() - test_time_start\n","\n","train_metrics = calculate_metrics(Y_train, best_lgb_model.predict(train_features_stacked))\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print results\n","print(\"Train Time:\", train_time)\n","print(\"Validation Time:\", val_time)\n","print( \"Test Time:\", test_time)\n","print( \"BOHB Optimization Time:\", bohb_time)\n","print(\"Train Metrics:\", train_metrics)\n","print(\"Validation Metrics:\", val_metrics)\n","print(\"Test Metrics:\", test_metrics)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JD8sUtJmavck","outputId":"8af68b57-7385-41d0-d511-590999c9bb1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] feature_fraction is set=0.5118293193572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5118293193572\n","[LightGBM] [Warning] feature_fraction is set=0.5118293193572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5118293193572\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 3\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.5118293193572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5118293193572\n","[LightGBM] [Warning] feature_fraction is set=0.5118293193572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5118293193572\n","[LightGBM] [Warning] feature_fraction is set=0.5118293193572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5118293193572\n","Train Time: 41.44368624687195\n","Validation Time: 0.009762287139892578\n","Test Time: 0.010579824447631836\n","BOHB Optimization Time: 33.71845746040344\n","Train Metrics: (0.0032904503032692473, 2.3947629243268174e-05, 0.004893631498516023, 0.9998631728285104, 0.97835113384228)\n","Validation Metrics: (0.1560734030743739, 0.030215128599767233, 0.17382499417450653, -4.1372788334895025, 8.758765955715802)\n","Test Metrics: (0.42502934597737363, 0.18668501533295204, 0.432070613827129, -29.933361707166213, 20.96683996141528)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MAo-DE3uws4z"},"execution_count":null,"outputs":[]}]}