{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["zNDDknAz7OyW","5d4hGeSAw953","ebpnl0PZ8Gf7","hIbMo04v8HVS","bnKg7qFE90d8","rAEZPr6r_VLh","VveqbuvbBv2d","2Y8xq-_5CpA1","EluDHIyzDV_j","Cr-pFI6IFz3N"],"gpuType":"T4","authorship_tag":"ABX9TyNgj9qabHtnpfTGvPnO5t4H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8WleV-jr6Ohz"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## Initial Code"],"metadata":{"id":"zNDDknAz7OyW"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"WY7I5C1zqFJt","executionInfo":{"status":"ok","timestamp":1739443834160,"user_tz":-330,"elapsed":3104,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}}},"outputs":[],"source":["# Importing necessary libraries for data analysis and manipulation\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","# For handling warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39643,"status":"ok","timestamp":1739443899109,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"},"user_tz":-330},"id":"3RKq8vfwqVHB","outputId":"68b13aba-cf43-4514-d754-3ceadd015253"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"X4yeLMDyqd2o","executionInfo":{"status":"ok","timestamp":1739443904931,"user_tz":-330,"elapsed":1542,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}}},"outputs":[],"source":["df_aapl = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stocks/AAPL.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"DHvcgRGPruCy","executionInfo":{"status":"ok","timestamp":1739443907341,"user_tz":-330,"elapsed":579,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}}},"outputs":[],"source":["import numpy as np\n","from scipy.stats import boxcox\n","\n","df_aapl['Close_log'] = np.log(df_aapl['Close'] + 1)\n","df_aapl['Close_sqrt'] = np.sqrt(df_aapl['Close'])\n","df_aapl['Close_boxcox'], _ = boxcox(df_aapl['Close'] + 1)\n"]},{"cell_type":"markdown","metadata":{"id":"lLz5cmQlryah"},"source":["This code calculates the skewness of the 'Close' column in the `df_aapl` DataFrame before and after applying various transformations:\n","\n","1. **Original Skewness**: Calculates the skewness of the original 'Close' data.\n","2. **Log Transformation Skewness**: Calculates the skewness of the 'Close_log' column after applying the log transformation.\n","3. **Square Root Transformation Skewness**: Calculates the skewness of the 'Close_sqrt' column after applying the square root transformation.\n","4. **Box-Cox Transformation Skewness**: Calculates the skewness of the 'Close_boxcox' column after applying the Box-Cox transformation.\n","\n","The printed results help assess how each transformation affects the distribution's symmetry and the success of skewness correction.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1739443908007,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"},"user_tz":-330},"id":"DIPGiQydr2K0","outputId":"1180af89-8157-4169-8df5-8f4e21224322"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Skewness: 2.5045276102319933\n","Log Transformation Skewness: 0.8535555176510308\n","Square Root Transformation Skewness: 1.6211545809555206\n","Box-Cox Transformation Skewness: 0.4352746472149233\n"]}],"source":["\n","skew_original = df_aapl['Close'].skew()\n","skew_log = df_aapl['Close_log'].skew()\n","skew_sqrt = df_aapl['Close_sqrt'].skew()\n","skew_boxcox = pd.Series(df_aapl['Close_boxcox']).skew()\n","\n","print(f\"Original Skewness: {skew_original}\")\n","print(f\"Log Transformation Skewness: {skew_log}\")\n","print(f\"Square Root Transformation Skewness: {skew_sqrt}\")\n","print(f\"Box-Cox Transformation Skewness: {skew_boxcox}\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"De4W27wEr9-p","executionInfo":{"status":"ok","timestamp":1739443908007,"user_tz":-330,"elapsed":3,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}}},"outputs":[],"source":["\n","df_aapl['Open_log'] = np.log(df_aapl['Open'])\n","df_aapl['High_log'] = np.log(df_aapl['High'])\n","df_aapl['Low_log'] = np.log(df_aapl['Low'])\n","df_aapl['Adj Close_log'] = np.log(df_aapl['Adj Close'])\n","df_aapl['Volume_log'] = np.log(df_aapl['Volume'])\n","\n","\n","df_aapl['Open_sqrt'] = np.sqrt(df_aapl['Open'])\n","df_aapl['High_sqrt'] = np.sqrt(df_aapl['High'])\n","df_aapl['Low_sqrt'] = np.sqrt(df_aapl['Low'])\n","df_aapl['Adj Close_sqrt'] = np.sqrt(df_aapl['Adj Close'])\n","df_aapl['Volume_sqrt'] = np.sqrt(df_aapl['Volume'])\n","\n","from scipy.stats import boxcox\n","df_aapl['Open_boxcox'], _ = boxcox(df_aapl['Open'])\n","df_aapl['High_boxcox'], _ = boxcox(df_aapl['High'])\n","df_aapl['Low_boxcox'], _ = boxcox(df_aapl['Low'])\n","df_aapl['Adj Close_boxcox'], _ = boxcox(df_aapl['Adj Close'])"]},{"cell_type":"markdown","source":["This helps compare how the transformations reduce skewness in the data, aiming for a more normal distribution."],"metadata":{"id":"2XrZQHaDAigS"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1739443908661,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"},"user_tz":-330},"id":"of1KONYmsC8t","outputId":"1358d023-81ee-42cc-9e47-b23a7181e88f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness Before Transformation:\n"," Open         2.504632\n","High         2.502208\n","Low          2.506714\n","Adj Close    2.550677\n","Volume       3.565699\n","dtype: float64\n","\n","Skewness After Transformation:\n"," Open_log            0.482872\n","High_log            0.481997\n","Low_log             0.484246\n","Adj Close_log       0.494009\n","Open_sqrt           1.620771\n","High_sqrt           1.621456\n","Low_sqrt            1.620661\n","Adj Close_sqrt      1.679402\n","Volume_sqrt         1.299776\n","Open_boxcox         0.181226\n","High_boxcox         0.179749\n","Low_boxcox          0.182882\n","Adj Close_boxcox    0.180085\n","dtype: float64\n"]}],"source":["\n","skewness_before = df_aapl[['Open', 'High', 'Low', 'Adj Close', 'Volume']].skew()\n","skewness_after = df_aapl[['Open_log', 'High_log', 'Low_log', 'Adj Close_log',\n","                          'Open_sqrt', 'High_sqrt', 'Low_sqrt', 'Adj Close_sqrt', 'Volume_sqrt',\n","                          'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox']].skew()\n","\n","print(\"Skewness Before Transformation:\\n\", skewness_before)\n","print(\"\\nSkewness After Transformation:\\n\", skewness_after)\n"]},{"cell_type":"markdown","source":["- Applied Box-Cox transformation to the 'Open', 'High', 'Low', 'Adj Close', and 'Close' columns.\n","- Recalculated skewness after the transformation to reduce skew and normalize the data for modeling."],"metadata":{"id":"zfEokf4iAmnv"}},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1087,"status":"ok","timestamp":1739443909745,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"},"user_tz":-330},"id":"s9oEP05csI66","outputId":"9be23973-fab4-470c-dfae-fecb30888fa5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness After Box-Cox Transformation:\n","Open_boxcox         0.435237\n","High_boxcox         0.433381\n","Low_boxcox          0.437331\n","Adj Close_boxcox    0.458762\n","Close_boxcox        0.435275\n","dtype: float64\n"]}],"source":["from scipy import stats\n","\n","df_aapl['Open_boxcox'], _ = stats.boxcox(df_aapl['Open'] + 1)\n","df_aapl['High_boxcox'], _ = stats.boxcox(df_aapl['High'] + 1)\n","df_aapl['Low_boxcox'], _ = stats.boxcox(df_aapl['Low'] + 1)\n","df_aapl['Adj Close_boxcox'], _ = stats.boxcox(df_aapl['Adj Close'] + 1)\n","df_aapl['Close_boxcox'], _ = stats.boxcox(df_aapl['Close'] + 1)\n","\n","skewness_after_boxcox = df_aapl[['Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox', 'Close_boxcox']].skew()\n","\n","print(\"Skewness After Box-Cox Transformation:\")\n","print(skewness_after_boxcox)\n"]},{"cell_type":"markdown","source":["Feature Selection"],"metadata":{"id":"uvZe7IzRAwHu"}},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1739443909745,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"},"user_tz":-330},"id":"aczNHUI4rk8x","outputId":"d4b2dae0-c4d5-4a26-ce73-d84de5639c07"},"outputs":[{"output_type":"stream","name":"stdout","text":["         Date      Open      High       Low  Adj Close     Close     Volume  \\\n","0  1980-12-12  0.128348  0.128906  0.128348   0.098943  0.128348  469033600   \n","1  1980-12-15  0.122210  0.122210  0.121652   0.093781  0.121652  175884800   \n","2  1980-12-16  0.113281  0.113281  0.112723   0.086898  0.112723  105728000   \n","3  1980-12-17  0.115513  0.116071  0.115513   0.089049  0.115513   86441600   \n","4  1980-12-18  0.118862  0.119420  0.118862   0.091630  0.118862   73449600   \n","\n","   Open_boxcox  High_boxcox  Low_boxcox  Adj Close_boxcox  Close_boxcox  \n","0     0.117689     0.118173    0.117674          0.092374      0.117689  \n","1     0.112503     0.112516    0.112016          0.087857      0.112030  \n","2     0.104886     0.104897    0.104395          0.081785      0.104407  \n","3     0.106798     0.107287    0.106786          0.083688      0.106798  \n","4     0.109657     0.110145    0.109644          0.085966      0.109657  \n"]}],"source":["\n","df_aapl_cleaned = df_aapl[['Date', 'Open', 'High', 'Low', 'Adj Close', 'Close', 'Volume',\n","                           'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox',\n","                           'Close_boxcox']]\n","\n","print(df_aapl_cleaned.head())\n"]},{"cell_type":"markdown","source":["### Train Validation Test Split\n","\n","The code splits the data into training, validation, and test sets. The features `X` and target `Y` are split as follows:\n","\n","- 70% for training (`X_train`, `Y_train`)\n","- 15% for validation (`X_val`, `Y_val`)\n","- 15% for testing (`X_test`, `Y_test`)\n","\n","The split is done using a 30% test size, followed by splitting the remaining 70% into validation and test sets without shuffling (time series data)."],"metadata":{"id":"chw5ijVT_JRM"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X = df_aapl_cleaned[['Open_boxcox', 'High_boxcox', 'Low_boxcox']]\n","Y = df_aapl_cleaned['Close_boxcox']\n","\n","X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, shuffle=False)\n","X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, shuffle=False)\n","\n","print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSztwxnoy8-U","executionInfo":{"status":"ok","timestamp":1739443909745,"user_tz":-330,"elapsed":4,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"ff1f99a0-5858-4b79-e4dc-602a01ce0486"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set: (7736, 3), Validation set: (1658, 3), Test set: (1658, 3)\n"]}]},{"cell_type":"markdown","source":["## GPU Activation"],"metadata":{"id":"5d4hGeSAw953"}},{"cell_type":"code","source":["import torch\n","\n","# Check GPU status\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is enabled:\", torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"No GPU found, using CPU.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R0d6nftTwf7T","executionInfo":{"status":"ok","timestamp":1739443914080,"user_tz":-330,"elapsed":4337,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"ab153641-721b-4d14-ccc9-0f821e49b292"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is enabled: Tesla T4\n"]}]},{"cell_type":"markdown","source":["## Bi-LSTM - XGB"],"metadata":{"id":"ebpnl0PZ8Gf7"}},{"cell_type":"code","source":["!pip install xgboost\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThVo2tJ79dGW","executionInfo":{"status":"ok","timestamp":1739444317189,"user_tz":-330,"elapsed":4757,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"e1017b2f-561c-4189-980f-6dc206189ff5"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n"]}]},{"cell_type":"markdown","source":["### Initial"],"metadata":{"id":"hIbMo04v8HVS"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import xgboost as xgb\n","import pandas as pd\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(BiLSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size * 2, 1)  # Bi-directional → *2\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        return self.fc(out[:, -1, :])  # Last time step output\n","\n","# Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]  # Different Bi-LSTM layers\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling (helps LSTM)\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100   # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# DataFrame to store results\n","columns = [\"Layers\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\"]\n","results_df = pd.DataFrame(columns=columns)\n","\n","# Train multiple Bi-LSTM models\n","bilstm_outputs = {}  # Store Bi-LSTM embeddings for XGBoost later\n","\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining Bi-LSTM with {num_layers} layers...\")\n","\n","    # Initialize model, loss function, and optimizer\n","    model = BiLSTMModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        train_pred = model(X_train_torch).cpu().numpy()\n","        val_pred = model(X_val_torch).cpu().numpy()\n","        test_pred = model(X_test_torch).cpu().numpy()\n","\n","    # Inverse transform predictions\n","    train_pred_actual = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    test_pred_actual = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","    # Compute metrics for each dataset\n","    metrics_train = compute_metrics(Y_train.values.flatten(), train_pred_actual.flatten())\n","    metrics_val = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","    metrics_test = compute_metrics(Y_test.values.flatten(), test_pred_actual.flatten())\n","\n","    # Append results to DataFrame\n","    results_df = pd.concat([\n","        results_df,\n","        pd.DataFrame([\n","            [num_layers, \"Train\", *metrics_train],\n","            [num_layers, \"Validation\", *metrics_val],\n","            [num_layers, \"Test\", *metrics_test]\n","        ], columns=columns)\n","    ], ignore_index=True)\n","\n","    # Store Bi-LSTM embeddings for XGBoost\n","    bilstm_outputs[num_layers] = {\n","        \"train\": train_pred_actual,\n","        \"val\": val_pred_actual,\n","        \"test\": test_pred_actual\n","    }\n","\n","# Find the best Bi-LSTM model (Lowest Validation MAPE)\n","best_model = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","best_layers = int(best_model[\"Layers\"])\n","\n","# Display Bi-LSTM Results\n","print(\"\\nBi-LSTM Model Performance Comparison (2, 3, and 5 Layers)\\n\")\n","print(results_df.to_string(index=False))\n","\n","print(f\"\\nBest Bi-LSTM Model: {best_layers} Layers (Based on Lowest Validation MAPE)\\n\")\n","\n","# ---------- XGBoost on Best Bi-LSTM Embeddings ----------\n","print(\"\\nTraining XGBoost on Best Bi-LSTM Embeddings...\")\n","\n","# Use the best Bi-LSTM's output as features for XGBoost\n","X_train_xgb = bilstm_outputs[best_layers][\"train\"]\n","X_val_xgb = bilstm_outputs[best_layers][\"val\"]\n","X_test_xgb = bilstm_outputs[best_layers][\"test\"]\n","\n","# XGBoost Model\n","xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.05)\n","xgb_model.fit(X_train_xgb, Y_train.values)\n","\n","# Predictions\n","train_pred_xgb = xgb_model.predict(X_train_xgb)\n","val_pred_xgb = xgb_model.predict(X_val_xgb)\n","test_pred_xgb = xgb_model.predict(X_test_xgb)\n","\n","# Compute metrics for XGBoost\n","metrics_train_xgb = compute_metrics(Y_train.values.flatten(), train_pred_xgb.flatten())\n","metrics_val_xgb = compute_metrics(Y_val.values.flatten(), val_pred_xgb.flatten())\n","metrics_test_xgb = compute_metrics(Y_test.values.flatten(), test_pred_xgb.flatten())\n","\n","# Append XGBoost results to DataFrame\n","results_df = pd.concat([\n","    results_df,\n","    pd.DataFrame([\n","        [f\"Bi-LSTM({best_layers}) + XGBoost\", \"Train\", *metrics_train_xgb],\n","        [f\"Bi-LSTM({best_layers}) + XGBoost\", \"Validation\", *metrics_val_xgb],\n","        [f\"Bi-LSTM({best_layers}) + XGBoost\", \"Test\", *metrics_test_xgb]\n","    ], columns=columns)\n","], ignore_index=True)\n","\n","# Display Final Results\n","print(\"\\nFinal Model Performance (Bi-LSTM vs Bi-LSTM + XGBoost)\\n\")\n","print(results_df.to_string(index=False))\n","\n","# Best Model Selection\n","best_overall = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","print(f\"\\nBest Overall Model: {best_overall['Layers']} (Based on Lowest Validation MAPE)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I64oGlhS8Aj7","executionInfo":{"status":"ok","timestamp":1739444353877,"user_tz":-330,"elapsed":27397,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"8f515687-ebe7-429a-9cde-330bef3eea8d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training Bi-LSTM with 2 layers...\n","Epoch [10/100], Loss: 0.0902\n","Epoch [20/100], Loss: 0.0734\n","Epoch [30/100], Loss: 0.0731\n","Epoch [40/100], Loss: 0.0719\n","Epoch [50/100], Loss: 0.0716\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training Bi-LSTM with 3 layers...\n","Epoch [10/100], Loss: 0.0751\n","Epoch [20/100], Loss: 0.0738\n","Epoch [30/100], Loss: 0.0716\n","Epoch [40/100], Loss: 0.0717\n","Epoch [50/100], Loss: 0.0715\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training Bi-LSTM with 5 layers...\n","Epoch [10/100], Loss: 0.1117\n","Epoch [20/100], Loss: 0.0739\n","Epoch [30/100], Loss: 0.0715\n","Epoch [40/100], Loss: 0.0724\n","Epoch [50/100], Loss: 0.0716\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Bi-LSTM Model Performance Comparison (2, 3, and 5 Layers)\n","\n","Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","     2      Train 0.328285 0.175352 0.418751   -0.001893 125.294782\n","     2 Validation 1.297969 1.690760 1.300292 -286.468876  74.265371\n","     2       Test 1.570859 2.473819 1.572838 -408.907354  77.901311\n","     3      Train 0.327277 0.174991 0.418319    0.000172 124.744005\n","     3 Validation 1.294117 1.680641 1.296396 -284.748409  74.046592\n","     3       Test 1.563716 2.451271 1.565654 -405.171240  77.548352\n","     5      Train 0.326679 0.174665 0.417929    0.002034 124.411644\n","     5 Validation 1.292907 1.677482 1.295176 -284.211206  73.977700\n","     5       Test 1.561766 2.445139 1.563694 -404.155151  77.451855\n","\n","Best Bi-LSTM Model: 5 Layers (Based on Lowest Validation MAPE)\n","\n","\n","Training XGBoost on Best Bi-LSTM Embeddings...\n","\n","Final Model Performance (Bi-LSTM vs Bi-LSTM + XGBoost)\n","\n","              Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","                   2      Train 0.328285 0.175352 0.418751   -0.001893 125.294782\n","                   2 Validation 1.297969 1.690760 1.300292 -286.468876  74.265371\n","                   2       Test 1.570859 2.473819 1.572838 -408.907354  77.901311\n","                   3      Train 0.327277 0.174991 0.418319    0.000172 124.744005\n","                   3 Validation 1.294117 1.680641 1.296396 -284.748409  74.046592\n","                   3       Test 1.563716 2.451271 1.565654 -405.171240  77.548352\n","                   5      Train 0.326679 0.174665 0.417929    0.002034 124.411644\n","                   5 Validation 1.292907 1.677482 1.295176 -284.211206  73.977700\n","                   5       Test 1.561766 2.445139 1.563694 -404.155151  77.451855\n","Bi-LSTM(5) + XGBoost      Train 0.004348 0.000039 0.006232    0.999778   1.411247\n","Bi-LSTM(5) + XGBoost Validation 0.167206 0.033836 0.183946   -4.752958   9.397029\n","Bi-LSTM(5) + XGBoost       Test 0.436234 0.196335 0.443097  -31.532343  21.523591\n","\n","Best Overall Model: Bi-LSTM(5) + XGBoost (Based on Lowest Validation MAPE)\n"]}]},{"cell_type":"markdown","source":["### optuna"],"metadata":{"id":"bnKg7qFE90d8"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3g0lTg8376-j","executionInfo":{"status":"ok","timestamp":1739444466046,"user_tz":-330,"elapsed":4393,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"69edef84-0157-4136-cfad-ddf0c157526d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import xgboost as xgb\n","import pandas as pd\n","import optuna\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(BiLSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size * 2, 1)  # Bi-directional LSTM has 2x hidden size\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        return self.fc(out[:, -1, :])  # Last time step output\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100   # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Optuna Bi-LSTM Optimization\n","def objective_bilstm(trial):\n","    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n","    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128)\n","    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n","    num_epochs = 100\n","\n","    # Train Bi-LSTM\n","    model = BiLSTMModel(input_size=3, hidden_size=hidden_size, num_layers=num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluate Bi-LSTM\n","    model.eval()\n","    with torch.no_grad():\n","        val_pred = model(X_val_torch).cpu().numpy()\n","\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    mape = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())[-1]  # MAPE\n","\n","    return mape  # Minimize Validation MAPE\n","\n","# Run Optuna for Bi-LSTM\n","study_bilstm = optuna.create_study(direction=\"minimize\")\n","study_bilstm.optimize(objective_bilstm, n_trials=20)\n","best_bilstm_params = study_bilstm.best_params\n","\n","# Train Best Bi-LSTM Model\n","best_bilstm_model = BiLSTMModel(input_size=3, hidden_size=best_bilstm_params[\"hidden_size\"], num_layers=best_bilstm_params[\"num_layers\"]).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(best_bilstm_model.parameters(), lr=best_bilstm_params[\"learning_rate\"])\n","\n","for epoch in range(100):\n","    best_bilstm_model.train()\n","    outputs = best_bilstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","# Extract Bi-LSTM Embeddings\n","best_bilstm_model.eval()\n","with torch.no_grad():\n","    X_train_xgb = best_bilstm_model(X_train_torch).cpu().numpy()\n","    X_val_xgb = best_bilstm_model(X_val_torch).cpu().numpy()\n","    X_test_xgb = best_bilstm_model(X_test_torch).cpu().numpy()\n","\n","# Optuna XGBoost Optimization\n","def objective_xgb(trial):\n","    params = {\n","        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n","        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n","        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n","        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n","        \"objective\": \"reg:squarederror\"\n","    }\n","\n","    xgb_model = xgb.XGBRegressor(**params)\n","    xgb_model.fit(X_train_xgb, Y_train.values)\n","\n","    val_pred_xgb = xgb_model.predict(X_val_xgb)\n","    mape = compute_metrics(Y_val.values.flatten(), val_pred_xgb.flatten())[-1]  # MAPE\n","\n","    return mape  # Minimize Validation MAPE\n","\n","# Run Optuna for XGBoost\n","study_xgb = optuna.create_study(direction=\"minimize\")\n","study_xgb.optimize(objective_xgb, n_trials=20)\n","best_xgb_params = study_xgb.best_params\n","\n","# Train Best XGBoost Model\n","best_xgb_model = xgb.XGBRegressor(**best_xgb_params)\n","best_xgb_model.fit(X_train_xgb, Y_train.values)\n","\n","# Predictions\n","train_pred_xgb = best_xgb_model.predict(X_train_xgb)\n","val_pred_xgb = best_xgb_model.predict(X_val_xgb)\n","test_pred_xgb = best_xgb_model.predict(X_test_xgb)\n","\n","# Compute final metrics\n","metrics_train_xgb = compute_metrics(Y_train.values.flatten(), train_pred_xgb.flatten())\n","metrics_val_xgb = compute_metrics(Y_val.values.flatten(), val_pred_xgb.flatten())\n","metrics_test_xgb = compute_metrics(Y_test.values.flatten(), test_pred_xgb.flatten())\n","\n","# Print Final Results\n","print(\"\\nFinal Model Performance (Bi-LSTM + XGBoost):\\n\")\n","print(f\"Training:    MAE={metrics_train_xgb[0]:.4f}, MSE={metrics_train_xgb[1]:.4f}, RMSE={metrics_train_xgb[2]:.4f}, R²={metrics_train_xgb[3]:.4f}, MAPE={metrics_train_xgb[4]:.2f}%\")\n","print(f\"Validation:  MAE={metrics_val_xgb[0]:.4f}, MSE={metrics_val_xgb[1]:.4f}, RMSE={metrics_val_xgb[2]:.4f}, R²={metrics_val_xgb[3]:.4f}, MAPE={metrics_val_xgb[4]:.2f}%\")\n","print(f\"Test:        MAE={metrics_test_xgb[0]:.4f}, MSE={metrics_test_xgb[1]:.4f}, RMSE={metrics_test_xgb[2]:.4f}, R²={metrics_test_xgb[3]:.4f}, MAPE={metrics_test_xgb[4]:.2f}%\")\n","\n","print(\"\\nBest Bi-LSTM Parameters:\", best_bilstm_params)\n","print(\"Best XGBoost Parameters:\", best_xgb_params)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TdjT9aKA-BrN","executionInfo":{"status":"ok","timestamp":1739444663086,"user_tz":-330,"elapsed":159788,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"1569b388-67b2-4c34-9a90-c8f17a2fb39f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:01:45,781] A new study created in memory with name: no-name-b1424143-bf1f-4f67-9cbb-f3684a142838\n","[I 2025-02-13 11:01:48,119] Trial 0 finished with value: 73.34624428452885 and parameters: {'num_layers': 3, 'hidden_size': 38, 'learning_rate': 0.0006057502407408701}. Best is trial 0 with value: 73.34624428452885.\n","[I 2025-02-13 11:01:54,593] Trial 1 finished with value: 73.7891409624451 and parameters: {'num_layers': 4, 'hidden_size': 83, 'learning_rate': 0.0006177757617505723}. Best is trial 0 with value: 73.34624428452885.\n","[I 2025-02-13 11:02:07,135] Trial 2 finished with value: 73.98659026626059 and parameters: {'num_layers': 4, 'hidden_size': 115, 'learning_rate': 0.0007457937543603483}. Best is trial 0 with value: 73.34624428452885.\n","[I 2025-02-13 11:02:26,266] Trial 3 finished with value: 73.9826529459119 and parameters: {'num_layers': 5, 'hidden_size': 112, 'learning_rate': 0.006138532859104829}. Best is trial 0 with value: 73.34624428452885.\n","[I 2025-02-13 11:02:36,897] Trial 4 finished with value: 73.8877534659142 and parameters: {'num_layers': 3, 'hidden_size': 119, 'learning_rate': 0.00043284588022946463}. Best is trial 0 with value: 73.34624428452885.\n","[I 2025-02-13 11:02:42,113] Trial 5 finished with value: 73.92288435002321 and parameters: {'num_layers': 3, 'hidden_size': 82, 'learning_rate': 0.009248073670339982}. Best is trial 0 with value: 73.34624428452885.\n","[I 2025-02-13 11:02:49,800] Trial 6 finished with value: 73.59581542417544 and parameters: {'num_layers': 5, 'hidden_size': 76, 'learning_rate': 0.0005120223485948626}. Best is trial 0 with value: 73.34624428452885.\n","[I 2025-02-13 11:02:59,544] Trial 7 finished with value: 69.71505460389 and parameters: {'num_layers': 3, 'hidden_size': 106, 'learning_rate': 0.00015205682133696315}. Best is trial 7 with value: 69.71505460389.\n","[I 2025-02-13 11:03:07,529] Trial 8 finished with value: 73.96645736933894 and parameters: {'num_layers': 4, 'hidden_size': 84, 'learning_rate': 0.007916082622916482}. Best is trial 7 with value: 69.71505460389.\n","[I 2025-02-13 11:03:16,295] Trial 9 finished with value: 73.88651704358443 and parameters: {'num_layers': 4, 'hidden_size': 90, 'learning_rate': 0.000567163405084853}. Best is trial 7 with value: 69.71505460389.\n","[I 2025-02-13 11:03:18,251] Trial 10 finished with value: 72.9170937052162 and parameters: {'num_layers': 2, 'hidden_size': 58, 'learning_rate': 0.0001368282616231335}. Best is trial 7 with value: 69.71505460389.\n","[I 2025-02-13 11:03:19,933] Trial 11 finished with value: 89.52144218766821 and parameters: {'num_layers': 2, 'hidden_size': 51, 'learning_rate': 0.00011078553587118165}. Best is trial 7 with value: 69.71505460389.\n","[I 2025-02-13 11:03:21,977] Trial 12 finished with value: 76.31058037848358 and parameters: {'num_layers': 2, 'hidden_size': 60, 'learning_rate': 0.00010542439783310543}. Best is trial 7 with value: 69.71505460389.\n","[I 2025-02-13 11:03:28,194] Trial 13 finished with value: 66.847115517903 and parameters: {'num_layers': 2, 'hidden_size': 105, 'learning_rate': 0.00022141301540287622}. Best is trial 13 with value: 66.847115517903.\n","[I 2025-02-13 11:03:34,321] Trial 14 finished with value: 69.80050728852153 and parameters: {'num_layers': 2, 'hidden_size': 100, 'learning_rate': 0.00023514675256157698}. Best is trial 13 with value: 66.847115517903.\n","[I 2025-02-13 11:03:43,085] Trial 15 finished with value: 73.99611742366073 and parameters: {'num_layers': 3, 'hidden_size': 100, 'learning_rate': 0.0022553349892047076}. Best is trial 13 with value: 66.847115517903.\n","[I 2025-02-13 11:03:50,796] Trial 16 finished with value: 69.86702781280074 and parameters: {'num_layers': 2, 'hidden_size': 126, 'learning_rate': 0.00025580062034740825}. Best is trial 13 with value: 66.847115517903.\n","[I 2025-02-13 11:03:59,953] Trial 17 finished with value: 73.03671536810458 and parameters: {'num_layers': 3, 'hidden_size': 105, 'learning_rate': 0.0002658578015890395}. Best is trial 13 with value: 66.847115517903.\n","[I 2025-02-13 11:04:04,317] Trial 18 finished with value: 74.13409305637178 and parameters: {'num_layers': 2, 'hidden_size': 94, 'learning_rate': 0.0015443459744698526}. Best is trial 13 with value: 66.847115517903.\n","[I 2025-02-13 11:04:16,090] Trial 19 finished with value: 71.1519819943508 and parameters: {'num_layers': 3, 'hidden_size': 127, 'learning_rate': 0.00018245147755345254}. Best is trial 13 with value: 66.847115517903.\n","[I 2025-02-13 11:04:22,260] A new study created in memory with name: no-name-463f161b-5d80-43c8-a715-015d5dba1857\n","[I 2025-02-13 11:04:22,494] Trial 0 finished with value: 9.324395389497392 and parameters: {'n_estimators': 248, 'learning_rate': 0.02109480719830452, 'max_depth': 10, 'subsample': 0.9714497397447329, 'colsample_bytree': 0.7372707610957511}. Best is trial 0 with value: 9.324395389497392.\n","[I 2025-02-13 11:04:22,828] Trial 1 finished with value: 8.856203801155802 and parameters: {'n_estimators': 463, 'learning_rate': 0.04330413244251315, 'max_depth': 6, 'subsample': 0.8590087934368451, 'colsample_bytree': 0.7459094221869473}. Best is trial 1 with value: 8.856203801155802.\n","[I 2025-02-13 11:04:23,006] Trial 2 finished with value: 9.371021620003466 and parameters: {'n_estimators': 283, 'learning_rate': 0.01826549846337443, 'max_depth': 10, 'subsample': 0.8132153529630943, 'colsample_bytree': 0.7824891879151663}. Best is trial 1 with value: 8.856203801155802.\n","[I 2025-02-13 11:04:23,150] Trial 3 finished with value: 9.759793967304542 and parameters: {'n_estimators': 229, 'learning_rate': 0.01988404919118727, 'max_depth': 8, 'subsample': 0.9596976789393676, 'colsample_bytree': 0.566261598862974}. Best is trial 1 with value: 8.856203801155802.\n","[I 2025-02-13 11:04:23,386] Trial 4 finished with value: 8.910978342374754 and parameters: {'n_estimators': 341, 'learning_rate': 0.1499653971637723, 'max_depth': 8, 'subsample': 0.7299945001157073, 'colsample_bytree': 0.9085567816309743}. Best is trial 1 with value: 8.856203801155802.\n","[I 2025-02-13 11:04:23,666] Trial 5 finished with value: 8.846399494832303 and parameters: {'n_estimators': 484, 'learning_rate': 0.0508474715271058, 'max_depth': 7, 'subsample': 0.7671149965574362, 'colsample_bytree': 0.9789567230475098}. Best is trial 5 with value: 8.846399494832303.\n","[I 2025-02-13 11:04:23,761] Trial 6 finished with value: 8.855872254565151 and parameters: {'n_estimators': 337, 'learning_rate': 0.11861970299966157, 'max_depth': 3, 'subsample': 0.9495312671956577, 'colsample_bytree': 0.7543351820290523}. Best is trial 5 with value: 8.846399494832303.\n","[I 2025-02-13 11:04:23,999] Trial 7 finished with value: 10.08785817832163 and parameters: {'n_estimators': 413, 'learning_rate': 0.010266360728336954, 'max_depth': 9, 'subsample': 0.9968348477347311, 'colsample_bytree': 0.9618448561681523}. Best is trial 5 with value: 8.846399494832303.\n","[I 2025-02-13 11:04:24,070] Trial 8 finished with value: 8.84182550513273 and parameters: {'n_estimators': 157, 'learning_rate': 0.062469065595715544, 'max_depth': 4, 'subsample': 0.7449310676579073, 'colsample_bytree': 0.9410228141653285}. Best is trial 8 with value: 8.84182550513273.\n","[I 2025-02-13 11:04:24,287] Trial 9 finished with value: 9.180452175717724 and parameters: {'n_estimators': 388, 'learning_rate': 0.01449152931563863, 'max_depth': 7, 'subsample': 0.93193318490594, 'colsample_bytree': 0.8293314486489638}. Best is trial 8 with value: 8.84182550513273.\n","[I 2025-02-13 11:04:24,350] Trial 10 finished with value: 9.047965921471336 and parameters: {'n_estimators': 88, 'learning_rate': 0.07530886230992935, 'max_depth': 3, 'subsample': 0.5825819091696446, 'colsample_bytree': 0.6103655731518103}. Best is trial 8 with value: 8.84182550513273.\n","[I 2025-02-13 11:04:24,453] Trial 11 finished with value: 9.408323968546139 and parameters: {'n_estimators': 124, 'learning_rate': 0.040953473488460186, 'max_depth': 5, 'subsample': 0.735998178912961, 'colsample_bytree': 0.9986247042970113}. Best is trial 8 with value: 8.84182550513273.\n","[I 2025-02-13 11:04:24,568] Trial 12 finished with value: 8.834829195444328 and parameters: {'n_estimators': 170, 'learning_rate': 0.07023805708636965, 'max_depth': 5, 'subsample': 0.6393200959791161, 'colsample_bytree': 0.8858655568649049}. Best is trial 12 with value: 8.834829195444328.\n","[I 2025-02-13 11:04:24,677] Trial 13 finished with value: 8.83334061891488 and parameters: {'n_estimators': 168, 'learning_rate': 0.07844400039983415, 'max_depth': 5, 'subsample': 0.6145317921662224, 'colsample_bytree': 0.8798391216773913}. Best is trial 13 with value: 8.83334061891488.\n","[I 2025-02-13 11:04:24,785] Trial 14 finished with value: 8.850337456378387 and parameters: {'n_estimators': 189, 'learning_rate': 0.0948383694184894, 'max_depth': 5, 'subsample': 0.5988000199876989, 'colsample_bytree': 0.8741210397774518}. Best is trial 13 with value: 8.83334061891488.\n","[I 2025-02-13 11:04:24,839] Trial 15 finished with value: 8.832461005511115 and parameters: {'n_estimators': 55, 'learning_rate': 0.18609957299978297, 'max_depth': 5, 'subsample': 0.6431468331972992, 'colsample_bytree': 0.8581224821638807}. Best is trial 15 with value: 8.832461005511115.\n","[I 2025-02-13 11:04:24,898] Trial 16 finished with value: 8.825764478108567 and parameters: {'n_estimators': 57, 'learning_rate': 0.19856376662696132, 'max_depth': 6, 'subsample': 0.6688614379208031, 'colsample_bytree': 0.6656730417110022}. Best is trial 16 with value: 8.825764478108567.\n","[I 2025-02-13 11:04:24,975] Trial 17 finished with value: 8.86117305257539 and parameters: {'n_estimators': 78, 'learning_rate': 0.19467832254135609, 'max_depth': 6, 'subsample': 0.5317417918223015, 'colsample_bytree': 0.6505784501898964}. Best is trial 16 with value: 8.825764478108567.\n","[I 2025-02-13 11:04:25,035] Trial 18 finished with value: 8.844457579087072 and parameters: {'n_estimators': 75, 'learning_rate': 0.1987192483648147, 'max_depth': 4, 'subsample': 0.6796709797923093, 'colsample_bytree': 0.5031450583355577}. Best is trial 16 with value: 8.825764478108567.\n","[I 2025-02-13 11:04:25,127] Trial 19 finished with value: 8.82751487167918 and parameters: {'n_estimators': 114, 'learning_rate': 0.12484542674989406, 'max_depth': 6, 'subsample': 0.5051140274108032, 'colsample_bytree': 0.6812183803485943}. Best is trial 16 with value: 8.825764478108567.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Final Model Performance (Bi-LSTM + XGBoost):\n","\n","Training:    MAE=0.0034, MSE=0.0000, RMSE=0.0051, R²=0.9998, MAPE=0.98%\n","Validation:  MAE=0.1572, MSE=0.0306, RMSE=0.1749, R²=-4.2004, MAPE=8.83%\n","Test:        MAE=0.4262, MSE=0.1877, RMSE=0.4332, R²=-30.1007, MAPE=21.03%\n","\n","Best Bi-LSTM Parameters: {'num_layers': 2, 'hidden_size': 105, 'learning_rate': 0.00022141301540287622}\n","Best XGBoost Parameters: {'n_estimators': 57, 'learning_rate': 0.19856376662696132, 'max_depth': 6, 'subsample': 0.6688614379208031, 'colsample_bytree': 0.6656730417110022}\n"]}]},{"cell_type":"markdown","source":["### bohb"],"metadata":{"id":"voPEuYx6-2sD"}},{"cell_type":"code","source":["!pip install ConfigSpace hpbandster"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SayOE4O-4fJ","executionInfo":{"status":"ok","timestamp":1739444736512,"user_tz":-330,"elapsed":13959,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"5f152195-6c7f-42ed-9770-5c9f9f83254f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.26.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.13.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (10.6.0)\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: ConfigSpace, hpbandster, netifaces\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=ConfigSpace-1.2.1-py3-none-any.whl size=115943 sha256=d1c2d7496ffdb2641949b1c0884e21a574c3070a1b2531bad52510cdf1517525\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=6ae796082e9069d6a85bed100d2179decd88469307e7236bcf6cca50d96e9167\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35181 sha256=1f7cc6e7fad5cdc56b39ab9a88ee1cf78796ab568b2df4bd1254a116b085031a\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built ConfigSpace hpbandster netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, ConfigSpace, hpbandster\n","Successfully installed ConfigSpace-1.2.1 Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import xgboost as xgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-LSTM has 2x hidden_dim\n","\n","    def forward(self, x):\n","        out, _ = self.bilstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Bi-LSTM Configurations (2, 3, and 5 layers)\n","bilstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store Bi-LSTM feature representations\n","bilstm_features = {}\n","\n","for num_layers in bilstm_layers:\n","    print(f\"Training Bi-LSTM with {num_layers} layers...\")\n","\n","    bilstm_model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(bilstm_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        bilstm_model.train()\n","        optimizer.zero_grad()\n","        outputs = bilstm_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Extract Feature Representations\n","    bilstm_model.eval()\n","    with torch.no_grad():\n","        train_features = bilstm_model(X_train_torch).numpy()\n","        val_features = bilstm_model(X_val_torch).numpy()\n","        test_features = bilstm_model(X_test_torch).numpy()\n","\n","    bilstm_features[num_layers] = (train_features, val_features, test_features)\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"subsample\", 0.5, 1.0, default_value=0.8))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"colsample_bytree\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for XGBoost\n","class XGBoostWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = xgb.XGBRegressor(\n","            n_estimators=config[\"n_estimators\"],\n","            learning_rate=config[\"learning_rate\"],\n","            max_depth=config[\"max_depth\"],\n","            subsample=config[\"subsample\"],\n","            colsample_bytree=config[\"colsample_bytree\"],\n","            random_state=42\n","        )\n","        model.fit(self.train_features, Y_train)\n","        Y_val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB for each Bi-LSTM configuration\n","best_models = {}\n","\n","for num_layers in bilstm_layers:\n","    print(f\"\\nRunning BOHB for Bi-LSTM ({num_layers} layers) + XGBoost...\")\n","\n","    train_features, val_features, test_features = bilstm_features[num_layers]\n","\n","    # Start NameServer\n","    NS = hpns.NameServer(run_id=f\"bilstm_{num_layers}_xgb_bohb\", host=\"127.0.0.1\", port=None)\n","    NS.start()\n","\n","    worker = XGBoostWorker(\n","        train_features=train_features,\n","        val_features=val_features,\n","        nameserver=\"127.0.0.1\",\n","        run_id=f\"bilstm_{num_layers}_xgb_bohb\"\n","    )\n","    worker.run(background=True)\n","\n","    bohb = BOHB(\n","        configspace=get_config_space(),\n","        run_id=f\"bilstm_{num_layers}_xgb_bohb\",\n","        nameserver=\"127.0.0.1\",\n","        min_budget=1,\n","        max_budget=3\n","    )\n","\n","    res = bohb.run(n_iterations=50)\n","\n","    # Shutdown BOHB\n","    bohb.shutdown()\n","    NS.shutdown()\n","\n","    # Retrieve Best Configuration\n","    best_config = res.get_incumbent_id()\n","    best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","    # Train Best XGB Model on Bi-LSTM Features\n","    best_xgb_model = xgb.XGBRegressor(\n","        n_estimators=best_params[\"n_estimators\"],\n","        learning_rate=best_params[\"learning_rate\"],\n","        max_depth=best_params[\"max_depth\"],\n","        subsample=best_params[\"subsample\"],\n","        colsample_bytree=best_params[\"colsample_bytree\"],\n","        random_state=42\n","    )\n","\n","    best_xgb_model.fit(train_features, Y_train)\n","\n","    # Make Predictions\n","    Y_train_pred = best_xgb_model.predict(train_features)\n","    Y_val_pred = best_xgb_model.predict(val_features)\n","    Y_test_pred = best_xgb_model.predict(test_features)\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store best model and metrics\n","    best_models[num_layers] = {\n","        \"params\": best_params,\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics\n","    }\n","\n","    # Print Results\n","    print(f\"\\nBest Parameters for Bi-LSTM ({num_layers} layers) + XGBoost:\")\n","    print(best_params)\n","\n","    print(\"\\nTraining set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"\\nValidation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"\\nTest set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bjk33e0P-L-Q","executionInfo":{"status":"ok","timestamp":1739445041092,"user_tz":-330,"elapsed":299996,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"5cc9d469-d4ae-4d02-f495-f6a530bc41ce"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Bi-LSTM with 2 layers...\n","Training Bi-LSTM with 3 layers...\n","Training Bi-LSTM with 5 layers...\n","\n","Running BOHB for Bi-LSTM (2 layers) + XGBoost...\n","\n","Best Parameters for Bi-LSTM (2 layers) + XGBoost:\n","{'colsample_bytree': 0.9861118922457, 'learning_rate': 0.2824946990358, 'max_depth': 6, 'n_estimators': 494, 'subsample': 0.7901141660438}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0055, R²: 0.9998, MAPE: 1.04%\n","\n","Validation set metrics:\n","MAE: 0.1568, MSE: 0.0304, RMSE: 0.1745, R²: -4.1756, MAPE: 8.80%\n","\n","Test set metrics:\n","MAE: 0.4258, MSE: 0.1873, RMSE: 0.4328, R²: -30.0351, MAPE: 21.00%\n","\n","Running BOHB for Bi-LSTM (3 layers) + XGBoost...\n","\n","Best Parameters for Bi-LSTM (3 layers) + XGBoost:\n","{'colsample_bytree': 0.9656658580314, 'learning_rate': 0.1676121612285, 'max_depth': 5, 'n_estimators': 82, 'subsample': 0.5409891053982}\n","\n","Training set metrics:\n","MAE: 0.0037, MSE: 0.0000, RMSE: 0.0056, R²: 0.9998, MAPE: 1.08%\n","\n","Validation set metrics:\n","MAE: 0.1567, MSE: 0.0304, RMSE: 0.1744, R²: -4.1687, MAPE: 8.79%\n","\n","Test set metrics:\n","MAE: 0.4256, MSE: 0.1872, RMSE: 0.4327, R²: -30.0166, MAPE: 21.00%\n","\n","Running BOHB for Bi-LSTM (5 layers) + XGBoost...\n","\n","Best Parameters for Bi-LSTM (5 layers) + XGBoost:\n","{'colsample_bytree': 0.8360103260249, 'learning_rate': 0.2659027927573, 'max_depth': 5, 'n_estimators': 429, 'subsample': 0.5383614921803}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0056, R²: 0.9998, MAPE: 1.06%\n","\n","Validation set metrics:\n","MAE: 0.1561, MSE: 0.0302, RMSE: 0.1739, R²: -4.1405, MAPE: 8.76%\n","\n","Test set metrics:\n","MAE: 0.4251, MSE: 0.1867, RMSE: 0.4321, R²: -29.9419, MAPE: 20.97%\n"]}]},{"cell_type":"markdown","source":["## Bi-LSTM [CatBoost]"],"metadata":{"id":"rAEZPr6r_VLh"}},{"cell_type":"code","source":["!pip install catboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8jNxjsT4_GAx","executionInfo":{"status":"ok","timestamp":1739445440756,"user_tz":-330,"elapsed":8933,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"6b23f050-a775-4284-a71e-617a6467cf7e"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.55.8)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n","Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.7\n"]}]},{"cell_type":"markdown","source":["### initial"],"metadata":{"id":"VveqbuvbBv2d"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define BiLSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(BiLSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size * 2, 1)  # *2 for bidirectional\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # *2 for bidirectional\n","        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        return self.fc(out[:, -1, :])\n","\n","# Set Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# DataFrame to store results\n","columns = [\"Layers\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\"]\n","results_df = pd.DataFrame(columns=columns)\n","\n","# Train multiple Bi-LSTM models\n","bilstm_outputs = {}\n","\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining Bi-LSTM with {num_layers} layers...\")\n","\n","    model = BiLSTMModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        train_pred = model(X_train_torch).cpu().numpy()\n","        val_pred = model(X_val_torch).cpu().numpy()\n","        test_pred = model(X_test_torch).cpu().numpy()\n","\n","    # Inverse transform predictions\n","    train_pred_actual = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    test_pred_actual = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","    # Compute metrics for each dataset\n","    metrics_train = compute_metrics(Y_train.values.flatten(), train_pred_actual.flatten())\n","    metrics_val = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","    metrics_test = compute_metrics(Y_test.values.flatten(), test_pred_actual.flatten())\n","\n","    # Append results to DataFrame\n","    results_df = pd.concat([\n","        results_df,\n","        pd.DataFrame([\n","            [num_layers, \"Train\", *metrics_train],\n","            [num_layers, \"Validation\", *metrics_val],\n","            [num_layers, \"Test\", *metrics_test]\n","        ], columns=columns)\n","    ], ignore_index=True)\n","\n","    # Store Bi-LSTM embeddings for CatBoost\n","    bilstm_outputs[num_layers] = {\n","        \"train\": train_pred_actual,\n","        \"val\": val_pred_actual,\n","        \"test\": test_pred_actual\n","    }\n","\n","# Find the best Bi-LSTM model (Lowest Validation MAPE)\n","best_model = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","best_layers = int(best_model[\"Layers\"])\n","\n","# Display Bi-LSTM Results\n","print(\"\\nBi-LSTM Model Performance Comparison (2, 3, and 5 Layers)\\n\")\n","print(results_df.to_string(index=False))\n","\n","print(f\"\\nBest Bi-LSTM Model: {best_layers} Layers (Based on Lowest Validation MAPE)\\n\")\n","\n","# ---------- CatBoost on Best Bi-LSTM Embeddings ----------\n","print(\"\\nTraining CatBoost on Best Bi-LSTM Embeddings...\")\n","\n","# Use the best Bi-LSTM's output as features for CatBoost\n","X_train_cat = bilstm_outputs[best_layers][\"train\"]\n","X_val_cat = bilstm_outputs[best_layers][\"val\"]\n","X_test_cat = bilstm_outputs[best_layers][\"test\"]\n","\n","# CatBoost Model\n","cat_model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, loss_function=\"RMSE\", verbose=100)\n","cat_model.fit(X_train_cat, Y_train.values, eval_set=(X_val_cat, Y_val.values), early_stopping_rounds=100)\n","\n","# Predictions\n","train_pred_cat = cat_model.predict(X_train_cat)\n","val_pred_cat = cat_model.predict(X_val_cat)\n","test_pred_cat = cat_model.predict(X_test_cat)\n","\n","# Compute metrics for CatBoost\n","metrics_train_cat = compute_metrics(Y_train.values.flatten(), train_pred_cat.flatten())\n","metrics_val_cat = compute_metrics(Y_val.values.flatten(), val_pred_cat.flatten())\n","metrics_test_cat = compute_metrics(Y_test.values.flatten(), test_pred_cat.flatten())\n","\n","# Append CatBoost results to DataFrame\n","results_df = pd.concat([\n","    results_df,\n","    pd.DataFrame([\n","        [f\"Bi-LSTM({best_layers}) + CatBoost\", \"Train\", *metrics_train_cat],\n","        [f\"Bi-LSTM({best_layers}) + CatBoost\", \"Validation\", *metrics_val_cat],\n","        [f\"Bi-LSTM({best_layers}) + CatBoost\", \"Test\", *metrics_test_cat]\n","    ], columns=columns)\n","], ignore_index=True)\n","\n","# Display Final Results\n","print(\"\\nFinal Model Performance (Bi-LSTM vs Bi-LSTM + CatBoost)\\n\")\n","print(results_df.to_string(index=False))\n","\n","# Best Model Selection\n","best_overall = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","print(f\"\\nBest Overall Model: {best_overall['Layers']} (Based on Lowest Validation MAPE)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJQ4wS-0BuF8","executionInfo":{"status":"ok","timestamp":1739445469628,"user_tz":-330,"elapsed":23867,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"bd323077-af1e-4d2c-a596-e0f0b94bf728"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training Bi-LSTM with 2 layers...\n","Epoch [10/100], Loss: 0.1042\n","Epoch [20/100], Loss: 0.0741\n","Epoch [30/100], Loss: 0.0754\n","Epoch [40/100], Loss: 0.0720\n","Epoch [50/100], Loss: 0.0719\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training Bi-LSTM with 3 layers...\n","Epoch [10/100], Loss: 0.0743\n","Epoch [20/100], Loss: 0.0734\n","Epoch [30/100], Loss: 0.0716\n","Epoch [40/100], Loss: 0.0716\n","Epoch [50/100], Loss: 0.0715\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training Bi-LSTM with 5 layers...\n","Epoch [10/100], Loss: 0.0885\n","Epoch [20/100], Loss: 0.0755\n","Epoch [30/100], Loss: 0.0715\n","Epoch [40/100], Loss: 0.0719\n","Epoch [50/100], Loss: 0.0715\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Bi-LSTM Model Performance Comparison (2, 3, and 5 Layers)\n","\n","Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","     2      Train 0.329173 0.175789 0.419272   -0.004389 125.800671\n","     2 Validation 1.299941 1.695946 1.302285 -287.350592  74.377542\n","     2       Test 1.574359 2.484899 1.576356 -410.743231  78.074273\n","     3      Train 0.327482 0.175139 0.418496   -0.000675 124.865528\n","     3 Validation 1.293804 1.679814 1.296076 -284.607657  74.028969\n","     3       Test 1.562905 2.448711 1.564836 -404.746996  77.508362\n","     5      Train 0.327128 0.175039 0.418377   -0.000103 124.624730\n","     5 Validation 1.293998 1.680314 1.296269 -284.692703  74.040137\n","     5       Test 1.563061 2.449197 1.564991 -404.827598  77.516136\n","\n","Best Bi-LSTM Model: 3 Layers (Based on Lowest Validation MAPE)\n","\n","\n","Training CatBoost on Best Bi-LSTM Embeddings...\n","0:\tlearn: 0.3981710\ttest: 1.2481061\tbest: 1.2481061 (0)\ttotal: 48.1ms\tremaining: 48.1s\n","100:\tlearn: 0.0112380\ttest: 0.1989609\tbest: 0.1989609 (100)\ttotal: 826ms\tremaining: 7.35s\n","200:\tlearn: 0.0098273\ttest: 0.1788871\tbest: 0.1788871 (200)\ttotal: 1.47s\tremaining: 5.84s\n","300:\tlearn: 0.0094117\ttest: 0.1771124\tbest: 0.1771023 (298)\ttotal: 1.95s\tremaining: 4.52s\n","400:\tlearn: 0.0092460\ttest: 0.1766474\tbest: 0.1766472 (399)\ttotal: 2.18s\tremaining: 3.26s\n","500:\tlearn: 0.0091566\ttest: 0.1763877\tbest: 0.1763874 (498)\ttotal: 2.52s\tremaining: 2.51s\n","600:\tlearn: 0.0091085\ttest: 0.1762271\tbest: 0.1762196 (593)\ttotal: 2.94s\tremaining: 1.95s\n","700:\tlearn: 0.0090713\ttest: 0.1761451\tbest: 0.1761406 (684)\ttotal: 3.3s\tremaining: 1.41s\n","800:\tlearn: 0.0090497\ttest: 0.1760754\tbest: 0.1760720 (791)\ttotal: 3.67s\tremaining: 911ms\n","900:\tlearn: 0.0090353\ttest: 0.1760255\tbest: 0.1760255 (898)\ttotal: 4.03s\tremaining: 443ms\n","999:\tlearn: 0.0090269\ttest: 0.1759757\tbest: 0.1759755 (998)\ttotal: 4.32s\tremaining: 0us\n","\n","bestTest = 0.1759755197\n","bestIteration = 998\n","\n","Shrink model to first 999 iterations.\n","\n","Final Model Performance (Bi-LSTM vs Bi-LSTM + CatBoost)\n","\n","               Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","                    2      Train 0.329173 0.175789 0.419272   -0.004389 125.800671\n","                    2 Validation 1.299941 1.695946 1.302285 -287.350592  74.377542\n","                    2       Test 1.574359 2.484899 1.576356 -410.743231  78.074273\n","                    3      Train 0.327482 0.175139 0.418496   -0.000675 124.865528\n","                    3 Validation 1.293804 1.679814 1.296076 -284.607657  74.028969\n","                    3       Test 1.562905 2.448711 1.564836 -404.746996  77.508362\n","                    5      Train 0.327128 0.175039 0.418377   -0.000103 124.624730\n","                    5 Validation 1.293998 1.680314 1.296269 -284.692703  74.040137\n","                    5       Test 1.563061 2.449197 1.564991 -404.827598  77.516136\n","Bi-LSTM(3) + CatBoost      Train 0.005211 0.000081 0.009027    0.999534   1.378785\n","Bi-LSTM(3) + CatBoost Validation 0.158502 0.030967 0.175976   -4.265181   8.898315\n","Bi-LSTM(3) + CatBoost       Test 0.427413 0.188717 0.434416  -30.270109  21.085304\n","\n","Best Overall Model: Bi-LSTM(3) + CatBoost (Based on Lowest Validation MAPE)\n"]}]},{"cell_type":"markdown","source":["### optuna"],"metadata":{"id":"2Y8xq-_5CpA1"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","import optuna\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(BiLSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size * 2, 1)  # Bi-directional => hidden_size * 2\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # *2 for bidirectional\n","        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        return self.fc(out[:, -1, :])\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Optuna objective function\n","def objective(trial):\n","    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128])\n","    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n","    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n","\n","    model = BiLSTMModel(input_size=3, hidden_size=hidden_size, num_layers=num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(50):  # Reduce epochs for faster tuning\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    with torch.no_grad():\n","        val_pred = model(X_val_torch).cpu().numpy()\n","\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    _, _, _, _, val_mape = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","\n","    return val_mape\n","\n","# Run Optuna optimization\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=20)\n","\n","# Get best hyperparameters\n","best_params = study.best_params\n","print(f\"\\nBest Bi-LSTM Hyperparameters: {best_params}\")\n","\n","# Train Best Bi-LSTM Model\n","best_bilstm = BiLSTMModel(input_size=3, hidden_size=best_params[\"hidden_size\"], num_layers=best_params[\"num_layers\"]).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(best_bilstm.parameters(), lr=best_params[\"learning_rate\"])\n","\n","for epoch in range(100):\n","    best_bilstm.train()\n","    outputs = best_bilstm(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","# Evaluate Best Bi-LSTM Model\n","best_bilstm.eval()\n","with torch.no_grad():\n","    train_pred = best_bilstm(X_train_torch).cpu().numpy()\n","    val_pred = best_bilstm(X_val_torch).cpu().numpy()\n","    test_pred = best_bilstm(X_test_torch).cpu().numpy()\n","\n","# Inverse transform predictions\n","train_pred_actual = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","test_pred_actual = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","# Compute metrics\n","metrics_train = compute_metrics(Y_train.values.flatten(), train_pred_actual.flatten())\n","metrics_val = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","metrics_test = compute_metrics(Y_test.values.flatten(), test_pred_actual.flatten())\n","\n","# Store Bi-LSTM embeddings\n","bilstm_outputs = {\n","    \"train\": train_pred_actual,\n","    \"val\": val_pred_actual,\n","    \"test\": test_pred_actual\n","}\n","\n","# Display Bi-LSTM Results\n","print(\"\\nBi-LSTM Model Performance (Best Hyperparameters):\\n\")\n","print(f\"Train: {metrics_train}\\nValidation: {metrics_val}\\nTest: {metrics_test}\")\n","\n","# ---------- CatBoost on Best Bi-LSTM Embeddings ----------\n","print(\"\\nTraining CatBoost on Best Bi-LSTM Embeddings...\")\n","\n","# Use Bi-LSTM embeddings as input for CatBoost\n","X_train_cat = bilstm_outputs[\"train\"]\n","X_val_cat = bilstm_outputs[\"val\"]\n","X_test_cat = bilstm_outputs[\"test\"]\n","\n","# CatBoost Model\n","cat_model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, loss_function=\"RMSE\", verbose=100)\n","cat_model.fit(X_train_cat, Y_train.values, eval_set=(X_val_cat, Y_val.values), early_stopping_rounds=100)\n","\n","# Predictions\n","train_pred_cat = cat_model.predict(X_train_cat)\n","val_pred_cat = cat_model.predict(X_val_cat)\n","test_pred_cat = cat_model.predict(X_test_cat)\n","\n","# Compute metrics for CatBoost\n","metrics_train_cat = compute_metrics(Y_train.values.flatten(), train_pred_cat.flatten())\n","metrics_val_cat = compute_metrics(Y_val.values.flatten(), val_pred_cat.flatten())\n","metrics_test_cat = compute_metrics(Y_test.values.flatten(), test_pred_cat.flatten())\n","\n","# Display Final Results\n","print(\"\\nFinal Model Performance (Bi-LSTM vs Bi-LSTM + CatBoost)\\n\")\n","print(f\"Train: {metrics_train_cat}\\nValidation: {metrics_val_cat}\\nTest: {metrics_test_cat}\")\n","\n","# Best Model Selection\n","best_model = \"Bi-LSTM + CatBoost\" if metrics_val_cat[4] < metrics_val[4] else \"Bi-LSTM Only\"\n","print(f\"\\nBest Overall Model: {best_model} (Based on Lowest Validation MAPE)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-Y2hShOBx4U","executionInfo":{"status":"ok","timestamp":1739445929650,"user_tz":-330,"elapsed":98991,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"dd25baa0-7a29-4534-fe6b-cee4b5cba2e2"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:23:52,988] A new study created in memory with name: no-name-207cd5da-a387-4f42-8bee-34e212ad8413\n","[I 2025-02-13 11:23:56,949] Trial 0 finished with value: 73.24154832223407 and parameters: {'hidden_size': 64, 'num_layers': 5, 'learning_rate': 0.002925754877493491}. Best is trial 0 with value: 73.24154832223407.\n","[I 2025-02-13 11:24:07,531] Trial 1 finished with value: 73.35417958185919 and parameters: {'hidden_size': 128, 'num_layers': 4, 'learning_rate': 0.0019432425149269342}. Best is trial 0 with value: 73.24154832223407.\n","[I 2025-02-13 11:24:08,208] Trial 2 finished with value: 71.77385283640025 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0013283048016925125}. Best is trial 2 with value: 71.77385283640025.\n","[I 2025-02-13 11:24:11,753] Trial 3 finished with value: 73.13190834486093 and parameters: {'hidden_size': 64, 'num_layers': 5, 'learning_rate': 0.0028865533935638573}. Best is trial 2 with value: 71.77385283640025.\n","[I 2025-02-13 11:24:25,359] Trial 4 finished with value: 74.01925158766574 and parameters: {'hidden_size': 128, 'num_layers': 5, 'learning_rate': 0.003529979573932025}. Best is trial 2 with value: 71.77385283640025.\n","[I 2025-02-13 11:24:30,941] Trial 5 finished with value: 78.75148521973891 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.00013472241764455787}. Best is trial 2 with value: 71.77385283640025.\n","[I 2025-02-13 11:24:39,144] Trial 6 finished with value: 66.06607423960179 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0002693375602077078}. Best is trial 6 with value: 66.06607423960179.\n","[I 2025-02-13 11:24:42,731] Trial 7 finished with value: 73.68409448895551 and parameters: {'hidden_size': 64, 'num_layers': 5, 'learning_rate': 0.009626024236340132}. Best is trial 6 with value: 66.06607423960179.\n","[I 2025-02-13 11:24:44,351] Trial 8 finished with value: 74.92516782931423 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.00292151014917222}. Best is trial 6 with value: 66.06607423960179.\n","[I 2025-02-13 11:24:55,008] Trial 9 finished with value: 74.31618958476658 and parameters: {'hidden_size': 128, 'num_layers': 4, 'learning_rate': 0.00044221457224124047}. Best is trial 6 with value: 66.06607423960179.\n","[I 2025-02-13 11:24:55,923] Trial 10 finished with value: 86.36920648325828 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.00032999438491432623}. Best is trial 6 with value: 66.06607423960179.\n","[I 2025-02-13 11:24:56,857] Trial 11 finished with value: 68.40958233696145 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0007021972725000749}. Best is trial 6 with value: 66.06607423960179.\n","[I 2025-02-13 11:24:57,786] Trial 12 finished with value: 72.15826281968526 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.000530889803063204}. Best is trial 6 with value: 66.06607423960179.\n","[I 2025-02-13 11:24:58,720] Trial 13 finished with value: 95.65649872381773 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.00014762568657013508}. Best is trial 6 with value: 66.06607423960179.\n","[I 2025-02-13 11:25:06,816] Trial 14 finished with value: 73.58539284265831 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0008286313156333877}. Best is trial 6 with value: 66.06607423960179.\n","[I 2025-02-13 11:25:07,921] Trial 15 finished with value: 80.56707059730586 and parameters: {'hidden_size': 32, 'num_layers': 4, 'learning_rate': 0.0002344820314673129}. Best is trial 6 with value: 66.06607423960179.\n","[I 2025-02-13 11:25:08,823] Trial 16 finished with value: 65.88007191272008 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.000644812874377265}. Best is trial 16 with value: 65.88007191272008.\n","[I 2025-02-13 11:25:19,434] Trial 17 finished with value: 72.53452596577117 and parameters: {'hidden_size': 128, 'num_layers': 4, 'learning_rate': 0.00027175200017299784}. Best is trial 16 with value: 65.88007191272008.\n","[I 2025-02-13 11:25:20,133] Trial 18 finished with value: 101.24840182729768 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.00010373330884297462}. Best is trial 16 with value: 65.88007191272008.\n","[I 2025-02-13 11:25:28,287] Trial 19 finished with value: 75.50160331489519 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.001237972499159244}. Best is trial 16 with value: 65.88007191272008.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Best Bi-LSTM Hyperparameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.000644812874377265}\n","\n","Bi-LSTM Model Performance (Best Hyperparameters):\n","\n","Train: (0.32683288987496467, 0.1744029795416491, 0.4176158276953223, 0.0035310996003891093, 124.62617348662235)\n","Validation: (1.2914852984509062, 1.6738109496480915, 1.2937584587735422, -283.58707968360284, 73.89607738874251)\n","Test: (1.5604630193532787, 2.441078961798212, 1.562395264265164, -403.48237692783965, 77.38704655921136)\n","\n","Training CatBoost on Best Bi-LSTM Embeddings...\n","0:\tlearn: 0.3980532\ttest: 1.2474559\tbest: 1.2474559 (0)\ttotal: 1.7ms\tremaining: 1.7s\n","100:\tlearn: 0.0080367\ttest: 0.1988989\tbest: 0.1988989 (100)\ttotal: 156ms\tremaining: 1.39s\n","200:\tlearn: 0.0056610\ttest: 0.1772462\tbest: 0.1772389 (199)\ttotal: 297ms\tremaining: 1.18s\n","300:\tlearn: 0.0050505\ttest: 0.1758311\tbest: 0.1758311 (300)\ttotal: 434ms\tremaining: 1.01s\n","400:\tlearn: 0.0047966\ttest: 0.1754897\tbest: 0.1754742 (392)\ttotal: 574ms\tremaining: 858ms\n","500:\tlearn: 0.0046578\ttest: 0.1752792\tbest: 0.1752753 (499)\ttotal: 713ms\tremaining: 710ms\n","600:\tlearn: 0.0045722\ttest: 0.1751345\tbest: 0.1751345 (600)\ttotal: 853ms\tremaining: 567ms\n","700:\tlearn: 0.0045244\ttest: 0.1750608\tbest: 0.1750608 (700)\ttotal: 989ms\tremaining: 422ms\n","800:\tlearn: 0.0044897\ttest: 0.1750092\tbest: 0.1750070 (795)\ttotal: 1.15s\tremaining: 286ms\n","900:\tlearn: 0.0044677\ttest: 0.1749724\tbest: 0.1749724 (900)\ttotal: 1.29s\tremaining: 142ms\n","999:\tlearn: 0.0044532\ttest: 0.1749228\tbest: 0.1749228 (998)\ttotal: 1.43s\tremaining: 0us\n","\n","bestTest = 0.1749227798\n","bestIteration = 998\n","\n","Shrink model to first 999 iterations.\n","\n","Final Model Performance (Bi-LSTM vs Bi-LSTM + CatBoost)\n","\n","Train: (0.0028078491831470672, 1.9832260308205942e-05, 0.00445334259946458, 0.9998866863933535, 0.7921117002223671)\n","Validation: (0.1572734625459855, 0.030597978969442517, 0.17492278001861997, -4.2023733510434385, 8.82748107020966)\n","Test: (0.4262519991356341, 0.1877258355390669, 0.4332733958357782, -30.10583203489314, 21.02759516841066)\n","\n","Best Overall Model: Bi-LSTM + CatBoost (Based on Lowest Validation MAPE)\n"]}]},{"cell_type":"markdown","source":["### BOHB"],"metadata":{"id":"EluDHIyzDV_j"}},{"cell_type":"code","source":["import numpy as np\n","import catboost as cb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-directional => hidden_dim * 2\n","\n","    def forward(self, x):\n","        out, _ = self.bilstm(x)\n","        out = self.fc(out[:, -1, :])  # Take last timestep\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Bi-LSTM Configurations (2, 3, and 5 layers)\n","bilstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store Bi-LSTM feature representations\n","bilstm_features = {}\n","\n","for num_layers in bilstm_layers:\n","    print(f\"Training Bi-LSTM with {num_layers} layers...\")\n","\n","    bilstm_model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(bilstm_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        bilstm_model.train()\n","        optimizer.zero_grad()\n","        outputs = bilstm_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Extract Feature Representations\n","    bilstm_model.eval()\n","    with torch.no_grad():\n","        train_features = bilstm_model(X_train_torch).numpy()\n","        val_features = bilstm_model(X_val_torch).numpy()\n","        test_features = bilstm_model(X_test_torch).numpy()\n","\n","    bilstm_features[num_layers] = (train_features, val_features, test_features)\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"iterations\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"bagging_temperature\", 0.0, 1.0, default_value=0.8))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"colsample_bylevel\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for CatBoost\n","class CatBoostWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = cb.CatBoostRegressor(\n","            iterations=config[\"iterations\"],\n","            learning_rate=config[\"learning_rate\"],\n","            depth=config[\"depth\"],\n","            bagging_temperature=config[\"bagging_temperature\"],\n","            colsample_bylevel=config[\"colsample_bylevel\"],\n","            random_seed=42,\n","            verbose=0\n","        )\n","        model.fit(self.train_features, Y_train)\n","        Y_val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB for each Bi-LSTM configuration\n","best_models = {}\n","\n","for num_layers in bilstm_layers:\n","    print(f\"\\nRunning BOHB for Bi-LSTM ({num_layers} layers) + CatBoost...\")\n","\n","    train_features, val_features, test_features = bilstm_features[num_layers]\n","\n","    # Start NameServer\n","    NS = hpns.NameServer(run_id=f\"bilstm_{num_layers}_catboost_bohb\", host=\"127.0.0.1\", port=None)\n","    NS.start()\n","\n","    worker = CatBoostWorker(\n","        train_features=train_features,\n","        val_features=val_features,\n","        nameserver=\"127.0.0.1\",\n","        run_id=f\"bilstm_{num_layers}_catboost_bohb\"\n","    )\n","    worker.run(background=True)\n","\n","    bohb = BOHB(\n","        configspace=get_config_space(),\n","        run_id=f\"bilstm_{num_layers}_catboost_bohb\",\n","        nameserver=\"127.0.0.1\",\n","        min_budget=1,\n","        max_budget=3\n","    )\n","\n","    res = bohb.run(n_iterations=50)\n","\n","    # Shutdown BOHB\n","    bohb.shutdown()\n","    NS.shutdown()\n","\n","    # Retrieve Best Configuration\n","    best_config = res.get_incumbent_id()\n","    best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","    # Train Best CatBoost Model on Bi-LSTM Features\n","    best_catboost_model = cb.CatBoostRegressor(\n","        iterations=best_params[\"iterations\"],\n","        learning_rate=best_params[\"learning_rate\"],\n","        depth=best_params[\"depth\"],\n","        bagging_temperature=best_params[\"bagging_temperature\"],\n","        colsample_bylevel=best_params[\"colsample_bylevel\"],\n","        random_seed=42,\n","        verbose=0\n","    )\n","\n","    best_catboost_model.fit(train_features, Y_train)\n","\n","    # Make Predictions\n","    Y_train_pred = best_catboost_model.predict(train_features)\n","    Y_val_pred = best_catboost_model.predict(val_features)\n","    Y_test_pred = best_catboost_model.predict(test_features)\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store best model and metrics\n","    best_models[num_layers] = {\n","        \"params\": best_params,\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics\n","    }\n","\n","    # Print Results\n","    print(f\"\\nBest Parameters for Bi-LSTM ({num_layers} layers) + CatBoost:\")\n","    print(best_params)\n","\n","    print(\"\\nTraining set metrics:\")\n","    print(train_metrics)\n","\n","    print(\"\\nValidation set metrics:\")\n","    print(val_metrics)\n","\n","    print(\"\\nTest set metrics:\")\n","    print(test_metrics)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PD9vXx6bDP7v","executionInfo":{"status":"ok","timestamp":1739446708459,"user_tz":-330,"elapsed":583579,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"6ceef4fa-5d24-4ae7-b8ac-1c8a0564abad"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Bi-LSTM with 2 layers...\n","Training Bi-LSTM with 3 layers...\n","Training Bi-LSTM with 5 layers...\n","\n","Running BOHB for Bi-LSTM (2 layers) + CatBoost...\n","\n","Best Parameters for Bi-LSTM (2 layers) + CatBoost:\n","{'bagging_temperature': 0.5488155518478, 'colsample_bylevel': 0.9577685831541, 'depth': 9, 'iterations': 471, 'learning_rate': 0.2826543846672}\n","\n","Training set metrics:\n","(0.0035974328653817647, 3.0181861275629613e-05, 0.005493802078308757, 0.9998275529111006, 1.0361633306967521)\n","\n","Validation set metrics:\n","(0.15750859843334464, 0.030671020072395385, 0.17513143656235847, -4.214792049935686, 8.840991623781303)\n","\n","Test set metrics:\n","(0.4264838004683717, 0.18792350083388817, 0.4335014427125799, -30.138584817386725, 21.039113477536315)\n","\n","Running BOHB for Bi-LSTM (3 layers) + CatBoost...\n","\n","Best Parameters for Bi-LSTM (3 layers) + CatBoost:\n","{'bagging_temperature': 0.2762614862038, 'colsample_bylevel': 0.9827069728425, 'depth': 9, 'iterations': 481, 'learning_rate': 0.2800476871542}\n","\n","Training set metrics:\n","(0.003594794634702393, 2.9922966959453818e-05, 0.0054701889327018515, 0.9998290321297196, 1.0380086198607061)\n","\n","Validation set metrics:\n","(0.157505786940121, 0.030670126132780046, 0.17512888434744295, -4.2146400592556175, 8.840830489924913)\n","\n","Test set metrics:\n","(0.4264809615793554, 0.18792107936159388, 0.43349864978058916, -30.138183583799208, 21.038972411905338)\n","\n","Running BOHB for Bi-LSTM (5 layers) + CatBoost...\n","\n","Best Parameters for Bi-LSTM (5 layers) + CatBoost:\n","{'bagging_temperature': 0.5476986093617, 'colsample_bylevel': 0.9894229419314, 'depth': 9, 'iterations': 243, 'learning_rate': 0.270553582584}\n","\n","Training set metrics:\n","(0.0036450723378588564, 3.0812073720193486e-05, 0.005550862430306978, 0.9998239521291454, 1.055629560090807)\n","\n","Validation set metrics:\n","(0.15752877860734987, 0.03067743701370143, 0.17514975596243756, -4.2158830803099185, 8.842148201087884)\n","\n","Test set metrics:\n","(0.42650417728231854, 0.18794088201120968, 0.4335214896763593, -30.141464847089505, 21.040126010291253)\n"]}]},{"cell_type":"markdown","source":["## Bi-LSTM [Lightboost]"],"metadata":{"id":"mFFhguZrEZG4"}},{"cell_type":"markdown","source":["### initial"],"metadata":{"id":"Cr-pFI6IFz3N"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import lightgbm as lgb\n","import pandas as pd\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(BiLSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size * 2, 1)  # Bi-directional (2x hidden size)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # Bi-directional\n","        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.bilstm(x, (h0, c0))\n","        return self.fc(out[:, -1, :])  # Take last time step output\n","\n","# Set Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]  # Different Bi-LSTM layers\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100  # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# DataFrame to store results\n","columns = [\"Layers\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\"]\n","results_df = pd.DataFrame(columns=columns)\n","\n","# Train multiple Bi-LSTM models\n","bilstm_outputs = {}  # Store Bi-LSTM embeddings for LGBM\n","\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining Bi-LSTM with {num_layers} layers...\")\n","\n","    # Initialize model, loss function, and optimizer\n","    model = BiLSTMModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        train_pred = model(X_train_torch).cpu().numpy()\n","        val_pred = model(X_val_torch).cpu().numpy()\n","        test_pred = model(X_test_torch).cpu().numpy()\n","\n","    # Inverse transform predictions\n","    train_pred_actual = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    test_pred_actual = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","    # Compute metrics for each dataset\n","    metrics_train = compute_metrics(Y_train.values.flatten(), train_pred_actual.flatten())\n","    metrics_val = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","    metrics_test = compute_metrics(Y_test.values.flatten(), test_pred_actual.flatten())\n","\n","    # Append results to DataFrame\n","    results_df = pd.concat([\n","        results_df,\n","        pd.DataFrame([\n","            [num_layers, \"Train\", *metrics_train],\n","            [num_layers, \"Validation\", *metrics_val],\n","            [num_layers, \"Test\", *metrics_test]\n","        ], columns=columns)\n","    ], ignore_index=True)\n","\n","    # Store Bi-LSTM embeddings for LGBM\n","    bilstm_outputs[num_layers] = {\n","        \"train\": train_pred_actual,\n","        \"val\": val_pred_actual,\n","        \"test\": test_pred_actual\n","    }\n","\n","# Find the best Bi-LSTM model (Lowest Validation MAPE)\n","best_model = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","best_layers = int(best_model[\"Layers\"])\n","\n","# Display Bi-LSTM Results\n","print(\"\\nBi-LSTM Model Performance Comparison (2, 3, and 5 Layers)\\n\")\n","print(results_df.to_string(index=False))\n","\n","print(f\"\\nBest Bi-LSTM Model: {best_layers} Layers (Based on Lowest Validation MAPE)\\n\")\n","\n","# ---------- LightGBM on Best Bi-LSTM Embeddings ----------\n","print(\"\\nTraining LightGBM on Best Bi-LSTM Embeddings...\")\n","\n","# Use the best Bi-LSTM's output as features for LGBM\n","X_train_lgb = bilstm_outputs[best_layers][\"train\"]\n","X_val_lgb = bilstm_outputs[best_layers][\"val\"]\n","X_test_lgb = bilstm_outputs[best_layers][\"test\"]\n","\n","# LightGBM Dataset\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","\n","# LGBM Parameters\n","lgb_params = {\n","    \"objective\": \"regression\",\n","    \"metric\": \"rmse\",\n","    \"boosting_type\": \"gbdt\",\n","    \"learning_rate\": 0.05,\n","    \"num_leaves\": 31\n","}\n","\n","# Train LGBM\n","lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","\n","# Predictions\n","train_pred_lgb = lgb_model.predict(X_train_lgb)\n","val_pred_lgb = lgb_model.predict(X_val_lgb)\n","test_pred_lgb = lgb_model.predict(X_test_lgb)\n","\n","# Compute metrics for LGBM\n","metrics_train_lgb = compute_metrics(Y_train.values.flatten(), train_pred_lgb.flatten())\n","metrics_val_lgb = compute_metrics(Y_val.values.flatten(), val_pred_lgb.flatten())\n","metrics_test_lgb = compute_metrics(Y_test.values.flatten(), test_pred_lgb.flatten())\n","\n","# Append LGBM results to DataFrame\n","results_df = pd.concat([\n","    results_df,\n","    pd.DataFrame([\n","        [f\"Bi-LSTM({best_layers}) + LGBM\", \"Train\", *metrics_train_lgb],\n","        [f\"Bi-LSTM({best_layers}) + LGBM\", \"Validation\", *metrics_val_lgb],\n","        [f\"Bi-LSTM({best_layers}) + LGBM\", \"Test\", *metrics_test_lgb]\n","    ], columns=columns)\n","], ignore_index=True)\n","\n","# Display Final Results\n","print(\"\\nFinal Model Performance (Bi-LSTM vs Bi-LSTM + LGBM)\\n\")\n","print(results_df.to_string(index=False))\n","\n","# Best Model Selection\n","best_overall = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","print(f\"\\nBest Overall Model: {best_overall['Layers']} (Based on Lowest Validation MAPE)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ZN4F_zQEX6J","executionInfo":{"status":"ok","timestamp":1739446797522,"user_tz":-330,"elapsed":26157,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"2dfa9cc4-8026-4b58-8126-4fba5f24b65a"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training Bi-LSTM with 2 layers...\n","Epoch [10/100], Loss: 0.1079\n","Epoch [20/100], Loss: 0.0741\n","Epoch [30/100], Loss: 0.0755\n","Epoch [40/100], Loss: 0.0720\n","Epoch [50/100], Loss: 0.0719\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training Bi-LSTM with 3 layers...\n","Epoch [10/100], Loss: 0.0759\n","Epoch [20/100], Loss: 0.0741\n","Epoch [30/100], Loss: 0.0716\n","Epoch [40/100], Loss: 0.0717\n","Epoch [50/100], Loss: 0.0715\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training Bi-LSTM with 5 layers...\n","Epoch [10/100], Loss: 0.0880\n","Epoch [20/100], Loss: 0.0762\n","Epoch [30/100], Loss: 0.0716\n","Epoch [40/100], Loss: 0.0719\n","Epoch [50/100], Loss: 0.0716\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Bi-LSTM Model Performance Comparison (2, 3, and 5 Layers)\n","\n","Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","     2      Train 0.328529 0.175300 0.418689   -0.001595 125.457876\n","     2 Validation 1.299678 1.695286 1.302031 -287.238343  74.362033\n","     2       Test 1.574787 2.486285 1.576796 -410.972869  78.095076\n","     3      Train 0.327429 0.175158 0.418519   -0.000784 124.805690\n","     3 Validation 1.294476 1.681567 1.296753 -284.905808  74.067245\n","     3       Test 1.563962 2.452032 1.565897 -405.297328  77.560612\n","     5      Train 0.327104 0.174901 0.418212    0.000684 124.657877\n","     5 Validation 1.293296 1.678493 1.295567 -284.383191  73.999925\n","     5       Test 1.562271 2.446723 1.564200 -404.417579  77.476905\n","\n","Best Bi-LSTM Model: 5 Layers (Based on Lowest Validation MAPE)\n","\n","\n","Training LightGBM on Best Bi-LSTM Embeddings...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000505 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0327292\tvalid_1's rmse: 0.257892\n","[100]\ttraining's rmse: 0.00603659\tvalid_1's rmse: 0.178435\n","[150]\ttraining's rmse: 0.00550188\tvalid_1's rmse: 0.17239\n","[200]\ttraining's rmse: 0.00549827\tvalid_1's rmse: 0.171903\n","\n","Final Model Performance (Bi-LSTM vs Bi-LSTM + LGBM)\n","\n","           Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","                2      Train 0.328529 0.175300 0.418689   -0.001595 125.457876\n","                2 Validation 1.299678 1.695286 1.302031 -287.238343  74.362033\n","                2       Test 1.574787 2.486285 1.576796 -410.972869  78.095076\n","                3      Train 0.327429 0.175158 0.418519   -0.000784 124.805690\n","                3 Validation 1.294476 1.681567 1.296753 -284.905808  74.067245\n","                3       Test 1.563962 2.452032 1.565897 -405.297328  77.560612\n","                5      Train 0.327104 0.174901 0.418212    0.000684 124.657877\n","                5 Validation 1.293296 1.678493 1.295567 -284.383191  73.999925\n","                5       Test 1.562271 2.446723 1.564200 -404.417579  77.476905\n","Bi-LSTM(5) + LGBM      Train 0.003565 0.000030 0.005498    0.999827   1.033336\n","Bi-LSTM(5) + LGBM Validation 0.153971 0.029551 0.171903   -4.024296   8.638401\n","Bi-LSTM(5) + LGBM       Test 0.422885 0.184866 0.429961  -29.632038  20.860268\n","\n","Best Overall Model: Bi-LSTM(5) + LGBM (Based on Lowest Validation MAPE)\n"]}]},{"cell_type":"markdown","source":["### optuna"],"metadata":{"id":"XgGykmgKHCri"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import lightgbm as lgb\n","import pandas as pd\n","import optuna\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(BiLSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size * 2, 1)  # Bi-directional doubles the hidden size\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        return self.fc(out[:, -1, :])\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# ----------- OPTUNA OPTIMIZATION FUNCTION -----------\n","\n","def objective(trial):\n","    # Sample Bi-LSTM hyperparameters\n","    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128, step=16)\n","    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n","\n","    # Initialize model, loss function, and optimizer\n","    model = BiLSTMModel(input_size=3, hidden_size=hidden_size, num_layers=num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    num_epochs = 50\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        val_pred = model(X_val_torch).cpu().numpy()\n","\n","    # Inverse transform predictions\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","\n","    # Compute MAPE (minimization objective)\n","    _, _, _, _, mape = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","\n","    return mape  # Optuna minimizes MAPE\n","\n","# Run Optuna for Bi-LSTM\n","study_bilstm = optuna.create_study(direction=\"minimize\")\n","study_bilstm.optimize(objective, n_trials=20)\n","\n","# Best Bi-LSTM Model Parameters\n","best_bilstm_params = study_bilstm.best_params\n","print(\"\\nBest Bi-LSTM Model:\", best_bilstm_params)\n","\n","# ----------- Train Best Bi-LSTM and Get Embeddings -----------\n","\n","best_bilstm = BiLSTMModel(input_size=3, hidden_size=best_bilstm_params[\"hidden_size\"], num_layers=best_bilstm_params[\"num_layers\"]).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(best_bilstm.parameters(), lr=best_bilstm_params[\"learning_rate\"])\n","\n","# Train Best Bi-LSTM\n","for epoch in range(50):\n","    best_bilstm.train()\n","    outputs = best_bilstm(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","# Get Bi-LSTM embeddings\n","best_bilstm.eval()\n","with torch.no_grad():\n","    train_pred = best_bilstm(X_train_torch).cpu().numpy()\n","    val_pred = best_bilstm(X_val_torch).cpu().numpy()\n","    test_pred = best_bilstm(X_test_torch).cpu().numpy()\n","\n","# Inverse transform predictions\n","X_train_lgb = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","X_val_lgb = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","X_test_lgb = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","# ----------- OPTUNA OPTIMIZATION FOR LIGHTGBM -----------\n","\n","def objective_lgb(trial):\n","    params = {\n","        \"objective\": \"regression\",\n","        \"metric\": \"rmse\",\n","        \"boosting_type\": \"gbdt\",\n","        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 50),\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n","        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 30)\n","    }\n","\n","    lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","    lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","\n","    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","\n","    val_pred = model.predict(X_val_lgb)\n","    _, _, _, _, mape = compute_metrics(Y_val.values.flatten(), val_pred.flatten())\n","\n","    return mape  # Optuna minimizes MAPE\n","\n","# Run Optuna for LGBM\n","study_lgb = optuna.create_study(direction=\"minimize\")\n","study_lgb.optimize(objective_lgb, n_trials=20)\n","\n","# Best LGBM Model Parameters\n","best_lgb_params = study_lgb.best_params\n","print(\"\\nBest LightGBM Model:\", best_lgb_params)\n","\n","# Train Final LightGBM Model\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","\n","final_lgb = lgb.train(best_lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","\n","# Predictions\n","train_pred_lgb = final_lgb.predict(X_train_lgb)\n","val_pred_lgb = final_lgb.predict(X_val_lgb)\n","test_pred_lgb = final_lgb.predict(X_test_lgb)\n","\n","# Compute metrics\n","metrics_train_lgb = compute_metrics(Y_train.values.flatten(), train_pred_lgb.flatten())\n","metrics_val_lgb = compute_metrics(Y_val.values.flatten(), val_pred_lgb.flatten())\n","metrics_test_lgb = compute_metrics(Y_test.values.flatten(), test_pred_lgb.flatten())\n","\n","print(\"\\nFinal Bi-LSTM + LightGBM Performance:\")\n","print(\"Train:\", metrics_train_lgb)\n","print(\"Validation:\", metrics_val_lgb)\n","print(\"Test:\", metrics_test_lgb)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrcN1eY_G1t6","executionInfo":{"status":"ok","timestamp":1739447041020,"user_tz":-330,"elapsed":73179,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"35dbae61-fa1d-4b17-82df-7bc13649eb9f"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:42:50,180] A new study created in memory with name: no-name-2c9b39e7-ec8d-4c27-aa90-62f453f115fa\n","[I 2025-02-13 11:42:51,494] Trial 0 finished with value: 65.96145833836951 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0005810662956926598}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:42:56,596] Trial 1 finished with value: 74.33416777285065 and parameters: {'hidden_size': 96, 'num_layers': 3, 'learning_rate': 0.004190507737909965}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:42:58,587] Trial 2 finished with value: 73.82826630362119 and parameters: {'hidden_size': 48, 'num_layers': 3, 'learning_rate': 0.0095897632099731}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:04,613] Trial 3 finished with value: 74.6627267901344 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.006302973092539772}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:06,044] Trial 4 finished with value: 75.13803727624936 and parameters: {'hidden_size': 48, 'num_layers': 3, 'learning_rate': 0.0034284819541336555}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:07,208] Trial 5 finished with value: 74.01928406514887 and parameters: {'hidden_size': 48, 'num_layers': 2, 'learning_rate': 0.0010625657550602779}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:13,239] Trial 6 finished with value: 74.56721100586707 and parameters: {'hidden_size': 96, 'num_layers': 4, 'learning_rate': 0.0001672650016612898}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:22,768] Trial 7 finished with value: 74.88108534263091 and parameters: {'hidden_size': 112, 'num_layers': 5, 'learning_rate': 0.0012929848732248432}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:30,418] Trial 8 finished with value: 74.05104055723044 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0054362911213849015}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:33,926] Trial 9 finished with value: 71.99734431281315 and parameters: {'hidden_size': 64, 'num_layers': 5, 'learning_rate': 0.0003847688976624563}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:35,017] Trial 10 finished with value: 85.67671592457636 and parameters: {'hidden_size': 32, 'num_layers': 4, 'learning_rate': 0.00039033515856647956}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:38,510] Trial 11 finished with value: 71.42121070323466 and parameters: {'hidden_size': 64, 'num_layers': 5, 'learning_rate': 0.000399933678207291}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:39,606] Trial 12 finished with value: 87.7300228736739 and parameters: {'hidden_size': 32, 'num_layers': 4, 'learning_rate': 0.0003598924681041598}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:44,234] Trial 13 finished with value: 89.05409569053472 and parameters: {'hidden_size': 80, 'num_layers': 5, 'learning_rate': 0.000110184728947817}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:47,117] Trial 14 finished with value: 75.10494225133749 and parameters: {'hidden_size': 64, 'num_layers': 4, 'learning_rate': 0.0006788621362325955}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:48,686] Trial 15 finished with value: 73.37470612747597 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0017735336328761643}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:50,004] Trial 16 finished with value: 82.2292061457555 and parameters: {'hidden_size': 32, 'num_layers': 5, 'learning_rate': 0.0002163008018899935}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:53,801] Trial 17 finished with value: 74.90757001036204 and parameters: {'hidden_size': 80, 'num_layers': 4, 'learning_rate': 0.0006398247052181724}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:55,125] Trial 18 finished with value: 72.76688917588403 and parameters: {'hidden_size': 48, 'num_layers': 3, 'learning_rate': 0.0019047484012245802}. Best is trial 0 with value: 65.96145833836951.\n","[I 2025-02-13 11:43:56,731] Trial 19 finished with value: 68.47371259664932 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.000582663569274417}. Best is trial 0 with value: 65.96145833836951.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Best Bi-LSTM Model: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0005810662956926598}\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:43:57,657] A new study created in memory with name: no-name-b853f1a7-9e7f-4b9b-bd28-22fd840a42c5\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.129609\tvalid_1's rmse: 0.516168\n","[100]\ttraining's rmse: 0.0404497\tvalid_1's rmse: 0.277452\n","[150]\ttraining's rmse: 0.0135326\tvalid_1's rmse: 0.205513\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:43:57,944] Trial 0 finished with value: 9.388178685176745 and parameters: {'num_leaves': 50, 'learning_rate': 0.02318593982807697, 'max_depth': 10, 'min_data_in_leaf': 10}. Best is trial 0 with value: 9.388178685176745.\n"]},{"output_type":"stream","name":"stdout","text":["[200]\ttraining's rmse: 0.00662389\tvalid_1's rmse: 0.183806\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000171 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.146501\tvalid_1's rmse: 0.563857\n","[100]\ttraining's rmse: 0.0515211\tvalid_1's rmse: 0.30778\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:43:58,213] Trial 1 finished with value: 9.783983640556343 and parameters: {'num_leaves': 41, 'learning_rate': 0.020788337592759658, 'max_depth': 10, 'min_data_in_leaf': 19}. Best is trial 0 with value: 9.388178685176745.\n"]},{"output_type":"stream","name":"stdout","text":["[150]\ttraining's rmse: 0.018727\tvalid_1's rmse: 0.220096\n","[200]\ttraining's rmse: 0.00827725\tvalid_1's rmse: 0.190108\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000130 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[50]\ttraining's rmse: 0.0365886\tvalid_1's rmse: 0.269326\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:43:58,444] Trial 2 finished with value: 8.785363864378352 and parameters: {'num_leaves': 33, 'learning_rate': 0.04781283066332281, 'max_depth': 5, 'min_data_in_leaf': 23}. Best is trial 2 with value: 8.785363864378352.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[100]\ttraining's rmse: 0.00623922\tvalid_1's rmse: 0.18236\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00540303\tvalid_1's rmse: 0.174902\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00539539\tvalid_1's rmse: 0.174249\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000164 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[50]\ttraining's rmse: 0.216208\tvalid_1's rmse: 0.76089\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:43:58,701] Trial 3 finished with value: 14.129720627047753 and parameters: {'num_leaves': 47, 'learning_rate': 0.013164449104261794, 'max_depth': 4, 'min_data_in_leaf': 5}. Best is trial 2 with value: 8.785363864378352.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[100]\ttraining's rmse: 0.111821\tvalid_1's rmse: 0.485557\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.0579822\tvalid_1's rmse: 0.339247\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.0303336\tvalid_1's rmse: 0.261214\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000199 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.00613784\tvalid_1's rmse: 0.181551\n","[100]\ttraining's rmse: 0.00539474\tvalid_1's rmse: 0.174242\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00539465\tvalid_1's rmse: 0.174186\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00539465\tvalid_1's rmse: 0.174185\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:43:58,992] Trial 4 finished with value: 8.781365066291881 and parameters: {'num_leaves': 47, 'learning_rate': 0.09452580397374, 'max_depth': 10, 'min_data_in_leaf': 10}. Best is trial 4 with value: 8.781365066291881.\n","[I 2025-02-13 11:43:59,182] Trial 5 finished with value: 12.261347124451238 and parameters: {'num_leaves': 23, 'learning_rate': 0.014940011649719134, 'max_depth': 6, 'min_data_in_leaf': 21}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.19739\tvalid_1's rmse: 0.710676\n","[100]\ttraining's rmse: 0.0932389\tvalid_1's rmse: 0.429762\n","[150]\ttraining's rmse: 0.0442447\tvalid_1's rmse: 0.294247\n","[200]\ttraining's rmse: 0.0214104\tvalid_1's rmse: 0.230281\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000181 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:43:59,412] Trial 6 finished with value: 9.50836507020083 and parameters: {'num_leaves': 35, 'learning_rate': 0.022472208158916023, 'max_depth': 6, 'min_data_in_leaf': 25}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[50]\ttraining's rmse: 0.134474\tvalid_1's rmse: 0.534735\n","[100]\ttraining's rmse: 0.0434966\tvalid_1's rmse: 0.28788\n","[150]\ttraining's rmse: 0.0148776\tvalid_1's rmse: 0.21018\n","[200]\ttraining's rmse: 0.00699693\tvalid_1's rmse: 0.185717\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000202 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:43:59,627] Trial 7 finished with value: 8.976299915408646 and parameters: {'num_leaves': 34, 'learning_rate': 0.02892787457788649, 'max_depth': 6, 'min_data_in_leaf': 15}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[50]\ttraining's rmse: 0.096652\tvalid_1's rmse: 0.432252\n","[100]\ttraining's rmse: 0.022908\tvalid_1's rmse: 0.232456\n","[150]\ttraining's rmse: 0.00745016\tvalid_1's rmse: 0.187557\n","[200]\ttraining's rmse: 0.00552489\tvalid_1's rmse: 0.177275\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:43:59,842] Trial 8 finished with value: 8.936054812576947 and parameters: {'num_leaves': 35, 'learning_rate': 0.030059955572117446, 'max_depth': 7, 'min_data_in_leaf': 14}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[50]\ttraining's rmse: 0.0911915\tvalid_1's rmse: 0.416773\n","[100]\ttraining's rmse: 0.0205331\tvalid_1's rmse: 0.225952\n","[150]\ttraining's rmse: 0.00690742\tvalid_1's rmse: 0.185351\n","[200]\ttraining's rmse: 0.00547676\tvalid_1's rmse: 0.176637\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000192 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[50]\ttraining's rmse: 0.0882904\tvalid_1's rmse: 0.423154\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[100]\ttraining's rmse: 0.0193738\tvalid_1's rmse: 0.233379\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00672324\tvalid_1's rmse: 0.193452\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00551912\tvalid_1's rmse: 0.184888\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:44:00,092] Trial 9 finished with value: 9.456247396252861 and parameters: {'num_leaves': 40, 'learning_rate': 0.030778126778320317, 'max_depth': 4, 'min_data_in_leaf': 30}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.00680973\tvalid_1's rmse: 0.185436\n","[100]\ttraining's rmse: 0.00539689\tvalid_1's rmse: 0.174348\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00539512\tvalid_1's rmse: 0.174201\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00539477\tvalid_1's rmse: 0.174192\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:44:00,359] Trial 10 finished with value: 8.78183713944408 and parameters: {'num_leaves': 24, 'learning_rate': 0.08830786394418103, 'max_depth': 8, 'min_data_in_leaf': 5}. Best is trial 4 with value: 8.781365066291881.\n","[I 2025-02-13 11:44:00,594] Trial 11 finished with value: 8.782039992982915 and parameters: {'num_leaves': 23, 'learning_rate': 0.09941330980510743, 'max_depth': 8, 'min_data_in_leaf': 5}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0058571\tvalid_1's rmse: 0.180419\n","[100]\ttraining's rmse: 0.0053979\tvalid_1's rmse: 0.174258\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00539544\tvalid_1's rmse: 0.174205\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00539484\tvalid_1's rmse: 0.174196\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.00600705\tvalid_1's rmse: 0.181251\n","[100]\ttraining's rmse: 0.00539535\tvalid_1's rmse: 0.174261\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.0053948\tvalid_1's rmse: 0.174191\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00539468\tvalid_1's rmse: 0.174188\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:44:00,849] Trial 12 finished with value: 8.781569511554514 and parameters: {'num_leaves': 28, 'learning_rate': 0.09651676726393442, 'max_depth': 9, 'min_data_in_leaf': 10}. Best is trial 4 with value: 8.781365066291881.\n","[I 2025-02-13 11:44:01,066] Trial 13 finished with value: 8.78186685093259 and parameters: {'num_leaves': 29, 'learning_rate': 0.061744197640949385, 'max_depth': 9, 'min_data_in_leaf': 11}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000202 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.018162\tvalid_1's rmse: 0.220222\n","[100]\ttraining's rmse: 0.00544371\tvalid_1's rmse: 0.176121\n","[150]\ttraining's rmse: 0.00539496\tvalid_1's rmse: 0.17428\n","[200]\ttraining's rmse: 0.00539471\tvalid_1's rmse: 0.174193\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:44:01,311] Trial 14 finished with value: 8.78183931884714 and parameters: {'num_leaves': 28, 'learning_rate': 0.061653104371179185, 'max_depth': 9, 'min_data_in_leaf': 10}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0182472\tvalid_1's rmse: 0.220561\n","[100]\ttraining's rmse: 0.00544481\tvalid_1's rmse: 0.176166\n","[150]\ttraining's rmse: 0.00539501\tvalid_1's rmse: 0.174285\n","[200]\ttraining's rmse: 0.00539473\tvalid_1's rmse: 0.174192\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0126401\tvalid_1's rmse: 0.203249\n","[100]\ttraining's rmse: 0.00540392\tvalid_1's rmse: 0.174995\n","[150]\ttraining's rmse: 0.00539467\tvalid_1's rmse: 0.174209\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:44:01,636] Trial 15 finished with value: 8.781405558547345 and parameters: {'num_leaves': 43, 'learning_rate': 0.0694979712502263, 'max_depth': 9, 'min_data_in_leaf': 14}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00539465\tvalid_1's rmse: 0.174186\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0357354\tvalid_1's rmse: 0.265424\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:44:01,935] Trial 16 finished with value: 8.784998652275856 and parameters: {'num_leaves': 44, 'learning_rate': 0.04824985150947946, 'max_depth': 10, 'min_data_in_leaf': 15}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[100]\ttraining's rmse: 0.00616577\tvalid_1's rmse: 0.181727\n","[150]\ttraining's rmse: 0.00540068\tvalid_1's rmse: 0.174837\n","[200]\ttraining's rmse: 0.0053947\tvalid_1's rmse: 0.174243\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000219 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:44:02,223] Trial 17 finished with value: 8.781405457572564 and parameters: {'num_leaves': 42, 'learning_rate': 0.06991333527652448, 'max_depth': 8, 'min_data_in_leaf': 13}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[50]\ttraining's rmse: 0.0124133\tvalid_1's rmse: 0.202585\n","[100]\ttraining's rmse: 0.00540328\tvalid_1's rmse: 0.174958\n","[150]\ttraining's rmse: 0.00539468\tvalid_1's rmse: 0.174206\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00539465\tvalid_1's rmse: 0.174186\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000545 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0382904\tvalid_1's rmse: 0.271586\n","[100]\ttraining's rmse: 0.00639663\tvalid_1's rmse: 0.182791\n","[150]\ttraining's rmse: 0.00540383\tvalid_1's rmse: 0.174971\n","[200]\ttraining's rmse: 0.00539475\tvalid_1's rmse: 0.174257\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 11:44:02,495] Trial 18 finished with value: 8.785889269054644 and parameters: {'num_leaves': 50, 'learning_rate': 0.04690101596653362, 'max_depth': 8, 'min_data_in_leaf': 8}. Best is trial 4 with value: 8.781365066291881.\n","[I 2025-02-13 11:44:02,687] Trial 19 finished with value: 8.781406431394585 and parameters: {'num_leaves': 39, 'learning_rate': 0.07339324118154672, 'max_depth': 7, 'min_data_in_leaf': 17}. Best is trial 4 with value: 8.781365066291881.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0107279\tvalid_1's rmse: 0.197937\n","[100]\ttraining's rmse: 0.00539906\tvalid_1's rmse: 0.174723\n","[150]\ttraining's rmse: 0.00539471\tvalid_1's rmse: 0.174197\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00539466\tvalid_1's rmse: 0.174186\n","\n","Best LightGBM Model: {'num_leaves': 47, 'learning_rate': 0.09452580397374, 'max_depth': 10, 'min_data_in_leaf': 10}\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's l2: 3.76731e-05\tvalid_1's l2: 0.0329609\n","[100]\ttraining's l2: 2.91032e-05\tvalid_1's l2: 0.0303603\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's l2: 2.91023e-05\tvalid_1's l2: 0.0303406\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's l2: 2.91022e-05\tvalid_1's l2: 0.0303404\n","\n","Final Bi-LSTM + LightGBM Performance:\n","Train: (0.0035479622018015204, 2.9102223081492793e-05, 0.0053946476327460715, 0.9998337215321125, 1.0152392033077196)\n","Validation: (0.15646795789736623, 0.030340382123326598, 0.17418490785176136, -4.158575851578371, 8.781365066291881)\n","Test: (0.42543027643308273, 0.1870259888778715, 0.4324650146287807, -29.98986870661276, 20.986763409629244)\n"]}]},{"cell_type":"markdown","source":["### BOHB"],"metadata":{"id":"LOfnBzrhIwrv"}},{"cell_type":"code","source":["import numpy as np\n","import lightgbm as lgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Bi-LSTM Configurations (2, 3, and 5 layers)\n","bilstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store Bi-LSTM feature representations\n","bilstm_features = {}\n","\n","for num_layers in bilstm_layers:\n","    print(f\"Training Bi-LSTM with {num_layers} layers...\")\n","\n","    bilstm_model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(bilstm_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        bilstm_model.train()\n","        optimizer.zero_grad()\n","        outputs = bilstm_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Extract Feature Representations\n","    bilstm_model.eval()\n","    with torch.no_grad():\n","        train_features = bilstm_model(X_train_torch).numpy()\n","        val_features = bilstm_model(X_val_torch).numpy()\n","        test_features = bilstm_model(X_test_torch).numpy()\n","\n","    bilstm_features[num_layers] = (train_features, val_features, test_features)\n","\n","# Define ConfigSpace for BOHB (LightGBM)\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_leaves\", 20, 300, default_value=50))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 12, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"feature_fraction\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for LightGBM\n","class LightGBMWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = lgb.LGBMRegressor(\n","            num_leaves=config[\"num_leaves\"],\n","            max_depth=config[\"max_depth\"],\n","            learning_rate=config[\"learning_rate\"],\n","            feature_fraction=config[\"feature_fraction\"],\n","            random_state=42\n","        )\n","        model.fit(self.train_features, Y_train)\n","        Y_val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB for each Bi-LSTM configuration\n","best_models = {}\n","\n","for num_layers in bilstm_layers:\n","    print(f\"\\nRunning BOHB for Bi-LSTM ({num_layers} layers) + LightGBM...\")\n","\n","    train_features, val_features, test_features = bilstm_features[num_layers]\n","\n","    # Start NameServer\n","    NS = hpns.NameServer(run_id=f\"bilstm_{num_layers}_lgb_bohb\", host=\"127.0.0.1\", port=None)\n","    NS.start()\n","\n","    worker = LightGBMWorker(\n","        train_features=train_features,\n","        val_features=val_features,\n","        nameserver=\"127.0.0.1\",\n","        run_id=f\"bilstm_{num_layers}_lgb_bohb\"\n","    )\n","    worker.run(background=True)\n","\n","    bohb = BOHB(\n","        configspace=get_config_space(),\n","        run_id=f\"bilstm_{num_layers}_lgb_bohb\",\n","        nameserver=\"127.0.0.1\",\n","        min_budget=1,\n","        max_budget=3\n","    )\n","\n","    res = bohb.run(n_iterations=50)\n","\n","    # Shutdown BOHB\n","    bohb.shutdown()\n","    NS.shutdown()\n","\n","    # Retrieve Best Configuration\n","    best_config = res.get_incumbent_id()\n","    best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","    # Train Best LightGBM Model on Bi-LSTM Features\n","    best_lgb_model = lgb.LGBMRegressor(\n","        num_leaves=best_params[\"num_leaves\"],\n","        max_depth=best_params[\"max_depth\"],\n","        learning_rate=best_params[\"learning_rate\"],\n","        feature_fraction=best_params[\"feature_fraction\"],\n","        random_state=42\n","    )\n","\n","    best_lgb_model.fit(train_features, Y_train)\n","\n","    # Make Predictions\n","    Y_train_pred = best_lgb_model.predict(train_features)\n","    Y_val_pred = best_lgb_model.predict(val_features)\n","    Y_test_pred = best_lgb_model.predict(test_features)\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store best model and metrics\n","    best_models[num_layers] = {\n","        \"params\": best_params,\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics\n","    }\n","\n","    # Print Results\n","    print(f\"\\nBest Parameters for Bi-LSTM ({num_layers} layers) + LightGBM:\")\n","    print(best_params)\n","\n","    print(\"\\nTraining set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"\\nValidation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"\\nTest set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5_WjlqjHltM","executionInfo":{"status":"ok","timestamp":1739447565003,"user_tz":-330,"elapsed":281189,"user":{"displayName":"JIYA GAYAWER (RA2211031010129)","userId":"04696136268844808803"}},"outputId":"7d4c6200-7d7e-4931-af5c-3a5c592bd54e"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Bi-LSTM with 2 layers...\n","Training Bi-LSTM with 3 layers...\n","Training Bi-LSTM with 5 layers...\n","\n","Running BOHB for Bi-LSTM (2 layers) + LightGBM...\n","[LightGBM] [Warning] feature_fraction is set=0.9800558313223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9800558313223\n","[LightGBM] [Warning] feature_fraction is set=0.9800558313223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9800558313223\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9800558313223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9800558313223\n","[LightGBM] [Warning] feature_fraction is set=0.9800558313223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9800558313223\n","[LightGBM] [Warning] feature_fraction is set=0.9800558313223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9800558313223\n","\n","Best Parameters for Bi-LSTM (2 layers) + LightGBM:\n","{'feature_fraction': 0.9800558313223, 'learning_rate': 0.2845468961482, 'max_depth': 9, 'num_leaves': 229}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0054, R²: 0.9998, MAPE: 1.02%\n","\n","Validation set metrics:\n","MAE: 0.1563, MSE: 0.0303, RMSE: 0.1741, R²: -4.1521, MAPE: 8.77%\n","\n","Test set metrics:\n","MAE: 0.4253, MSE: 0.1869, RMSE: 0.4323, R²: -29.9727, MAPE: 20.98%\n","\n","Running BOHB for Bi-LSTM (3 layers) + LightGBM...\n","[LightGBM] [Warning] feature_fraction is set=0.5249448986466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5249448986466\n","[LightGBM] [Warning] feature_fraction is set=0.5249448986466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5249448986466\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000207 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.5249448986466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5249448986466\n","[LightGBM] [Warning] feature_fraction is set=0.5249448986466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5249448986466\n","[LightGBM] [Warning] feature_fraction is set=0.5249448986466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5249448986466\n","\n","Best Parameters for Bi-LSTM (3 layers) + LightGBM:\n","{'feature_fraction': 0.5249448986466, 'learning_rate': 0.2910349267581, 'max_depth': 10, 'num_leaves': 153}\n","\n","Training set metrics:\n","MAE: 0.0035, MSE: 0.0000, RMSE: 0.0053, R²: 0.9998, MAPE: 1.02%\n","\n","Validation set metrics:\n","MAE: 0.1563, MSE: 0.0303, RMSE: 0.1741, R²: -4.1521, MAPE: 8.77%\n","\n","Test set metrics:\n","MAE: 0.4253, MSE: 0.1869, RMSE: 0.4323, R²: -29.9727, MAPE: 20.98%\n","\n","Running BOHB for Bi-LSTM (5 layers) + LightGBM...\n","[LightGBM] [Warning] feature_fraction is set=0.7962495536056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7962495536056\n","[LightGBM] [Warning] feature_fraction is set=0.7962495536056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7962495536056\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000187 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.7962495536056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7962495536056\n","[LightGBM] [Warning] feature_fraction is set=0.7962495536056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7962495536056\n","[LightGBM] [Warning] feature_fraction is set=0.7962495536056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7962495536056\n","\n","Best Parameters for Bi-LSTM (5 layers) + LightGBM:\n","{'feature_fraction': 0.7962495536056, 'learning_rate': 0.2808371870756, 'max_depth': 11, 'num_leaves': 281}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0054, R²: 0.9998, MAPE: 1.04%\n","\n","Validation set metrics:\n","MAE: 0.1563, MSE: 0.0303, RMSE: 0.1741, R²: -4.1521, MAPE: 8.77%\n","\n","Test set metrics:\n","MAE: 0.4253, MSE: 0.1869, RMSE: 0.4323, R²: -29.9727, MAPE: 20.98%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_G6XCS6dIynm"},"execution_count":null,"outputs":[]}]}