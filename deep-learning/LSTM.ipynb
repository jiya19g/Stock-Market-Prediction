{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"b3zZmLiLkbT4"},"source":["##Initial Code"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Z7gOgVN8kW8r","executionInfo":{"status":"ok","timestamp":1741853899964,"user_tz":-330,"elapsed":14504,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}}},"outputs":[],"source":["# Importing necessary libraries for data analysis and manipulation\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","# For handling warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30902,"status":"ok","timestamp":1741853930861,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"oFuOd4x5kfMG","outputId":"1719c3f2-574a-42b7-dec2-dcf308bb026b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"orOZDFCjlf5a","executionInfo":{"status":"ok","timestamp":1741853931744,"user_tz":-330,"elapsed":893,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}}},"outputs":[],"source":["\n","df_aapl = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stocks/AAPL.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-r0_rYh1lwPD","executionInfo":{"status":"ok","timestamp":1741853931875,"user_tz":-330,"elapsed":128,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}}},"outputs":[],"source":["import numpy as np\n","from scipy.stats import boxcox\n","\n","df_aapl['Close_log'] = np.log(df_aapl['Close'] + 1)\n","df_aapl['Close_sqrt'] = np.sqrt(df_aapl['Close'])\n","df_aapl['Close_boxcox'], _ = boxcox(df_aapl['Close'] + 1)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1741853931886,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"kNuStzUZlyqT","outputId":"3ef9d367-d2b1-4068-a8a1-221431c32e51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Skewness: 2.5045276102319933\n","Log Transformation Skewness: 0.8535555176510303\n","Square Root Transformation Skewness: 1.6211545809555206\n","Box-Cox Transformation Skewness: 0.43527466713563334\n"]}],"source":["\n","skew_original = df_aapl['Close'].skew()\n","skew_log = df_aapl['Close_log'].skew()\n","skew_sqrt = df_aapl['Close_sqrt'].skew()\n","skew_boxcox = pd.Series(df_aapl['Close_boxcox']).skew()\n","\n","print(f\"Original Skewness: {skew_original}\")\n","print(f\"Log Transformation Skewness: {skew_log}\")\n","print(f\"Square Root Transformation Skewness: {skew_sqrt}\")\n","print(f\"Box-Cox Transformation Skewness: {skew_boxcox}\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pNrMNRaKnRvc","executionInfo":{"status":"ok","timestamp":1741853932127,"user_tz":-330,"elapsed":242,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}}},"outputs":[],"source":["\n","df_aapl['Open_log'] = np.log(df_aapl['Open'])\n","df_aapl['High_log'] = np.log(df_aapl['High'])\n","df_aapl['Low_log'] = np.log(df_aapl['Low'])\n","df_aapl['Adj Close_log'] = np.log(df_aapl['Adj Close'])\n","df_aapl['Volume_log'] = np.log(df_aapl['Volume'])\n","\n","\n","df_aapl['Open_sqrt'] = np.sqrt(df_aapl['Open'])\n","df_aapl['High_sqrt'] = np.sqrt(df_aapl['High'])\n","df_aapl['Low_sqrt'] = np.sqrt(df_aapl['Low'])\n","df_aapl['Adj Close_sqrt'] = np.sqrt(df_aapl['Adj Close'])\n","df_aapl['Volume_sqrt'] = np.sqrt(df_aapl['Volume'])\n","\n","from scipy.stats import boxcox\n","df_aapl['Open_boxcox'], _ = boxcox(df_aapl['Open'])\n","df_aapl['High_boxcox'], _ = boxcox(df_aapl['High'])\n","df_aapl['Low_boxcox'], _ = boxcox(df_aapl['Low'])\n","df_aapl['Adj Close_boxcox'], _ = boxcox(df_aapl['Adj Close'])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59,"status":"ok","timestamp":1741853932201,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"Ky1SQb1-nUAl","outputId":"f6db97fa-02fe-4df2-c9c3-409a9837f924"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness Before Transformation:\n"," Open         2.504632\n","High         2.502208\n","Low          2.506714\n","Adj Close    2.550677\n","Volume       3.565699\n","dtype: float64\n","\n","Skewness After Transformation:\n"," Open_log            0.482872\n","High_log            0.481997\n","Low_log             0.484246\n","Adj Close_log       0.494009\n","Open_sqrt           1.620771\n","High_sqrt           1.621456\n","Low_sqrt            1.620661\n","Adj Close_sqrt      1.679402\n","Volume_sqrt         1.299776\n","Open_boxcox         0.181226\n","High_boxcox         0.179749\n","Low_boxcox          0.182882\n","Adj Close_boxcox    0.180085\n","dtype: float64\n"]}],"source":["\n","skewness_before = df_aapl[['Open', 'High', 'Low', 'Adj Close', 'Volume']].skew()\n","skewness_after = df_aapl[['Open_log', 'High_log', 'Low_log', 'Adj Close_log',\n","                          'Open_sqrt', 'High_sqrt', 'Low_sqrt', 'Adj Close_sqrt', 'Volume_sqrt',\n","                          'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox']].skew()\n","\n","print(\"Skewness Before Transformation:\\n\", skewness_before)\n","print(\"\\nSkewness After Transformation:\\n\", skewness_after)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":578,"status":"ok","timestamp":1741853932806,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"48zmxKq7nV39","outputId":"fcd27675-8a6a-460b-b255-fca303e00d5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness After Box-Cox Transformation:\n","Open_boxcox         0.435237\n","High_boxcox         0.433381\n","Low_boxcox          0.437331\n","Adj Close_boxcox    0.458762\n","Close_boxcox        0.435275\n","dtype: float64\n"]}],"source":["from scipy import stats\n","\n","df_aapl['Open_boxcox'], _ = stats.boxcox(df_aapl['Open'] + 1)\n","df_aapl['High_boxcox'], _ = stats.boxcox(df_aapl['High'] + 1)\n","df_aapl['Low_boxcox'], _ = stats.boxcox(df_aapl['Low'] + 1)\n","df_aapl['Adj Close_boxcox'], _ = stats.boxcox(df_aapl['Adj Close'] + 1)\n","df_aapl['Close_boxcox'], _ = stats.boxcox(df_aapl['Close'] + 1)\n","\n","skewness_after_boxcox = df_aapl[['Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox', 'Close_boxcox']].skew()\n","\n","print(\"Skewness After Box-Cox Transformation:\")\n","print(skewness_after_boxcox)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65,"status":"ok","timestamp":1741853932839,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"giiXoL8snYA1","outputId":"9a109556-9b64-498f-e953-ff4444742e92"},"outputs":[{"output_type":"stream","name":"stdout","text":["         Date      Open      High       Low  Adj Close     Close     Volume  \\\n","0  1980-12-12  0.128348  0.128906  0.128348   0.098943  0.128348  469033600   \n","1  1980-12-15  0.122210  0.122210  0.121652   0.093781  0.121652  175884800   \n","2  1980-12-16  0.113281  0.113281  0.112723   0.086898  0.112723  105728000   \n","3  1980-12-17  0.115513  0.116071  0.115513   0.089049  0.115513   86441600   \n","4  1980-12-18  0.118862  0.119420  0.118862   0.091630  0.118862   73449600   \n","\n","   Open_boxcox  High_boxcox  Low_boxcox  Adj Close_boxcox  Close_boxcox  \n","0     0.117689     0.118173    0.117674          0.092374      0.117689  \n","1     0.112503     0.112516    0.112016          0.087857      0.112030  \n","2     0.104886     0.104897    0.104395          0.081785      0.104407  \n","3     0.106798     0.107287    0.106786          0.083688      0.106798  \n","4     0.109657     0.110145    0.109644          0.085966      0.109657  \n"]}],"source":["\n","df_aapl_cleaned = df_aapl[['Date', 'Open', 'High', 'Low', 'Adj Close', 'Close', 'Volume',\n","                           'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox',\n","                           'Close_boxcox']]\n","\n","print(df_aapl_cleaned.head())\n"]},{"cell_type":"markdown","metadata":{"id":"kMvxyuvvnbxW"},"source":["##Train Validation Test"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1741853941996,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"xJpYQOmAnZ2u","outputId":"1647d72d-6c5c-46ef-fa4c-05c68715aa36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training set: (7736, 3), Validation set: (1658, 3), Test set: (1658, 3)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","X = df_aapl_cleaned[['Open_boxcox', 'High_boxcox', 'Low_boxcox']]\n","Y = df_aapl_cleaned['Close_boxcox']\n","\n","X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, shuffle=False)\n","X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, shuffle=False)\n","\n","print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n"]},{"cell_type":"markdown","source":["#LSTM"],"metadata":{"id":"X6-_9u4A4vs9"}},{"cell_type":"markdown","source":["## Initial"],"metadata":{"id":"w_ILvj2-4x15"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)  # LSTM output directly connected to FC layer\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])  # Take the last time step output\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# LSTM Configurations (2, 3, and 5 layers)\n","lstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store results\n","lstm_results = {}\n","\n","for num_layers in lstm_layers:\n","    print(f\"\\nTraining LSTM with {num_layers} layers...\")\n","    model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_train = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    end_train = time.time()\n","    train_time = end_train - start_train\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        start_val = time.time()\n","        Y_train_pred = model(X_train_torch).numpy()\n","        end_val = time.time()\n","        train_eval_time = end_val - start_val\n","\n","        start_val = time.time()\n","        Y_val_pred = model(X_val_torch).numpy()\n","        end_val = time.time()\n","        val_time = end_val - start_val\n","\n","        start_test = time.time()\n","        Y_test_pred = model(X_test_torch).numpy()\n","        end_test = time.time()\n","        test_time = end_test - start_test\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store results\n","    lstm_results[num_layers] = {\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics,\n","        \"train_time\": train_time,\n","        \"val_time\": val_time,\n","        \"test_time\": test_time\n","    }\n","\n","    # Print Results\n","    print(f\"\\nLSTM ({num_layers} layers) Metrics:\")\n","    print(f\"Training Time: {train_time:.4f} sec, Validation Time: {val_time:.4f} sec, Testing Time: {test_time:.4f} sec\")\n","\n","    print(\"Training set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"Validation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"Test set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8joGfysM5cLx","executionInfo":{"status":"ok","timestamp":1741854250348,"user_tz":-330,"elapsed":130284,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"4ced850a-8021-478a-f36e-5b719c8c129a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training LSTM with 2 layers...\n","\n","LSTM (2 layers) Metrics:\n","Training Time: 21.6387 sec, Validation Time: 0.0173 sec, Testing Time: 0.0142 sec\n","Training set metrics:\n","MAE: 0.0334, MSE: 0.0018, RMSE: 0.0419, R²: 0.9900, MAPE: 19.14%\n","Validation set metrics:\n","MAE: 0.0726, MSE: 0.0053, RMSE: 0.0731, R²: 0.0919, MAPE: 4.14%\n","Test set metrics:\n","MAE: 0.0866, MSE: 0.0075, RMSE: 0.0866, R²: -0.2437, MAPE: 4.30%\n","\n","Training LSTM with 3 layers...\n","\n","LSTM (3 layers) Metrics:\n","Training Time: 31.3031 sec, Validation Time: 0.0184 sec, Testing Time: 0.0190 sec\n","Training set metrics:\n","MAE: 0.0471, MSE: 0.0030, RMSE: 0.0547, R²: 0.9829, MAPE: 19.93%\n","Validation set metrics:\n","MAE: 0.1160, MSE: 0.0135, RMSE: 0.1161, R²: -1.2908, MAPE: 6.65%\n","Test set metrics:\n","MAE: 0.0980, MSE: 0.0097, RMSE: 0.0987, R²: -0.6153, MAPE: 4.89%\n","\n","Training LSTM with 5 layers...\n","\n","LSTM (5 layers) Metrics:\n","Training Time: 58.5818 sec, Validation Time: 0.0529 sec, Testing Time: 0.0424 sec\n","Training set metrics:\n","MAE: 0.0490, MSE: 0.0033, RMSE: 0.0571, R²: 0.9813, MAPE: 26.31%\n","Validation set metrics:\n","MAE: 0.1960, MSE: 0.0413, RMSE: 0.2033, R²: -6.0250, MAPE: 11.11%\n","Test set metrics:\n","MAE: 0.4035, MSE: 0.1668, RMSE: 0.4085, R²: -26.6442, MAPE: 19.93%\n"]}]},{"cell_type":"markdown","source":["##Optuna"],"metadata":{"id":"NMCzLvow57cA"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXt13y196iNZ","executionInfo":{"status":"ok","timestamp":1741758811470,"user_tz":-330,"elapsed":4832,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"f6db1fd6-fb27-485f-e468-bde9e35e2f02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import optuna\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# LSTM Configurations (2, 3, and 5 layers)\n","lstm_layers = [2, 3, 5]\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Optuna Optimization Function\n","def objective(trial, num_layers):\n","    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n","    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n","\n","    model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    num_epochs = 50\n","    patience = 5\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).numpy()\n","            val_loss = mean_squared_error(Y_val, Y_val_pred)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= patience:\n","            break\n","\n","    return best_val_loss\n","\n","# Dictionary to store results\n","lstm_results = {}\n","\n","for num_layers in lstm_layers:\n","    print(f\"\\nOptimizing LSTM with {num_layers} layers using Optuna...\")\n","\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective(trial, num_layers), n_trials=10)\n","\n","    best_params = study.best_params\n","    hidden_dim = best_params[\"hidden_dim\"]\n","    lr = best_params[\"lr\"]\n","\n","    print(f\"Best Params for {num_layers} layers - Hidden Dim: {hidden_dim}, LR: {lr}\")\n","\n","    model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    num_epochs = 100\n","    patience = 10\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","\n","    start_train = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).numpy()\n","            val_loss = mean_squared_error(Y_val, Y_val_pred)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= patience:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    end_train = time.time()\n","    train_time = end_train - start_train\n","\n","    model.eval()\n","    with torch.no_grad():\n","        start_val = time.time()\n","        Y_train_pred = model(X_train_torch).numpy()\n","        end_val = time.time()\n","        train_eval_time = end_val - start_val\n","\n","        start_val = time.time()\n","        Y_val_pred = model(X_val_torch).numpy()\n","        end_val = time.time()\n","        val_time = end_val - start_val\n","\n","        start_test = time.time()\n","        Y_test_pred = model(X_test_torch).numpy()\n","        end_test = time.time()\n","        test_time = end_test - start_test\n","\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    lstm_results[num_layers] = {\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics,\n","        \"train_time\": train_time,\n","        \"val_time\": val_time,\n","        \"test_time\": test_time\n","    }\n","\n","    print(f\"\\nLSTM ({num_layers} layers) Metrics:\")\n","    print(f\"Training Time: {train_time:.4f} sec, Validation Time: {val_time:.4f} sec, Testing Time: {test_time:.4f} sec\")\n","\n","    print(\"Training set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"Validation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"Test set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LOP6rzmK5esI","executionInfo":{"status":"ok","timestamp":1741759404269,"user_tz":-330,"elapsed":587355,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"fb5fb3be-55ea-469a-e09e-f9b231143a36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 05:53:44,978] A new study created in memory with name: no-name-2430c4ad-7db9-49d5-b458-23bafb1194d3\n"]},{"output_type":"stream","name":"stdout","text":["\n","Optimizing LSTM with 2 layers using Optuna...\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 05:54:02,400] Trial 0 finished with value: 0.04651753221181586 and parameters: {'hidden_dim': 122, 'lr': 0.003077529655765805}. Best is trial 0 with value: 0.04651753221181586.\n","[I 2025-03-12 05:54:21,958] Trial 1 finished with value: 0.037621788033646755 and parameters: {'hidden_dim': 118, 'lr': 0.0017894994745394272}. Best is trial 1 with value: 0.037621788033646755.\n","[I 2025-03-12 05:54:41,753] Trial 2 finished with value: 0.05117221209141317 and parameters: {'hidden_dim': 124, 'lr': 0.0011337036357827077}. Best is trial 1 with value: 0.037621788033646755.\n","[I 2025-03-12 05:54:53,618] Trial 3 finished with value: 0.06570410271790128 and parameters: {'hidden_dim': 114, 'lr': 0.0022621777804527816}. Best is trial 1 with value: 0.037621788033646755.\n","[I 2025-03-12 05:55:01,417] Trial 4 finished with value: 3.1210358708843216 and parameters: {'hidden_dim': 49, 'lr': 0.0002781186916902079}. Best is trial 1 with value: 0.037621788033646755.\n","[I 2025-03-12 05:55:06,152] Trial 5 finished with value: 3.4368410291487894 and parameters: {'hidden_dim': 35, 'lr': 0.0001638276212099623}. Best is trial 1 with value: 0.037621788033646755.\n","[I 2025-03-12 05:55:18,175] Trial 6 finished with value: 2.674966649611432 and parameters: {'hidden_dim': 55, 'lr': 0.00022827864975358289}. Best is trial 1 with value: 0.037621788033646755.\n","[I 2025-03-12 05:55:21,048] Trial 7 finished with value: 0.5533396557835548 and parameters: {'hidden_dim': 32, 'lr': 0.0013296050233830043}. Best is trial 1 with value: 0.037621788033646755.\n","[I 2025-03-12 05:55:33,859] Trial 8 finished with value: 1.4030525137537844 and parameters: {'hidden_dim': 75, 'lr': 0.000569307600768351}. Best is trial 1 with value: 0.037621788033646755.\n","[I 2025-03-12 05:55:54,144] Trial 9 finished with value: 2.5890852184157405 and parameters: {'hidden_dim': 103, 'lr': 0.00015738039181947797}. Best is trial 1 with value: 0.037621788033646755.\n"]},{"output_type":"stream","name":"stdout","text":["Best Params for 2 layers - Hidden Dim: 118, LR: 0.0017894994745394272\n","Early stopping at epoch 36\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 05:56:12,311] A new study created in memory with name: no-name-46c3af5d-1e7f-43ae-98dd-8e3d8c38d79b\n"]},{"output_type":"stream","name":"stdout","text":["\n","LSTM (2 layers) Metrics:\n","Training Time: 17.9259 sec, Validation Time: 0.0294 sec, Testing Time: 0.0308 sec\n","Training set metrics:\n","MAE: 0.1350, MSE: 0.0256, RMSE: 0.1600, R²: 0.8537, MAPE: 58.04%\n","Validation set metrics:\n","MAE: 0.4124, MSE: 0.1705, RMSE: 0.4129, R²: -27.9823, MAPE: 23.61%\n","Test set metrics:\n","MAE: 0.4827, MSE: 0.2335, RMSE: 0.4832, R²: -37.6857, MAPE: 23.95%\n","\n","Optimizing LSTM with 3 layers using Optuna...\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 05:56:25,297] Trial 0 finished with value: 2.711929630328987 and parameters: {'hidden_dim': 47, 'lr': 0.00045914268703724866}. Best is trial 0 with value: 2.711929630328987.\n","[I 2025-03-12 05:56:31,233] Trial 1 finished with value: 0.3832047882099273 and parameters: {'hidden_dim': 49, 'lr': 0.0027771895746656616}. Best is trial 1 with value: 0.3832047882099273.\n","[I 2025-03-12 05:56:44,218] Trial 2 finished with value: 0.28953137169245086 and parameters: {'hidden_dim': 83, 'lr': 0.00206639658042873}. Best is trial 2 with value: 0.28953137169245086.\n","[I 2025-03-12 05:57:18,496] Trial 3 finished with value: 2.4604542816649806 and parameters: {'hidden_dim': 110, 'lr': 0.00019417014039787003}. Best is trial 2 with value: 0.28953137169245086.\n","[I 2025-03-12 05:57:27,167] Trial 4 finished with value: 2.8758925260374126 and parameters: {'hidden_dim': 42, 'lr': 0.00034177012464908525}. Best is trial 2 with value: 0.28953137169245086.\n","[I 2025-03-12 05:57:54,040] Trial 5 finished with value: 0.0007211198835275421 and parameters: {'hidden_dim': 99, 'lr': 0.0013394787225778622}. Best is trial 5 with value: 0.0007211198835275421.\n","[I 2025-03-12 05:58:02,138] Trial 6 finished with value: 0.760939619609661 and parameters: {'hidden_dim': 40, 'lr': 0.0011259194565629807}. Best is trial 5 with value: 0.0007211198835275421.\n","[I 2025-03-12 05:58:08,081] Trial 7 finished with value: 0.0017413900949631766 and parameters: {'hidden_dim': 64, 'lr': 0.007582871312094513}. Best is trial 5 with value: 0.0007211198835275421.\n","[I 2025-03-12 05:58:18,503] Trial 8 finished with value: 0.4614103423343714 and parameters: {'hidden_dim': 46, 'lr': 0.0016813460220028303}. Best is trial 5 with value: 0.0007211198835275421.\n","[I 2025-03-12 05:58:37,929] Trial 9 finished with value: 2.3527103268929843 and parameters: {'hidden_dim': 71, 'lr': 0.00036070026154343365}. Best is trial 5 with value: 0.0007211198835275421.\n"]},{"output_type":"stream","name":"stdout","text":["Best Params for 3 layers - Hidden Dim: 99, LR: 0.0013394787225778622\n","Early stopping at epoch 39\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 05:59:00,483] A new study created in memory with name: no-name-eddca303-4914-405f-a5cd-ca55259014fc\n"]},{"output_type":"stream","name":"stdout","text":["\n","LSTM (3 layers) Metrics:\n","Training Time: 22.2688 sec, Validation Time: 0.0336 sec, Testing Time: 0.0330 sec\n","Training set metrics:\n","MAE: 0.1863, MSE: 0.0451, RMSE: 0.2125, R²: 0.7421, MAPE: 83.03%\n","Validation set metrics:\n","MAE: 0.5273, MSE: 0.2788, RMSE: 0.5280, R²: -46.4013, MAPE: 30.18%\n","Test set metrics:\n","MAE: 0.6257, MSE: 0.3923, RMSE: 0.6263, R²: -64.0004, MAPE: 31.03%\n","\n","Optimizing LSTM with 5 layers using Optuna...\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 05:59:22,532] Trial 0 finished with value: 0.8711299455473696 and parameters: {'hidden_dim': 68, 'lr': 0.0012053178706390789}. Best is trial 0 with value: 0.8711299455473696.\n","[I 2025-03-12 06:00:15,466] Trial 1 finished with value: 2.800176766629129 and parameters: {'hidden_dim': 103, 'lr': 0.0001227770177631478}. Best is trial 0 with value: 0.8711299455473696.\n","[I 2025-03-12 06:00:24,443] Trial 2 finished with value: 0.7451103741238901 and parameters: {'hidden_dim': 92, 'lr': 0.008281463580527138}. Best is trial 2 with value: 0.7451103741238901.\n","[I 2025-03-12 06:01:23,919] Trial 3 finished with value: 2.4446187821917045 and parameters: {'hidden_dim': 120, 'lr': 0.00019087451666886644}. Best is trial 2 with value: 0.7451103741238901.\n","[I 2025-03-12 06:01:31,337] Trial 4 finished with value: 0.8489208349149941 and parameters: {'hidden_dim': 48, 'lr': 0.004481448413800887}. Best is trial 2 with value: 0.7451103741238901.\n","[I 2025-03-12 06:01:47,832] Trial 5 finished with value: 2.8095219420509148 and parameters: {'hidden_dim': 43, 'lr': 0.00030864869884651426}. Best is trial 2 with value: 0.7451103741238901.\n","[I 2025-03-12 06:01:56,336] Trial 6 finished with value: 0.944656810142513 and parameters: {'hidden_dim': 80, 'lr': 0.0051809200442166055}. Best is trial 2 with value: 0.7451103741238901.\n","[I 2025-03-12 06:02:20,416] Trial 7 finished with value: 3.043670498866435 and parameters: {'hidden_dim': 54, 'lr': 0.00011051608106823979}. Best is trial 2 with value: 0.7451103741238901.\n","[I 2025-03-12 06:02:48,632] Trial 8 finished with value: 0.8537346795537215 and parameters: {'hidden_dim': 86, 'lr': 0.0011657438373360465}. Best is trial 2 with value: 0.7451103741238901.\n","[I 2025-03-12 06:03:12,249] Trial 9 finished with value: 2.5984990295957764 and parameters: {'hidden_dim': 53, 'lr': 0.0002005694756826581}. Best is trial 2 with value: 0.7451103741238901.\n"]},{"output_type":"stream","name":"stdout","text":["Best Params for 5 layers - Hidden Dim: 92, LR: 0.008281463580527138\n","Early stopping at epoch 23\n","\n","LSTM (5 layers) Metrics:\n","Training Time: 19.2698 sec, Validation Time: 0.0539 sec, Testing Time: 0.0534 sec\n","Training set metrics:\n","MAE: 0.1093, MSE: 0.0165, RMSE: 0.1283, R²: 0.9060, MAPE: 35.17%\n","Validation set metrics:\n","MAE: 0.3217, MSE: 0.1089, RMSE: 0.3299, R²: -17.5096, MAPE: 18.27%\n","Test set metrics:\n","MAE: 0.5825, MSE: 0.3450, RMSE: 0.5874, R²: -56.1737, MAPE: 28.79%\n"]}]},{"cell_type":"markdown","source":["##BOHB"],"metadata":{"id":"Vnjt9cSV-qXS"}},{"cell_type":"code","source":["!pip install ConfigSpace hpbandster"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBgfVyeY_D27","executionInfo":{"status":"ok","timestamp":1741760242542,"user_tz":-330,"elapsed":17699,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"5e16dd39-ece1-45c3-927e-ca289ad631b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.26.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.14.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (10.6.0)\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: ConfigSpace, hpbandster, netifaces\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=configspace-1.2.1-py3-none-any.whl size=115950 sha256=b7ed9545cf9854ffcb54955b46cb4e03b8296f7f785a1c5fef2a404d921b5d07\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=487792d838ac212d9f89357f4c04e97b8747e49a6c74c5161dcd4338335345f7\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35189 sha256=9b042ccee708b0933fe707fb76e835b39948921628664767cbc5f8d538545350\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built ConfigSpace hpbandster netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, ConfigSpace, hpbandster\n","Successfully installed ConfigSpace-1.2.1 Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# LSTM Configuration\n","num_layers = 2\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training LSTM with {num_layers} layers...\")\n","\n","lstm_model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","# Train LSTM\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","lstm_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = lstm_model(X_train_torch).cpu().numpy()\n","    val_features = lstm_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = lstm_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for LSTM\n","class LSTMWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = LSTMModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"lstm_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = LSTMWorker(nameserver=\"127.0.0.2\", run_id=\"lstm_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"lstm_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best LSTM Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_lstm_model = LSTMModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_lstm_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_lstm_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_lstm_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_lstm_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_lstm_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-qLr7RhM-uOm","executionInfo":{"status":"ok","timestamp":1741760456079,"user_tz":-330,"elapsed":211323,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"692939a5-1823-40f2-bcc3-e51c3a822e08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training LSTM with 2 layers...\n","Train Metrics: (0.010655230130874207, 0.00016445769845203624, 0.0128241061463182, 0.9990603544597149, 5.853473452873122) Time: 1.545008659362793\n","Validation Metrics: (0.0056614460681615195, 4.700666093324246e-05, 0.006856140381675572, 0.9920077662513312, 0.3324592692614566) Time: 0.3037400245666504\n","Test Metrics: (0.022358513718004992, 0.0005990847309494662, 0.024476207446201017, 0.9007327416491633, 1.0915918018252004) Time: 0.0021195411682128906\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# LSTM Configuration\n","num_layers = 3\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training LSTM with {num_layers} layers...\")\n","\n","lstm_model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","# Train LSTM\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","lstm_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = lstm_model(X_train_torch).cpu().numpy()\n","    val_features = lstm_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = lstm_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for LSTM\n","class LSTMWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = LSTMModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"lstm_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = LSTMWorker(nameserver=\"127.0.0.2\", run_id=\"lstm_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"lstm_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best LSTM Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_lstm_model = LSTMModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_lstm_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_lstm_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_lstm_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_lstm_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_lstm_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAz5DZO76glB","executionInfo":{"status":"ok","timestamp":1741760627391,"user_tz":-330,"elapsed":163550,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"516e55ba-198e-4dc1-8e14-0ac1e1142b3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training LSTM with 3 layers...\n","Train Metrics: (0.03998856216987476, 0.002213173078862887, 0.047044373509091256, 0.9873548138335454, 15.303968598041118) Time: 1.267256259918213\n","Validation Metrics: (0.1547264064233051, 0.024172983542141546, 0.15547663342811854, -3.109973587485605, 8.838209398599703) Time: 0.43764328956604004\n","Test Metrics: (0.18964332011431392, 0.03600476257207245, 0.18974920967443434, -4.965924156401063, 9.411479859545574) Time: 0.003278017044067383\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# LSTM Configuration\n","num_layers = 5\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training LSTM with {num_layers} layers...\")\n","\n","lstm_model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","# Train LSTM\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","lstm_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = lstm_model(X_train_torch).cpu().numpy()\n","    val_features = lstm_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = lstm_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for LSTM\n","class LSTMWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = LSTMModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"lstm_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = LSTMWorker(nameserver=\"127.0.0.2\", run_id=\"lstm_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"lstm_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best LSTM Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_lstm_model = LSTMModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_lstm_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_lstm_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_lstm_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_lstm_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_lstm_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W3groxZvAdhc","executionInfo":{"status":"ok","timestamp":1741760748876,"user_tz":-330,"elapsed":116810,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"42ab3236-4682-4540-e3a8-a7ff68e57f10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training LSTM with 5 layers...\n","Train Metrics: (0.010890893246101709, 0.0001801770319481916, 0.01342300383476782, 0.9989705404725623, 6.340996167077166) Time: 2.062328815460205\n","Validation Metrics: (0.004326330782998235, 2.7755266808108614e-05, 0.005268326756011686, 0.9952809543225778, 0.24744428256284226) Time: 0.7281322479248047\n","Test Metrics: (0.02698135570084039, 0.0008263490171925536, 0.028746287015761766, 0.8630754597140073, 1.3213878153713008) Time: 0.009002208709716797\n"]}]},{"cell_type":"markdown","source":["#Stacked Model"],"metadata":{"id":"yt9fDxtaB3T2"}},{"cell_type":"markdown","source":["## XGBoost"],"metadata":{"id":"FATLR-0kB6O1"}},{"cell_type":"markdown","source":["### Initial"],"metadata":{"id":"4kANZwgUB8cX"}},{"cell_type":"code","source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Enable GPU for TensorFlow\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train an LSTM model on GPU\n","def train_lstm(X_train, Y_train, X_val, Y_val, layers):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(LSTM(64, return_sequences=(layers > 1), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(LSTM(64, return_sequences=(_ < layers - 2)))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","        return model\n","\n","# Reshaping input for LSTM\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train 2, 3, and 5-layer LSTM models\n","lstm_models = {}\n","lstm_predictions = {}\n","\n","start_train_time = time.time()\n","for layers in [2, 3, 5]:\n","    model = train_lstm(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    Y_train_pred = model.predict(X_train_r)\n","    Y_val_pred = model.predict(X_val_r)\n","    Y_test_pred = model.predict(X_test_r)\n","\n","    lstm_models[layers] = model\n","    lstm_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","train_time_total = time.time() - start_train_time\n","\n","# Prepare input for XGBoost\n","X_train_xgb = np.column_stack([lstm_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_xgb = np.column_stack([lstm_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_xgb = np.column_stack([lstm_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train XGBoost model\n","xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=3, objective='reg:squarederror', tree_method='gpu_hist')\n","start_train_xgb = time.time()\n","xgb_model.fit(X_train_xgb, Y_train, eval_set=[(X_val_xgb, Y_val)], verbose=False)\n","train_time_total += time.time() - start_train_xgb\n","\n","# Predictions from XGBoost\n","start_validate_time = time.time()\n","Y_train_pred_xgb = xgb_model.predict(X_train_xgb)\n","Y_val_pred_xgb = xgb_model.predict(X_val_xgb)\n","validate_time_total = time.time() - start_validate_time\n","\n","start_test_time = time.time()\n","Y_test_pred_xgb = xgb_model.predict(X_test_xgb)\n","test_time_total = time.time() - start_test_time\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_xgb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_xgb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_xgb)\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print overall training, validation, and test times\n","print(\"Execution Times:\")\n","print(f\"Total Training Time: {train_time_total:.2f} seconds\")\n","print(f\"Total Validation Time: {validate_time_total:.2f} seconds\")\n","print(f\"Total Testing Time: {test_time_total:.2f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RaUcqM-ABhPU","executionInfo":{"status":"ok","timestamp":1741761442693,"user_tz":-330,"elapsed":301211,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"23a18480-cb6a-4357-f4a5-e3161d6ad6ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU activated for TensorFlow!\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","Train Metrics: MAE=0.0046, MSE=0.0000, RMSE=0.0065, R²=0.9998, MAPE=1.60%\n","Validation Metrics: MAE=0.1719, MSE=0.0354, RMSE=0.1882, R²=-5.0249, MAPE=9.67%\n","Test Metrics: MAE=0.4409, MSE=0.2005, RMSE=0.4477, R²=-32.2178, MAPE=21.76%\n","Execution Times:\n","Total Training Time: 296.99 seconds\n","Total Validation Time: 0.01 seconds\n","Total Testing Time: 0.00 seconds\n"]}]},{"cell_type":"markdown","source":["###Optuna"],"metadata":{"id":"nzDNFuE0FL-B"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ckJGeapIFJvc","executionInfo":{"status":"ok","timestamp":1741761594901,"user_tz":-330,"elapsed":3635,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"9d6969ca-86ed-4607-ac27-98209f8c8c4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import optuna\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Enable GPU for TensorFlow\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train an LSTM model on GPU\n","def train_lstm(X_train, Y_train, X_val, Y_val, units, layers, batch_size):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(LSTM(units, return_sequences=(layers > 1), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(LSTM(units, return_sequences=(_ < layers - 2)))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=batch_size, verbose=0)\n","        return model\n","\n","# Function to compute metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Start tracking total time\n","total_start_time = time.time()\n","\n","# Optuna objective function\n","def objective(trial):\n","    # LSTM hyperparameters\n","    units = trial.suggest_int(\"units\", 32, 128, step=16)\n","    layers = trial.suggest_int(\"layers\", 2, 5)\n","    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n","\n","    # XGBoost hyperparameters\n","    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True)\n","    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n","    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200, step=50)\n","\n","    # Reshaping input for LSTM\n","    X_train_r = np.expand_dims(X_train, axis=-1)\n","    X_val_r = np.expand_dims(X_val, axis=-1)\n","    X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","    # Start training timer\n","    train_start_time = time.time()\n","\n","    # Train LSTM models\n","    lstm_models = {}\n","    lstm_predictions = {}\n","    for layer in [2, 3, 5]:\n","        lstm_model = train_lstm(X_train_r, Y_train, X_val_r, Y_val, units, layer, batch_size)\n","        Y_train_pred = lstm_model.predict(X_train_r)\n","        Y_val_pred = lstm_model.predict(X_val_r)\n","        Y_test_pred = lstm_model.predict(X_test_r)\n","\n","        lstm_models[layer] = lstm_model\n","        lstm_predictions[layer] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","\n","    # Prepare input for XGBoost\n","    X_train_xgb = np.column_stack([lstm_predictions[layers][0] for layers in [2, 3, 5]])\n","    X_val_xgb = np.column_stack([lstm_predictions[layers][1] for layers in [2, 3, 5]])\n","\n","    # Train XGBoost model\n","    xgb_model = XGBRegressor(\n","        n_estimators=n_estimators,\n","        learning_rate=learning_rate,\n","        max_depth=max_depth,\n","        objective='reg:squarederror',\n","        tree_method='gpu_hist'\n","    )\n","    xgb_model.fit(X_train_xgb, Y_train, eval_set=[(X_val_xgb, Y_val)], verbose=False)\n","\n","    train_end_time = time.time()\n","    train_time = train_end_time - train_start_time\n","\n","    # Get validation score (optimize for RMSE)\n","    Y_val_pred_xgb = xgb_model.predict(X_val_xgb)\n","    _, _, rmse, _, _ = compute_metrics(Y_val, Y_val_pred_xgb)\n","\n","    return rmse\n","\n","# Run Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=20)\n","\n","# Best hyperparameters\n","best_params = study.best_params\n","print(\"Best Parameters:\", best_params)\n","\n","# Start training timer\n","train_start_time = time.time()\n","\n","# Train final models with best parameters\n","final_lstm_models = {}\n","final_lstm_predictions = {}\n","for layer in [2, 3, 5]:\n","    model = train_lstm(X_train_r, Y_train, X_val_r, Y_val, best_params['units'], layer, best_params['batch_size'])\n","    final_lstm_models[layer] = model\n","    final_lstm_predictions[layer] = (\n","        model.predict(X_train_r),\n","        model.predict(X_val_r),\n","        model.predict(X_test_r)\n","    )\n","\n","# Prepare final inputs for XGBoost\n","X_train_xgb = np.column_stack([final_lstm_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_xgb = np.column_stack([final_lstm_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_xgb = np.column_stack([final_lstm_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train final XGBoost model\n","final_xgb_model = XGBRegressor(\n","    n_estimators=best_params['n_estimators'],\n","    learning_rate=best_params['learning_rate'],\n","    max_depth=best_params['max_depth'],\n","    objective='reg:squarederror',\n","    tree_method='gpu_hist'\n",")\n","final_xgb_model.fit(X_train_xgb, Y_train, eval_set=[(X_val_xgb, Y_val)], verbose=False)\n","\n","train_end_time = time.time()\n","train_time = train_end_time - train_start_time\n","\n","# Start validation timer\n","val_start_time = time.time()\n","\n","# Validation predictions\n","Y_val_pred_xgb = final_xgb_model.predict(X_val_xgb)\n","\n","val_end_time = time.time()\n","val_time = val_end_time - val_start_time\n","\n","# Start test timer\n","test_start_time = time.time()\n","\n","# Test predictions\n","Y_test_pred_xgb = final_xgb_model.predict(X_test_xgb)\n","\n","test_end_time = time.time()\n","test_time = test_end_time - test_start_time\n","\n","# Compute final metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_xgb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_xgb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_xgb)\n","\n","# Total execution time\n","total_end_time = time.time()\n","total_time = total_end_time - total_start_time\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","print(f\"\\nOverall Timing:\")\n","print(f\"Total Training Time: {train_time:.2f} seconds\")\n","print(f\"Total Validation Time: {val_time:.2f} seconds\")\n","print(f\"Total Testing Time: {test_time:.2f} seconds\")\n","print(f\"Total Execution Time: {total_time:.2f} seconds\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzhZ3H_XDdnX","executionInfo":{"status":"ok","timestamp":1741765141318,"user_tz":-330,"elapsed":3056687,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"2b0b1fcf-1de8-4b6a-bfcd-e3d735876a20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 06:48:12,725] A new study created in memory with name: no-name-637810bd-b5a3-4e6a-9762-c349365912ac\n"]},{"output_type":"stream","name":"stdout","text":["GPU activated for TensorFlow!\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 06:49:39,043] Trial 0 finished with value: 0.17889190653762826 and parameters: {'units': 112, 'layers': 4, 'batch_size': 64, 'learning_rate': 0.08800885048692345, 'max_depth': 2, 'n_estimators': 100}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 06:52:17,620] Trial 1 finished with value: 0.2679860093297166 and parameters: {'units': 80, 'layers': 3, 'batch_size': 32, 'learning_rate': 0.016375032223498908, 'max_depth': 2, 'n_estimators': 200}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 06:54:42,336] Trial 2 finished with value: 0.18689273594910502 and parameters: {'units': 64, 'layers': 4, 'batch_size': 32, 'learning_rate': 0.04734702011305001, 'max_depth': 6, 'n_estimators': 100}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 06:56:09,030] Trial 3 finished with value: 0.38869341426471016 and parameters: {'units': 32, 'layers': 2, 'batch_size': 64, 'learning_rate': 0.01397592045049318, 'max_depth': 2, 'n_estimators': 150}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 06:58:50,086] Trial 4 finished with value: 0.5731680800823078 and parameters: {'units': 48, 'layers': 3, 'batch_size': 32, 'learning_rate': 0.011166791985854307, 'max_depth': 3, 'n_estimators': 100}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:00:16,103] Trial 5 finished with value: 0.22903690842543675 and parameters: {'units': 48, 'layers': 5, 'batch_size': 64, 'learning_rate': 0.017161567578010725, 'max_depth': 3, 'n_estimators': 200}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:04:43,094] Trial 6 finished with value: 0.302287823926586 and parameters: {'units': 48, 'layers': 3, 'batch_size': 16, 'learning_rate': 0.015329713639797087, 'max_depth': 4, 'n_estimators': 150}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:06:10,493] Trial 7 finished with value: 0.24241766156808883 and parameters: {'units': 64, 'layers': 5, 'batch_size': 64, 'learning_rate': 0.014803123407768213, 'max_depth': 4, 'n_estimators': 200}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:08:43,673] Trial 8 finished with value: 0.20531508264431245 and parameters: {'units': 112, 'layers': 3, 'batch_size': 32, 'learning_rate': 0.025224119479849504, 'max_depth': 6, 'n_estimators': 150}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:10:07,430] Trial 9 finished with value: 0.25470951421475685 and parameters: {'units': 128, 'layers': 3, 'batch_size': 64, 'learning_rate': 0.023391710267954512, 'max_depth': 2, 'n_estimators': 150}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:15:12,074] Trial 10 finished with value: 0.18493218780467005 and parameters: {'units': 112, 'layers': 4, 'batch_size': 16, 'learning_rate': 0.09589856375170491, 'max_depth': 5, 'n_estimators': 50}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:20:01,537] Trial 11 finished with value: 0.18549611299151095 and parameters: {'units': 112, 'layers': 4, 'batch_size': 16, 'learning_rate': 0.0947531298879789, 'max_depth': 5, 'n_estimators': 50}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:24:36,642] Trial 12 finished with value: 0.19236567080267497 and parameters: {'units': 96, 'layers': 4, 'batch_size': 16, 'learning_rate': 0.08509604134464822, 'max_depth': 5, 'n_estimators': 50}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:29:06,072] Trial 13 finished with value: 0.23776413432132124 and parameters: {'units': 128, 'layers': 5, 'batch_size': 16, 'learning_rate': 0.059593976128942315, 'max_depth': 5, 'n_estimators': 50}. Best is trial 0 with value: 0.17889190653762826.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:30:33,087] Trial 14 finished with value: 0.17887844425376298 and parameters: {'units': 96, 'layers': 4, 'batch_size': 64, 'learning_rate': 0.059512746340798485, 'max_depth': 4, 'n_estimators': 100}. Best is trial 14 with value: 0.17887844425376298.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:32:00,301] Trial 15 finished with value: 0.18976948278072467 and parameters: {'units': 96, 'layers': 2, 'batch_size': 64, 'learning_rate': 0.04863503271024656, 'max_depth': 3, 'n_estimators': 100}. Best is trial 14 with value: 0.17887844425376298.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:33:25,827] Trial 16 finished with value: 0.17840428707682254 and parameters: {'units': 96, 'layers': 4, 'batch_size': 64, 'learning_rate': 0.06559588131957612, 'max_depth': 3, 'n_estimators': 100}. Best is trial 16 with value: 0.17840428707682254.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:34:51,307] Trial 17 finished with value: 0.2037167715822176 and parameters: {'units': 80, 'layers': 5, 'batch_size': 64, 'learning_rate': 0.03813229068689874, 'max_depth': 4, 'n_estimators': 100}. Best is trial 16 with value: 0.17840428707682254.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:36:21,518] Trial 18 finished with value: 0.17974361476365114 and parameters: {'units': 96, 'layers': 4, 'batch_size': 64, 'learning_rate': 0.06122669416415635, 'max_depth': 3, 'n_estimators': 100}. Best is trial 16 with value: 0.17840428707682254.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 07:37:42,852] Trial 19 finished with value: 0.17577605821255066 and parameters: {'units': 96, 'layers': 5, 'batch_size': 64, 'learning_rate': 0.06936160364486853, 'max_depth': 4, 'n_estimators': 150}. Best is trial 19 with value: 0.17577605821255066.\n"]},{"output_type":"stream","name":"stdout","text":["Best Parameters: {'units': 96, 'layers': 5, 'batch_size': 64, 'learning_rate': 0.06936160364486853, 'max_depth': 4, 'n_estimators': 150}\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","Train Metrics: MAE=0.0046, MSE=0.0000, RMSE=0.0065, R²=0.9998, MAPE=1.60%\n","Validation Metrics: MAE=0.1582, MSE=0.0309, RMSE=0.1758, R²=-4.2540, MAPE=8.88%\n","Test Metrics: MAE=0.4272, MSE=0.1885, RMSE=0.4342, R²=-30.2420, MAPE=21.08%\n","\n","Overall Timing:\n","Total Training Time: 86.47 seconds\n","Total Validation Time: 0.00 seconds\n","Total Testing Time: 0.00 seconds\n","Total Execution Time: 3056.61 seconds\n"]}]},{"cell_type":"markdown","source":["### bohb"],"metadata":{"id":"XZcwiuVWX2z7"}},{"cell_type":"code","source":["!pip install hpbandster configspace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCxEa4PjX5Rn","executionInfo":{"status":"ok","timestamp":1741766518149,"user_tz":-330,"elapsed":3432,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"812f82f2-89eb-4dd1-d829-24d809545bd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: hpbandster in /usr/local/lib/python3.11/dist-packages (0.7.4)\n","Requirement already satisfied: configspace in /usr/local/lib/python3.11/dist-packages (1.2.1)\n","Requirement already satisfied: Pyro4 in /usr/local/lib/python3.11/dist-packages (from hpbandster) (4.82)\n","Requirement already satisfied: serpent in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.41)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.14.1)\n","Requirement already satisfied: netifaces in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.11.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from configspace) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from configspace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from configspace) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import xgboost as xgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Define LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# LSTM Configurations\n","lstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store LSTM feature representations\n","lstm_features = []\n","\n","for num_layers in lstm_layers:\n","    print(f\"Training LSTM with {num_layers} layers...\")\n","\n","    lstm_model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        lstm_model.train()\n","        optimizer.zero_grad()\n","        outputs = lstm_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    # Extract Feature Representations\n","    lstm_model.eval()\n","    with torch.no_grad():\n","        val_start = time.time()\n","        train_features = lstm_model(X_train_torch).numpy()\n","        val_features = lstm_model(X_val_torch).numpy()\n","        val_time = time.time() - val_start\n","\n","        test_start = time.time()\n","        test_features = lstm_model(X_test_torch).numpy()\n","        test_time = time.time() - test_start\n","\n","    lstm_features.append((train_features, val_features, test_features, train_time, val_time, test_time))\n","\n","# Concatenate Features from All Layers\n","final_train_features = np.hstack([feat[0] for feat in lstm_features])\n","final_val_features = np.hstack([feat[1] for feat in lstm_features])\n","final_test_features = np.hstack([feat[2] for feat in lstm_features])\n","\n","# Record Time for Each Stage\n","total_train_time = sum([feat[3] for feat in lstm_features])\n","total_val_time = sum([feat[4] for feat in lstm_features])\n","total_test_time = sum([feat[5] for feat in lstm_features])\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"subsample\", 0.5, 1.0, default_value=0.8))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"colsample_bytree\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for XGBoost\n","class XGBoostWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = xgb.XGBRegressor(\n","            n_estimators=config[\"n_estimators\"],\n","            learning_rate=config[\"learning_rate\"],\n","            max_depth=config[\"max_depth\"],\n","            subsample=config[\"subsample\"],\n","            colsample_bytree=config[\"colsample_bytree\"],\n","            random_state=42\n","        )\n","        model.fit(final_train_features, Y_train)\n","        Y_val_pred = model.predict(final_val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"lstm_xgb_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = XGBoostWorker(nameserver=\"127.0.0.2\", run_id=\"lstm_xgb_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"lstm_xgb_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best XGBoost Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_xgb_model = xgb.XGBRegressor(\n","    n_estimators=best_params[\"n_estimators\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    max_depth=best_params[\"max_depth\"],\n","    subsample=best_params[\"subsample\"],\n","    colsample_bytree=best_params[\"colsample_bytree\"],\n","    random_state=42\n",")\n","\n","best_xgb_model.fit(final_train_features, Y_train)\n","\n","# Predictions\n","Y_train_pred = best_xgb_model.predict(final_train_features)\n","Y_val_pred = best_xgb_model.predict(final_val_features)\n","Y_test_pred = best_xgb_model.predict(final_test_features)\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", total_train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", total_val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", total_test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_6A0ry-FOSi","executionInfo":{"status":"ok","timestamp":1741767099210,"user_tz":-330,"elapsed":96676,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"a0c672d7-8f02-4d43-853f-dde8623cc384"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training LSTM with 2 layers...\n","Training LSTM with 3 layers...\n","Training LSTM with 5 layers...\n","Train Metrics: (0.003584944664315948, 3.0063306480354804e-05, 0.005483001594049996, 0.9998282302858037, 1.0322452722211097) Time: 54.99114274978638\n","Validation Metrics: (0.15581174276444293, 0.030132125789281858, 0.17358607602363116, -4.123167395238743, 8.743781299923103) Time: 0.16620230674743652\n","Test Metrics: (0.4247630431221444, 0.18645871157432706, 0.4318086515741979, -29.89587188155887, 20.953608295965513) Time: 0.016576290130615234\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"-hE65qREFi0q"}},{"cell_type":"markdown","source":["# Catboost"],"metadata":{"id":"l8gXhi2tF6ov"}},{"cell_type":"markdown","metadata":{"id":"2wfumXfNONWx"},"source":["## Initial"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1P4JE00ZITMA"},"outputs":[],"source":["!pip install catboost"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":429742,"status":"ok","timestamp":1741794361851,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"},"user_tz":-330},"id":"aPmaVdDHIWIH","outputId":"0d022f43-26a7-42ad-b0f8-33d1df6719de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running on CPU\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","Train Metrics: MAE=0.0079, MSE=0.0001, RMSE=0.0112, R²=0.9993, MAPE=2.94%\n","Validation Metrics: MAE=0.1994, MSE=0.0456, RMSE=0.2136, R²=-6.7588, MAPE=11.24%\n","Test Metrics: MAE=0.4684, MSE=0.2254, RMSE=0.4748, R²=-36.3564, MAPE=23.12%\n","Training Times:\n","LSTM-2: 107.05 seconds\n","LSTM-3: 127.05 seconds\n","LSTM-5: 184.52 seconds\n","CatBoost: 0.38 seconds\n","CatBoost Train: 0.01 seconds\n","CatBoost Validate: 0.01 seconds\n","CatBoost Test: 0.01 seconds\n"]}],"source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Ensure TensorFlow runs on CPU\n","tf.config.set_visible_devices([], 'GPU')\n","print(\"Running on CPU\")\n","\n","# Function to define and train an LSTM model on CPU\n","def train_lstm(X_train, Y_train, X_val, Y_val, layers):\n","    model = Sequential()\n","    model.add(LSTM(64, return_sequences=(layers > 1), input_shape=(X_train.shape[1], 1)))\n","    for _ in range(layers - 1):\n","        model.add(LSTM(64, return_sequences=(_ < layers - 2)))\n","    model.add(Dense(1))\n","\n","    model.compile(optimizer='adam', loss='mse')\n","    start_time = time.time()\n","    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","    train_time = time.time() - start_time\n","    return model, train_time\n","\n","# Reshaping input for LSTM\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train 2, 3, and 5-layer LSTM models\n","lstm_models = {}\n","lstm_predictions = {}\n","times = {}\n","\n","for layers in [2, 3, 5]:\n","    model, train_time = train_lstm(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    Y_train_pred = model.predict(X_train_r)\n","    Y_val_pred = model.predict(X_val_r)\n","    Y_test_pred = model.predict(X_test_r)\n","\n","    lstm_models[layers] = model\n","    lstm_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","    times[f'LSTM-{layers}'] = train_time\n","\n","# Prepare input for CatBoost\n","X_train_cat = np.column_stack([lstm_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_cat = np.column_stack([lstm_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_cat = np.column_stack([lstm_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train CatBoost model\n","cat_model = CatBoostRegressor(iterations=100, learning_rate=0.05, depth=3, loss_function='RMSE', task_type='CPU', verbose=0)\n","\n","start_time = time.time()\n","cat_model.fit(X_train_cat, Y_train, eval_set=(X_val_cat, Y_val), verbose=0)\n","times['CatBoost'] = time.time() - start_time\n","\n","# Predictions from CatBoost\n","start_time = time.time()\n","Y_train_pred_cat = cat_model.predict(X_train_cat)\n","times['CatBoost Train'] = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_cat = cat_model.predict(X_val_cat)\n","times['CatBoost Validate'] = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_cat = cat_model.predict(X_test_cat)\n","times['CatBoost Test'] = time.time() - start_time\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_cat)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_cat)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_cat)\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print training times\n","print(\"Training Times:\")\n","for model, t in times.items():\n","    print(f\"{model}: {t:.2f} seconds\")\n"]},{"cell_type":"markdown","metadata":{"id":"3WytbMT3OQUF"},"source":["## Optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3614,"status":"ok","timestamp":1741800563285,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"},"user_tz":-330},"id":"CODtLaVAJKnT","outputId":"11657a74-d257-45b5-fb7b-be7b6d7e8caa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"]}],"source":["!pip install optuna"]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import LSTM, Dense\n","import catboost as cb\n","import optuna\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import time\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Reshape input for LSTM\n","X_train_r = np.expand_dims(X_train.values, axis=-1)\n","X_val_r = np.expand_dims(X_val.values, axis=-1)\n","X_test_r = np.expand_dims(X_test.values, axis=-1)\n","\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","training_times = []  # Store training times per trial\n","\n","def objective(trial):\n","    # LSTM hyperparameters\n","    lstm_units = trial.suggest_int(\"lstm_units\", 32, 128, step=16)\n","    lstm_learning_rate = trial.suggest_loguniform(\"lstm_learning_rate\", 1e-4, 1e-2)\n","    lstm_batch_size = trial.suggest_categorical(\"lstm_batch_size\", [16, 32, 64])\n","    lstm_epochs = trial.suggest_int(\"lstm_epochs\", 10, 50, step=10)\n","\n","    lstm_predictions = {}\n","    start_train_time = time.time()\n","\n","    for layers in [2, 3, 5]:  # Train LSTM models with 2, 3, and 5 layers\n","        model = keras.Sequential()\n","        model.add(LSTM(lstm_units, return_sequences=(layers > 1), input_shape=(X_train_r.shape[1], 1)))\n","\n","        for _ in range(layers - 1):\n","            model.add(LSTM(lstm_units, return_sequences=(_ < layers - 2)))\n","\n","        model.add(Dense(1))  # Output layer\n","\n","        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lstm_learning_rate), loss=\"mse\")\n","\n","        early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","\n","        model.fit(X_train_r, Y_train, validation_data=(X_val_r, Y_val), epochs=lstm_epochs, batch_size=lstm_batch_size, verbose=0, callbacks=[early_stopping])\n","\n","        lstm_predictions[layers] = {\n","            \"train\": model.predict(X_train_r).flatten(),\n","            \"val\": model.predict(X_val_r).flatten(),\n","            \"test\": model.predict(X_test_r).flatten()\n","        }\n","\n","    # Stack LSTM outputs as features for CatBoost\n","    X_train_cat = np.column_stack([lstm_predictions[2][\"train\"], lstm_predictions[3][\"train\"], lstm_predictions[5][\"train\"]])\n","    X_val_cat = np.column_stack([lstm_predictions[2][\"val\"], lstm_predictions[3][\"val\"], lstm_predictions[5][\"val\"]])\n","\n","    # CatBoost hyperparameters\n","    cat_params = {\n","        \"depth\": trial.suggest_int(\"cat_depth\", 3, 10),\n","        \"learning_rate\": trial.suggest_loguniform(\"cat_learning_rate\", 0.01, 0.3),\n","        \"iterations\": trial.suggest_int(\"cat_iterations\", 50, 200, step=50),\n","        \"loss_function\": \"RMSE\",\n","        \"verbose\": 0\n","    }\n","\n","    cat_model = cb.CatBoostRegressor(**cat_params)\n","    cat_model.fit(X_train_cat, Y_train, eval_set=(X_val_cat, Y_val), verbose=0)\n","\n","    training_times.append(time.time() - start_train_time)\n","\n","    # Compute RMSE without 'squared' argument\n","    rmse = np.sqrt(mean_squared_error(Y_val, cat_model.predict(X_val_cat)))\n","    return rmse  # Return RMSE for validation set\n","\n","# Run Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=10)\n","\n","# Extract best parameters separately\n","best_lstm_params = {key: study.best_params[key] for key in [\"lstm_units\", \"lstm_learning_rate\", \"lstm_batch_size\", \"lstm_epochs\"]}\n","best_cat_params = {key: study.best_params[key] for key in [\"cat_depth\", \"cat_learning_rate\", \"cat_iterations\"]}\n","\n","final_lstm_predictions = {}\n","\n","start_total_training_time = time.time()\n","for layers in [2, 3, 5]:\n","    model = keras.Sequential()\n","    model.add(LSTM(best_lstm_params[\"lstm_units\"], return_sequences=(layers > 1), input_shape=(X_train_r.shape[1], 1)))\n","\n","    for _ in range(layers - 1):\n","        model.add(LSTM(best_lstm_params[\"lstm_units\"], return_sequences=(_ < layers - 2)))\n","\n","    model.add(Dense(1))\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=best_lstm_params[\"lstm_learning_rate\"]), loss=\"mse\")\n","\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","    model.fit(X_train_r, Y_train, validation_data=(X_val_r, Y_val), epochs=best_lstm_params[\"lstm_epochs\"], batch_size=best_lstm_params[\"lstm_batch_size\"], verbose=0, callbacks=[early_stopping])\n","\n","    final_lstm_predictions[layers] = {\n","        \"train\": model.predict(X_train_r).flatten(),\n","        \"val\": model.predict(X_val_r).flatten(),\n","        \"test\": model.predict(X_test_r).flatten()\n","    }\n","\n","# Stack LSTM outputs as features for CatBoost\n","X_train_cat = np.column_stack([final_lstm_predictions[2][\"train\"], final_lstm_predictions[3][\"train\"], final_lstm_predictions[5][\"train\"]])\n","X_val_cat = np.column_stack([final_lstm_predictions[2][\"val\"], final_lstm_predictions[3][\"val\"], final_lstm_predictions[5][\"val\"]])\n","X_test_cat = np.column_stack([final_lstm_predictions[2][\"test\"], final_lstm_predictions[3][\"test\"], final_lstm_predictions[5][\"test\"]])\n","\n","# Train final CatBoost model with best parameters\n","final_cat_model = cb.CatBoostRegressor(\n","    depth=best_cat_params[\"cat_depth\"],\n","    learning_rate=best_cat_params[\"cat_learning_rate\"],\n","    iterations=best_cat_params[\"cat_iterations\"],\n","    loss_function=\"RMSE\",\n","    verbose=0\n",")\n","final_cat_model.fit(X_train_cat, Y_train, eval_set=(X_val_cat, Y_val), verbose=0)\n","\n","total_training_time = time.time() - start_total_training_time\n","\n","# Compute final metrics\n","metrics_train = compute_metrics(Y_train, final_cat_model.predict(X_train_cat))\n","metrics_val = compute_metrics(Y_val, final_cat_model.predict(X_val_cat))\n","metrics_test = compute_metrics(Y_test, final_cat_model.predict(X_test_cat))\n","\n","print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n","print(f\"Train Metrics: {metrics_train}\")\n","print(f\"Validation Metrics: {metrics_val}\")\n","print(f\"Test Metrics: {metrics_test}\")\n","print(\"Best hyperparameters:\", study.best_params)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIbW5SzoAK8l","executionInfo":{"status":"ok","timestamp":1741803225020,"user_tz":-330,"elapsed":2624311,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"}},"outputId":"4c9d4c49-f949-411e-9064-383ef4c7c8ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 17:26:33,782] A new study created in memory with name: no-name-ebcda9ec-032e-43f1-8725-3498eb66c6fa\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 17:28:31,258] Trial 0 finished with value: 0.25816994917365316 and parameters: {'lstm_units': 64, 'lstm_learning_rate': 0.0001705971958100671, 'lstm_batch_size': 64, 'lstm_epochs': 10, 'cat_depth': 7, 'cat_learning_rate': 0.020639603404140214, 'cat_iterations': 150}. Best is trial 0 with value: 0.25816994917365316.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 17:31:27,419] Trial 1 finished with value: 0.1962773193804212 and parameters: {'lstm_units': 80, 'lstm_learning_rate': 0.0008710210231706901, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'cat_depth': 3, 'cat_learning_rate': 0.030622147259098682, 'cat_iterations': 200}. Best is trial 1 with value: 0.1962773193804212.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 17:34:33,719] Trial 2 finished with value: 0.26267095914943134 and parameters: {'lstm_units': 128, 'lstm_learning_rate': 0.00027669303492747937, 'lstm_batch_size': 64, 'lstm_epochs': 10, 'cat_depth': 4, 'cat_learning_rate': 0.06333834021957514, 'cat_iterations': 50}. Best is trial 1 with value: 0.1962773193804212.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 17:38:13,343] Trial 3 finished with value: 0.6450866298658534 and parameters: {'lstm_units': 64, 'lstm_learning_rate': 0.004158754741398677, 'lstm_batch_size': 16, 'lstm_epochs': 40, 'cat_depth': 6, 'cat_learning_rate': 0.010104134101946076, 'cat_iterations': 100}. Best is trial 1 with value: 0.1962773193804212.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 17:46:22,223] Trial 4 finished with value: 0.28885128765335555 and parameters: {'lstm_units': 128, 'lstm_learning_rate': 0.00031989849938777223, 'lstm_batch_size': 16, 'lstm_epochs': 50, 'cat_depth': 9, 'cat_learning_rate': 0.013190442535242788, 'cat_iterations': 200}. Best is trial 1 with value: 0.1962773193804212.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 17:53:16,629] Trial 5 finished with value: 0.20152323943935466 and parameters: {'lstm_units': 112, 'lstm_learning_rate': 0.00020441323526829903, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.030867809759269654, 'cat_iterations': 150}. Best is trial 1 with value: 0.1962773193804212.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 17:55:58,179] Trial 6 finished with value: 0.3183646992828983 and parameters: {'lstm_units': 128, 'lstm_learning_rate': 0.00605108383557619, 'lstm_batch_size': 64, 'lstm_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.015650285383408798, 'cat_iterations': 150}. Best is trial 1 with value: 0.1962773193804212.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 18:03:01,220] Trial 7 finished with value: 0.17747179784786724 and parameters: {'lstm_units': 128, 'lstm_learning_rate': 0.00027484521818618376, 'lstm_batch_size': 16, 'lstm_epochs': 40, 'cat_depth': 3, 'cat_learning_rate': 0.1756009026226736, 'cat_iterations': 150}. Best is trial 7 with value: 0.17747179784786724.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 18:06:17,214] Trial 8 finished with value: 0.18194091770413157 and parameters: {'lstm_units': 48, 'lstm_learning_rate': 0.005846682996782721, 'lstm_batch_size': 16, 'lstm_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.044980192488315356, 'cat_iterations': 150}. Best is trial 7 with value: 0.17747179784786724.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-12 18:08:15,488] Trial 9 finished with value: 0.17552266867295574 and parameters: {'lstm_units': 64, 'lstm_learning_rate': 0.008530989433351942, 'lstm_batch_size': 64, 'lstm_epochs': 50, 'cat_depth': 5, 'cat_learning_rate': 0.2182292510555109, 'cat_iterations': 150}. Best is trial 9 with value: 0.17552266867295574.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","Total Training Time: 121.51 seconds\n","Train Metrics: (0.004257372915513549, 3.947103273340496e-05, 0.006282597610336425, 0.9997744783122446, 1.295182957032049)\n","Validation Metrics: (0.15793739230056988, 0.030807504712369788, 0.17552066747927375, -4.237996633008031, 8.86556731118651)\n","Test Metrics: (0.426916664431017, 0.18829290875990368, 0.43392730815184205, -30.19978667370016, 21.060621632420016)\n","Best hyperparameters: {'lstm_units': 64, 'lstm_learning_rate': 0.008530989433351942, 'lstm_batch_size': 64, 'lstm_epochs': 50, 'cat_depth': 5, 'cat_learning_rate': 0.2182292510555109, 'cat_iterations': 150}\n"]}]},{"cell_type":"markdown","metadata":{"id":"eNKo53ErWRl_"},"source":["## BOHB"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14987,"status":"ok","timestamp":1741783787841,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"},"user_tz":-330},"id":"NOVZ9gYjTdOK","outputId":"bb4bf035-54e9-410b-8331-7584bfd961a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.26.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.14.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (10.6.0)\n","Building wheels for collected packages: ConfigSpace\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=configspace-1.2.1-py3-none-any.whl size=115950 sha256=dbd60cdc5f756f07232079b0b07b7296f4f26bd79cce4fefc6be782088d0f828\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","Successfully built ConfigSpace\n","Installing collected packages: ConfigSpace\n","Successfully installed ConfigSpace-1.2.1\n"]}],"source":["!pip install ConfigSpace"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15700,"status":"ok","timestamp":1741783805315,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"},"user_tz":-330},"id":"jrsQ7B-nWOmd","outputId":"3daf3447-a4da-4410-ca20-d18606986bb3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: ConfigSpace in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.14.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=d63aa3b6617a9231d460d008b92cb379a485eb271b190a9bb742af48e2f5b830\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35181 sha256=c21abed8f3c130ca9fcafff180faa48aa569f14d1fc94f5747cf6ef94089b01e\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, hpbandster\n","Successfully installed Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}],"source":["!pip install hpbandster"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":396427,"status":"ok","timestamp":1741784309923,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"},"user_tz":-330},"id":"3KRriiyGWbGM","outputId":"e1472b9a-0244-4782-f07f-96c1e8659b83"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training LSTM with 2 layers...\n","Training LSTM with 3 layers...\n","Training LSTM with 5 layers...\n","Train Metrics: (0.0035267679216303507, 2.885922865108158e-05, 0.005372078615497132, 0.9998351099051409, 1.0097136704254701) Time: 53.528841972351074\n","Validation Metrics: (0.1574971157320392, 0.03066736914552056, 0.1751210128611657, -4.2141713068238875, 8.840333520845986) Time: 0.16505217552185059\n","Test Metrics: (0.42647220587716317, 0.1879136111576758, 0.43349003582282697, -30.136946115892822, 21.038537337252098) Time: 0.016920089721679688\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Define LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# LSTM Configurations\n","lstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","lstm_features = []\n","\n","for num_layers in lstm_layers:\n","    print(f\"Training LSTM with {num_layers} layers...\")\n","\n","    lstm_model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        lstm_model.train()\n","        optimizer.zero_grad()\n","        outputs = lstm_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    lstm_model.eval()\n","    with torch.no_grad():\n","        val_start = time.time()\n","        train_features = lstm_model(X_train_torch).numpy()\n","        val_features = lstm_model(X_val_torch).numpy()\n","        val_time = time.time() - val_start\n","\n","        test_start = time.time()\n","        test_features = lstm_model(X_test_torch).numpy()\n","        test_time = time.time() - test_start\n","\n","    lstm_features.append((train_features, val_features, test_features, train_time, val_time, test_time))\n","\n","# Concatenate Features\n","final_train_features = np.hstack([feat[0] for feat in lstm_features])\n","final_val_features = np.hstack([feat[1] for feat in lstm_features])\n","final_test_features = np.hstack([feat[2] for feat in lstm_features])\n","\n","total_train_time = sum([feat[3] for feat in lstm_features])\n","total_val_time = sum([feat[4] for feat in lstm_features])\n","total_test_time = sum([feat[5] for feat in lstm_features])\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"subsample\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for CatBoost\n","class CatBoostWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = CatBoostRegressor(\n","            iterations=config[\"n_estimators\"],\n","            learning_rate=config[\"learning_rate\"],\n","            depth=config[\"depth\"],\n","            subsample=config[\"subsample\"],\n","            loss_function='RMSE',\n","            verbose=False,\n","            random_seed=42\n","        )\n","        model.fit(final_train_features, Y_train)\n","        Y_val_pred = model.predict(final_val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"lstm_catboost_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = CatBoostWorker(nameserver=\"127.0.0.2\", run_id=\"lstm_catboost_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"lstm_catboost_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best CatBoost Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_catboost_model = CatBoostRegressor(\n","    iterations=best_params[\"n_estimators\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    depth=best_params[\"depth\"],\n","    subsample=best_params[\"subsample\"],\n","    loss_function='RMSE',\n","    verbose=False,\n","    random_seed=42\n",")\n","\n","best_catboost_model.fit(final_train_features, Y_train)\n","\n","# Predictions\n","Y_train_pred = best_catboost_model.predict(final_train_features)\n","Y_val_pred = best_catboost_model.predict(final_val_features)\n","Y_test_pred = best_catboost_model.predict(final_test_features)\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", total_train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", total_val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", total_test_time)\n"]},{"cell_type":"markdown","source":["#Lightboost"],"metadata":{"id":"dXJfUF9MFndB"}},{"cell_type":"markdown","source":["##Initial"],"metadata":{"id":"rTca825_Ft8_"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17018,"status":"ok","timestamp":1741769528723,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"},"user_tz":-330},"id":"jx7RLwDyi_yY","outputId":"304eb543-0fa1-433b-b1e1-58d61a9deb1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training LSTM with 2 layers...\n","Epoch [10/100], Loss: 0.1365\n","Epoch [20/100], Loss: 0.1283\n","Epoch [30/100], Loss: 0.1168\n","Epoch [40/100], Loss: 0.1007\n","Epoch [50/100], Loss: 0.0797\n","Epoch [60/100], Loss: 0.0573\n","Epoch [70/100], Loss: 0.0398\n","Epoch [80/100], Loss: 0.0303\n","Epoch [90/100], Loss: 0.0251\n","Epoch [100/100], Loss: 0.0209\n","\n","Training LSTM with 3 layers...\n","Epoch [10/100], Loss: 0.1326\n","Epoch [20/100], Loss: 0.1253\n","Epoch [30/100], Loss: 0.1130\n","Epoch [40/100], Loss: 0.0904\n","Epoch [50/100], Loss: 0.0599\n","Epoch [60/100], Loss: 0.0455\n","Epoch [70/100], Loss: 0.0359\n","Epoch [80/100], Loss: 0.0253\n","Epoch [90/100], Loss: 0.0173\n","Epoch [100/100], Loss: 0.0112\n","\n","Training LSTM with 5 layers...\n","Epoch [10/100], Loss: 0.1347\n","Epoch [20/100], Loss: 0.1276\n","Epoch [30/100], Loss: 0.1145\n","Epoch [40/100], Loss: 0.0839\n","Epoch [50/100], Loss: 0.0619\n","Epoch [60/100], Loss: 0.0475\n","Epoch [70/100], Loss: 0.0327\n","Epoch [80/100], Loss: 0.0170\n","Epoch [90/100], Loss: 0.0082\n","Epoch [100/100], Loss: 0.0055\n","\n","Training LightGBM on Combined LSTM Embeddings...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024797 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 48960\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 192\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0325901\tvalid_1's rmse: 0.2593\n","[100]\ttraining's rmse: 0.00522188\tvalid_1's rmse: 0.180319\n","[150]\ttraining's rmse: 0.00456783\tvalid_1's rmse: 0.174437\n","[200]\ttraining's rmse: 0.00454574\tvalid_1's rmse: 0.173854\n","\n","Final Model Performance\n","\n","             Model    Dataset      MAE      MSE     RMSE         R²      MAPE  Time (s)\n","LSTM(2,3,5) + LGBM      Train 0.003046 0.000021 0.004546   0.999882  0.925576 11.619256\n","LSTM(2,3,5) + LGBM Validation 0.156105 0.030225 0.173854  -4.138990  8.760582  0.082382\n","LSTM(2,3,5) + LGBM       Test 0.425062 0.186712 0.432102 -29.937905 20.968443  0.157956\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import lightgbm as lgb\n","import pandas as pd\n","import time\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(LSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        return out[:, -1, :]\n","\n","# Set Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).to(device)\n","\n","# Store embeddings for LGBM\n","train_embeddings, val_embeddings, test_embeddings = [], [], []\n","train_time, val_time, test_time = 0, 0, 0\n","\n","# Train multiple LSTM models\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining LSTM with {num_layers} layers...\")\n","    model = LSTMModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Train LSTM\n","    start_time = time.time()\n","    model.train()\n","    for epoch in range(num_epochs):\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","    train_time += time.time() - start_time\n","\n","    # Extract embeddings\n","    model.eval()\n","    with torch.no_grad():\n","        start_time = time.time()\n","        train_embed = model(X_train_torch).cpu().numpy()\n","        val_embed = model(X_val_torch).cpu().numpy()\n","        test_embed = model(X_test_torch).cpu().numpy()\n","        val_time += time.time() - start_time\n","        test_time += time.time() - start_time\n","\n","    train_embeddings.append(train_embed)\n","    val_embeddings.append(val_embed)\n","    test_embeddings.append(test_embed)\n","\n","# Concatenate embeddings from all LSTM models\n","X_train_lgb = np.concatenate(train_embeddings, axis=1)\n","X_val_lgb = np.concatenate(val_embeddings, axis=1)\n","X_test_lgb = np.concatenate(test_embeddings, axis=1)\n","\n","# Ensure correct label shape\n","Y_train_lgb = Y_train.values.flatten()\n","Y_val_lgb = Y_val.values.flatten()\n","Y_test_lgb = Y_test.values.flatten()\n","\n","# Train LightGBM on LSTM embeddings\n","print(\"\\nTraining LightGBM on Combined LSTM Embeddings...\")\n","start_time = time.time()\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train_lgb)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val_lgb, reference=lgb_train)\n","\n","lgb_params = {\n","    \"objective\": \"regression\",\n","    \"metric\": \"rmse\",\n","    \"boosting_type\": \"gbdt\",\n","    \"learning_rate\": 0.05,\n","    \"num_leaves\": 31\n","}\n","\n","lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","train_time += time.time() - start_time\n","\n","# Predictions\n","start_time = time.time()\n","train_pred_lgb = lgb_model.predict(X_train_lgb)\n","val_pred_lgb = lgb_model.predict(X_val_lgb)\n","test_pred_lgb = lgb_model.predict(X_test_lgb)\n","test_time += time.time() - start_time\n","\n","# Compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100  # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# Store results\n","results_df = pd.DataFrame([\n","    [\"LSTM(2,3,5) + LGBM\", \"Train\", *compute_metrics(Y_train_lgb, train_pred_lgb), train_time],\n","    [\"LSTM(2,3,5) + LGBM\", \"Validation\", *compute_metrics(Y_val_lgb, val_pred_lgb), val_time],\n","    [\"LSTM(2,3,5) + LGBM\", \"Test\", *compute_metrics(Y_test_lgb, test_pred_lgb), test_time]\n","], columns=[\"Model\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\", \"Time (s)\"])\n","\n","print(\"\\nFinal Model Performance\\n\")\n","print(results_df.to_string(index=False))"]},{"cell_type":"markdown","metadata":{"id":"7YBxgG0BjIh8"},"source":["## Optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3354,"status":"ok","timestamp":1741771035028,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"},"user_tz":-330},"id":"8ZNylSsba4Ve","outputId":"77825dfe-b01d-4e34-c72a-79b2eb298dc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"]}],"source":["!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":831610,"status":"ok","timestamp":1741775620620,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"},"user_tz":-330},"id":"W5E7hhK3pGBH","outputId":"d82a814e-cbaf-4d41-bd89-8c4ef00c543e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:16:20,688] A new study created in memory with name: no-name-bcf82d8d-d4a5-456d-b09c-fc8c989e4a7e\n"]},{"name":"stdout","output_type":"stream","text":["GPU activated for TensorFlow!\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000112 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:17:06,291] Trial 0 finished with value: 0.17429273242439094 and parameters: {'units': 32, 'layers': 3, 'batch_size': 32, 'learning_rate': 0.056911248396717594, 'num_leaves': 25, 'max_depth': 8, 'min_data_in_leaf': 22}. Best is trial 0 with value: 0.17429273242439094.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000104 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:18:12,846] Trial 1 finished with value: 0.1742805358248773 and parameters: {'units': 128, 'layers': 5, 'batch_size': 32, 'learning_rate': 0.0962007088198759, 'num_leaves': 29, 'max_depth': 10, 'min_data_in_leaf': 8}. Best is trial 1 with value: 0.1742805358248773.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000181 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:18:39,879] Trial 2 finished with value: 0.1742767101719378 and parameters: {'units': 112, 'layers': 2, 'batch_size': 64, 'learning_rate': 0.08608171188110691, 'num_leaves': 48, 'max_depth': 10, 'min_data_in_leaf': 16}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000160 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:20:12,045] Trial 3 finished with value: 0.20752555728636782 and parameters: {'units': 96, 'layers': 3, 'batch_size': 16, 'learning_rate': 0.017293731737602312, 'num_leaves': 48, 'max_depth': 5, 'min_data_in_leaf': 22}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000119 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:20:56,354] Trial 4 finished with value: 0.2767937465671527 and parameters: {'units': 128, 'layers': 3, 'batch_size': 32, 'learning_rate': 0.011979063004636607, 'num_leaves': 26, 'max_depth': 10, 'min_data_in_leaf': 10}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000104 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:21:31,589] Trial 5 finished with value: 0.17443020988475816 and parameters: {'units': 80, 'layers': 5, 'batch_size': 64, 'learning_rate': 0.06237673813838819, 'num_leaves': 43, 'max_depth': 4, 'min_data_in_leaf': 5}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000188 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:22:15,562] Trial 6 finished with value: 0.17437736903676795 and parameters: {'units': 32, 'layers': 2, 'batch_size': 32, 'learning_rate': 0.047131569303028725, 'num_leaves': 22, 'max_depth': 8, 'min_data_in_leaf': 7}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000130 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:23:03,289] Trial 7 finished with value: 0.17444283468950897 and parameters: {'units': 64, 'layers': 3, 'batch_size': 32, 'learning_rate': 0.07873362107737836, 'num_leaves': 41, 'max_depth': 3, 'min_data_in_leaf': 21}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000133 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:24:31,216] Trial 8 finished with value: 0.1742771439509039 and parameters: {'units': 80, 'layers': 2, 'batch_size': 16, 'learning_rate': 0.0751161070599288, 'num_leaves': 37, 'max_depth': 8, 'min_data_in_leaf': 9}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:26:05,919] Trial 9 finished with value: 0.18134520961882286 and parameters: {'units': 80, 'layers': 4, 'batch_size': 16, 'learning_rate': 0.025810517755316478, 'num_leaves': 38, 'max_depth': 4, 'min_data_in_leaf': 16}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000100 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:26:29,268] Trial 10 finished with value: 0.17484687467108034 and parameters: {'units': 112, 'layers': 2, 'batch_size': 64, 'learning_rate': 0.036949833077404945, 'num_leaves': 50, 'max_depth': 7, 'min_data_in_leaf': 28}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000102 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:27:44,137] Trial 11 finished with value: 0.17427751887206752 and parameters: {'units': 64, 'layers': 2, 'batch_size': 16, 'learning_rate': 0.09770932527223122, 'num_leaves': 33, 'max_depth': 9, 'min_data_in_leaf': 14}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000102 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:28:07,749] Trial 12 finished with value: 0.1777818744635142 and parameters: {'units': 96, 'layers': 2, 'batch_size': 64, 'learning_rate': 0.028136147300306644, 'num_leaves': 45, 'max_depth': 6, 'min_data_in_leaf': 12}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000199 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:28:38,162] Trial 13 finished with value: 0.1742791274996243 and parameters: {'units': 96, 'layers': 4, 'batch_size': 64, 'learning_rate': 0.06684431352843298, 'num_leaves': 34, 'max_depth': 9, 'min_data_in_leaf': 18}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000104 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-12 10:29:48,603] Trial 14 finished with value: 0.17439040129346967 and parameters: {'units': 64, 'layers': 2, 'batch_size': 16, 'learning_rate': 0.04496966817326947, 'num_leaves': 38, 'max_depth': 8, 'min_data_in_leaf': 29}. Best is trial 2 with value: 0.1742767101719378.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","[LightGBM] [Warning] Unknown parameter: units\n","[LightGBM] [Warning] Unknown parameter: layers\n","[LightGBM] [Warning] Unknown parameter: batch_size\n","[LightGBM] [Warning] Unknown parameter: units\n","[LightGBM] [Warning] Unknown parameter: layers\n","[LightGBM] [Warning] Unknown parameter: batch_size\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000100 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Warning] Unknown parameter: units\n","[LightGBM] [Warning] Unknown parameter: layers\n","[LightGBM] [Warning] Unknown parameter: batch_size\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Train Metrics: MAE=0.0040, MSE=0.0000, RMSE=0.0056, R²=0.9998, MAPE=1.17%\n","Validation Metrics: MAE=0.0185, MSE=0.0004, RMSE=0.0191, R²=0.9378, MAPE=1.05%\n","Test Metrics: MAE=0.4255, MSE=0.1871, RMSE=0.4326, R²=-30.0043, MAPE=20.99%\n","Total Training Time: 22.32 sec | Validation Time: 1.24 sec | Testing Time: 0.01 sec\n"]}],"source":["import time\n","import numpy as np\n","import optuna\n","import tensorflow as tf\n","import lightgbm as lgb\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Enable GPU for TensorFlow\n","if tf.config.list_physical_devices('GPU'):\n","    try:\n","        tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train an LSTM model on GPU\n","def train_lstm(X_train, Y_train, X_val, Y_val, units, layers, batch_size):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(LSTM(units, return_sequences=(layers > 1), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(LSTM(units, return_sequences=(_ < layers - 2)))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=batch_size, verbose=0)\n","        return model\n","\n","# Function to compute metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Optuna objective function\n","def objective(trial):\n","    # LSTM hyperparameters\n","    units = trial.suggest_int(\"units\", 32, 128, step=16)\n","    layers = trial.suggest_int(\"layers\", 2, 5)\n","    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n","\n","    # LightGBM hyperparameters\n","    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True)\n","    num_leaves = trial.suggest_int(\"num_leaves\", 20, 50)\n","    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n","    min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 5, 30)\n","\n","    # Reshape input for LSTM\n","    X_train_r = np.expand_dims(X_train, axis=-1)\n","    X_val_r = np.expand_dims(X_val, axis=-1)\n","    X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","    train_start_time = time.time()\n","\n","    # Train LSTM\n","    lstm_model = train_lstm(X_train_r, Y_train, X_val_r, Y_val, units, layers, batch_size)\n","    train_time = time.time() - train_start_time\n","\n","    val_start_time = time.time()\n","\n","    # Extract features from LSTM\n","    Y_train_pred = lstm_model.predict(X_train_r)\n","    Y_val_pred = lstm_model.predict(X_val_r)\n","    Y_test_pred = lstm_model.predict(X_test_r)\n","\n","    val_time = time.time() - val_start_time\n","\n","    # Scale extracted features\n","    scaler = MinMaxScaler()\n","    X_train_lgb = scaler.fit_transform(Y_train_pred)\n","    X_val_lgb = scaler.transform(Y_val_pred)\n","    X_test_lgb = scaler.transform(Y_test_pred)\n","\n","    # Train LightGBM\n","    lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","    lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","\n","\n","    model = lgb.train({\n","    \"objective\": \"regression\", \"metric\": \"rmse\", \"boosting_type\": \"gbdt\",\n","    \"num_leaves\": num_leaves, \"learning_rate\": learning_rate,\n","    \"max_depth\": max_depth, \"min_data_in_leaf\": min_data_in_leaf\n","}, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(0)])\n","\n","\n","\n","    Y_val_pred_lgb = model.predict(X_val_lgb)\n","    _, _, rmse, _, _ = compute_metrics(Y_val, Y_val_pred_lgb)\n","\n","    return rmse\n","\n","# Run Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=15)\n","\n","# Train final models with best parameters\n","best_params = study.best_params\n","\n","final_train_start = time.time()\n","final_lstm_model = train_lstm(np.expand_dims(X_train, axis=-1), Y_train, np.expand_dims(X_val, axis=-1), Y_val,\n","                              best_params['units'], best_params['layers'], best_params['batch_size'])\n","final_train_time = time.time() - final_train_start\n","\n","final_val_start = time.time()\n","Y_train_pred = final_lstm_model.predict(np.expand_dims(X_train, axis=-1))\n","Y_val_pred = final_lstm_model.predict(np.expand_dims(X_val, axis=-1))\n","Y_test_pred = final_lstm_model.predict(np.expand_dims(X_test, axis=-1))\n","final_val_time = time.time() - final_val_start\n","\n","scaler = MinMaxScaler()\n","X_train_lgb = scaler.fit_transform(Y_train_pred)\n","X_val_lgb = scaler.transform(Y_val_pred)\n","X_test_lgb = scaler.transform(Y_test_pred)\n","\n","final_lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","final_lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=final_lgb_train)\n","\n","final_lgb_model = lgb.train(best_params, final_lgb_train, valid_sets=[final_lgb_train, final_lgb_val], num_boost_round=200)\n","\n","final_test_start = time.time()\n","Y_test_pred_lgb = final_lgb_model.predict(X_test_lgb)\n","final_test_time = time.time() - final_test_start\n","\n","metrics_train = compute_metrics(Y_train.to_numpy().flatten(), Y_train_pred.flatten())\n","metrics_val = compute_metrics(Y_val.to_numpy().flatten(), Y_val_pred.flatten())\n","metrics_test = compute_metrics(Y_test.to_numpy().flatten(), Y_test_pred_lgb.flatten())\n","\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","print(f\"Total Training Time: {final_train_time:.2f} sec | Validation Time: {final_val_time:.2f} sec | Testing Time: {final_test_time:.2f} sec\")"]},{"cell_type":"markdown","metadata":{"id":"zWOKnkQ_4PJG"},"source":["##BOHB"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13578,"status":"ok","timestamp":1741775656305,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"},"user_tz":-330},"id":"u5fgBROtpNTE","outputId":"35b895c5-3bce-403e-8861-72780808d0d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting configSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.14.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from configSpace) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from configSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from configSpace) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, configSpace, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=ac00d022225905d9732c486b1b169f5b53c60e71b7055fab28a43e54cf0e42bb\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for configSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configSpace: filename=configspace-1.2.1-py3-none-any.whl size=115950 sha256=77dcd888109ca61981e8c7523154b1d53113de212beccc196fb927b601687017\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35181 sha256=6a382a38c3b2bb4e4ed9e56988f542407c5fef26068540ef2e5ad9456af026de\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster configSpace netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, configSpace, hpbandster\n","Successfully installed Pyro4-4.82 configSpace-1.2.1 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}],"source":["!pip install hpbandster configSpace"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114574,"status":"ok","timestamp":1741775781115,"user":{"displayName":"Anoushka Shrivastava","userId":"00353895164281512638"},"user_tz":-330},"id":"L1hAjERP4TBy","outputId":"18fb8d6b-3fb4-4307-e694-330b49d66bc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] feature_fraction is set=0.9091178775308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9091178775308\n","[LightGBM] [Warning] feature_fraction is set=0.9091178775308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9091178775308\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000353 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 3\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9091178775308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9091178775308\n","[LightGBM] [Warning] feature_fraction is set=0.9091178775308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9091178775308\n","[LightGBM] [Warning] feature_fraction is set=0.9091178775308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9091178775308\n","Train Time: 76.72150444984436 BOHB Optimization Time: 30.87048602104187 Validation Time: 0.005930423736572266 Test Time: 0.005907535552978516\n","Train Metrics: (0.0034154536185345124, 2.6371892757062174e-05, 0.005135357120693961, 0.9998493215567972, 0.9895300460277248)\n","Validation Metrics: (0.15634523120140575, 0.03030139991553575, 0.17407297296115715, -4.151946975740545, 8.774335135193532)\n","Test Metrics: (0.4253056256637224, 0.18691994561220834, 0.43234239395669766, -29.97228921986047, 20.980568418634792)\n"]}],"source":["import time\n","import numpy as np\n","import lightgbm as lgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# LSTM Training\n","lstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","lstm_features = []\n","train_time_start = time.time()\n","\n","for num_layers in lstm_layers:\n","    lstm_model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        lstm_model.train()\n","        optimizer.zero_grad()\n","        outputs = lstm_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    lstm_model.eval()\n","    with torch.no_grad():\n","        train_features = lstm_model(X_train_torch).numpy()\n","        val_features = lstm_model(X_val_torch).numpy()\n","        test_features = lstm_model(X_test_torch).numpy()\n","\n","    lstm_features.append((train_features, val_features, test_features))\n","\n","train_time = time.time() - train_time_start\n","\n","# Stack extracted features\n","train_features_stacked = np.hstack([feat[0] for feat in lstm_features])\n","val_features_stacked = np.hstack([feat[1] for feat in lstm_features])\n","test_features_stacked = np.hstack([feat[2] for feat in lstm_features])\n","\n","# Define ConfigSpace for BOHB (LightGBM)\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_leaves\", 20, 300, default_value=50))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 12, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"feature_fraction\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Optimization\n","bohb_start_time = time.time()\n","\n","NS = hpns.NameServer(run_id=\"stacked_lstm_lgb_bohb\", host=\"127.0.0.1\", port=None)\n","NS.start()\n","\n","class LightGBMWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = lgb.LGBMRegressor(\n","            num_leaves=config[\"num_leaves\"],\n","            max_depth=config[\"max_depth\"],\n","            learning_rate=config[\"learning_rate\"],\n","            feature_fraction=config[\"feature_fraction\"],\n","            random_state=42\n","        )\n","        model.fit(self.train_features, Y_train)\n","        val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","worker = LightGBMWorker(\n","    train_features=train_features_stacked,\n","    val_features=val_features_stacked,\n","    nameserver=\"127.0.0.1\",\n","    run_id=\"stacked_lstm_lgb_bohb\"\n",")\n","worker.run(background=True)\n","\n","bohb = BOHB(\n","    configspace=get_config_space(),\n","    run_id=\"stacked_lstm_lgb_bohb\",\n","    nameserver=\"127.0.0.1\",\n","    min_budget=1,\n","    max_budget=3\n",")\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","bohb_time = time.time() - bohb_start_time\n","\n","# Train final LightGBM model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","best_lgb_model = lgb.LGBMRegressor(**best_params, random_state=42)\n","best_lgb_model.fit(train_features_stacked, Y_train)\n","\n","# Predictions and metrics\n","val_time_start = time.time()\n","Y_val_pred = best_lgb_model.predict(val_features_stacked)\n","val_time = time.time() - val_time_start\n","\n","test_time_start = time.time()\n","Y_test_pred = best_lgb_model.predict(test_features_stacked)\n","test_time = time.time() - test_time_start\n","\n","train_metrics = calculate_metrics(Y_train, best_lgb_model.predict(train_features_stacked))\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print results\n","print(\"Train Time:\", train_time, \"BOHB Optimization Time:\", bohb_time, \"Validation Time:\", val_time, \"Test Time:\", test_time)\n","print(\"Train Metrics:\", train_metrics)\n","print(\"Validation Metrics:\", val_metrics)\n","print(\"Test Metrics:\", test_metrics)\n"]}]}