{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["5ebGneEwoKEe","5d4hGeSAw953","ufX0JKGFDR-c","CWWTk6lQM-90","63SmLe5aNDh7","nW9Vq7ibPQQf","mrXDDEWZQrDj","0XWZweeUQs5E","11fjBsEN1A98","AoMJJT3BqcII","772VFiL1qd8f","QbsT7a3yqiVz"],"authorship_tag":"ABX9TyPeuzsWPvJ7XRWqpXFxaeGd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Initial Code"],"metadata":{"id":"5ebGneEwoKEe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WY7I5C1zqFJt"},"outputs":[],"source":["# Importing necessary libraries for data analysis and manipulation\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","# For handling warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33651,"status":"ok","timestamp":1739422213154,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"3RKq8vfwqVHB","outputId":"0cb86fb0-b86e-4fb3-8463-2463752c7748"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4yeLMDyqd2o"},"outputs":[],"source":["df_aapl = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stocks/AAPL.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHvcgRGPruCy"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import boxcox\n","\n","df_aapl['Close_log'] = np.log(df_aapl['Close'] + 1)\n","df_aapl['Close_sqrt'] = np.sqrt(df_aapl['Close'])\n","df_aapl['Close_boxcox'], _ = boxcox(df_aapl['Close'] + 1)\n"]},{"cell_type":"markdown","metadata":{"id":"lLz5cmQlryah"},"source":["This code calculates the skewness of the 'Close' column in the `df_aapl` DataFrame before and after applying various transformations:\n","\n","1. **Original Skewness**: Calculates the skewness of the original 'Close' data.\n","2. **Log Transformation Skewness**: Calculates the skewness of the 'Close_log' column after applying the log transformation.\n","3. **Square Root Transformation Skewness**: Calculates the skewness of the 'Close_sqrt' column after applying the square root transformation.\n","4. **Box-Cox Transformation Skewness**: Calculates the skewness of the 'Close_boxcox' column after applying the Box-Cox transformation.\n","\n","The printed results help assess how each transformation affects the distribution's symmetry and the success of skewness correction.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1739422213670,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"DIPGiQydr2K0","outputId":"1d0fc15e-14ba-44a1-a1b4-1c04112e3117"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Skewness: 2.5045276102319933\n","Log Transformation Skewness: 0.8535555176510303\n","Square Root Transformation Skewness: 1.6211545809555206\n","Box-Cox Transformation Skewness: 0.43527466713563334\n"]}],"source":["\n","skew_original = df_aapl['Close'].skew()\n","skew_log = df_aapl['Close_log'].skew()\n","skew_sqrt = df_aapl['Close_sqrt'].skew()\n","skew_boxcox = pd.Series(df_aapl['Close_boxcox']).skew()\n","\n","print(f\"Original Skewness: {skew_original}\")\n","print(f\"Log Transformation Skewness: {skew_log}\")\n","print(f\"Square Root Transformation Skewness: {skew_sqrt}\")\n","print(f\"Box-Cox Transformation Skewness: {skew_boxcox}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"De4W27wEr9-p"},"outputs":[],"source":["\n","df_aapl['Open_log'] = np.log(df_aapl['Open'])\n","df_aapl['High_log'] = np.log(df_aapl['High'])\n","df_aapl['Low_log'] = np.log(df_aapl['Low'])\n","df_aapl['Adj Close_log'] = np.log(df_aapl['Adj Close'])\n","df_aapl['Volume_log'] = np.log(df_aapl['Volume'])\n","\n","\n","df_aapl['Open_sqrt'] = np.sqrt(df_aapl['Open'])\n","df_aapl['High_sqrt'] = np.sqrt(df_aapl['High'])\n","df_aapl['Low_sqrt'] = np.sqrt(df_aapl['Low'])\n","df_aapl['Adj Close_sqrt'] = np.sqrt(df_aapl['Adj Close'])\n","df_aapl['Volume_sqrt'] = np.sqrt(df_aapl['Volume'])\n","\n","from scipy.stats import boxcox\n","df_aapl['Open_boxcox'], _ = boxcox(df_aapl['Open'])\n","df_aapl['High_boxcox'], _ = boxcox(df_aapl['High'])\n","df_aapl['Low_boxcox'], _ = boxcox(df_aapl['Low'])\n","df_aapl['Adj Close_boxcox'], _ = boxcox(df_aapl['Adj Close'])"]},{"cell_type":"markdown","source":["This helps compare how the transformations reduce skewness in the data, aiming for a more normal distribution."],"metadata":{"id":"2XrZQHaDAigS"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1739422214125,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"of1KONYmsC8t","outputId":"068f89fd-9855-4e96-c667-f466b9ca2268"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness Before Transformation:\n"," Open         2.504632\n","High         2.502208\n","Low          2.506714\n","Adj Close    2.550677\n","Volume       3.565699\n","dtype: float64\n","\n","Skewness After Transformation:\n"," Open_log            0.482872\n","High_log            0.481997\n","Low_log             0.484246\n","Adj Close_log       0.494009\n","Open_sqrt           1.620771\n","High_sqrt           1.621456\n","Low_sqrt            1.620661\n","Adj Close_sqrt      1.679402\n","Volume_sqrt         1.299776\n","Open_boxcox         0.181226\n","High_boxcox         0.179749\n","Low_boxcox          0.182882\n","Adj Close_boxcox    0.180085\n","dtype: float64\n"]}],"source":["\n","skewness_before = df_aapl[['Open', 'High', 'Low', 'Adj Close', 'Volume']].skew()\n","skewness_after = df_aapl[['Open_log', 'High_log', 'Low_log', 'Adj Close_log',\n","                          'Open_sqrt', 'High_sqrt', 'Low_sqrt', 'Adj Close_sqrt', 'Volume_sqrt',\n","                          'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox']].skew()\n","\n","print(\"Skewness Before Transformation:\\n\", skewness_before)\n","print(\"\\nSkewness After Transformation:\\n\", skewness_after)\n"]},{"cell_type":"markdown","source":["- Applied Box-Cox transformation to the 'Open', 'High', 'Low', 'Adj Close', and 'Close' columns.\n","- Recalculated skewness after the transformation to reduce skew and normalize the data for modeling."],"metadata":{"id":"zfEokf4iAmnv"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1739422214126,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"s9oEP05csI66","outputId":"8bc23e11-5b0c-4765-fe00-85d5cd84210f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness After Box-Cox Transformation:\n","Open_boxcox         0.435237\n","High_boxcox         0.433381\n","Low_boxcox          0.437331\n","Adj Close_boxcox    0.458762\n","Close_boxcox        0.435275\n","dtype: float64\n"]}],"source":["from scipy import stats\n","\n","df_aapl['Open_boxcox'], _ = stats.boxcox(df_aapl['Open'] + 1)\n","df_aapl['High_boxcox'], _ = stats.boxcox(df_aapl['High'] + 1)\n","df_aapl['Low_boxcox'], _ = stats.boxcox(df_aapl['Low'] + 1)\n","df_aapl['Adj Close_boxcox'], _ = stats.boxcox(df_aapl['Adj Close'] + 1)\n","df_aapl['Close_boxcox'], _ = stats.boxcox(df_aapl['Close'] + 1)\n","\n","skewness_after_boxcox = df_aapl[['Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox', 'Close_boxcox']].skew()\n","\n","print(\"Skewness After Box-Cox Transformation:\")\n","print(skewness_after_boxcox)\n"]},{"cell_type":"markdown","source":["Feature Selection"],"metadata":{"id":"uvZe7IzRAwHu"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1739422214126,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"},"user_tz":-330},"id":"aczNHUI4rk8x","outputId":"ff8ef5ac-ad44-4584-cf12-1e57c4ecf4c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["         Date      Open      High       Low  Adj Close     Close     Volume  \\\n","0  1980-12-12  0.128348  0.128906  0.128348   0.098943  0.128348  469033600   \n","1  1980-12-15  0.122210  0.122210  0.121652   0.093781  0.121652  175884800   \n","2  1980-12-16  0.113281  0.113281  0.112723   0.086898  0.112723  105728000   \n","3  1980-12-17  0.115513  0.116071  0.115513   0.089049  0.115513   86441600   \n","4  1980-12-18  0.118862  0.119420  0.118862   0.091630  0.118862   73449600   \n","\n","   Open_boxcox  High_boxcox  Low_boxcox  Adj Close_boxcox  Close_boxcox  \n","0     0.117689     0.118173    0.117674          0.092374      0.117689  \n","1     0.112503     0.112516    0.112016          0.087857      0.112030  \n","2     0.104886     0.104897    0.104395          0.081785      0.104407  \n","3     0.106798     0.107287    0.106786          0.083688      0.106798  \n","4     0.109657     0.110145    0.109644          0.085966      0.109657  \n"]}],"source":["\n","df_aapl_cleaned = df_aapl[['Date', 'Open', 'High', 'Low', 'Adj Close', 'Close', 'Volume',\n","                           'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox',\n","                           'Close_boxcox']]\n","\n","print(df_aapl_cleaned.head())\n"]},{"cell_type":"markdown","source":["### Train Validation Test Split\n","\n","The code splits the data into training, validation, and test sets. The features `X` and target `Y` are split as follows:\n","\n","- 70% for training (`X_train`, `Y_train`)\n","- 15% for validation (`X_val`, `Y_val`)\n","- 15% for testing (`X_test`, `Y_test`)\n","\n","The split is done using a 30% test size, followed by splitting the remaining 70% into validation and test sets without shuffling (time series data)."],"metadata":{"id":"chw5ijVT_JRM"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X = df_aapl_cleaned[['Open_boxcox', 'High_boxcox', 'Low_boxcox']]\n","Y = df_aapl_cleaned['Close_boxcox']\n","\n","X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, shuffle=False)\n","X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, shuffle=False)\n","\n","print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSztwxnoy8-U","executionInfo":{"status":"ok","timestamp":1739422214126,"user_tz":-330,"elapsed":7,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"b126d68e-d8bb-43a3-9f7b-d4b033ea5bb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set: (7736, 3), Validation set: (1658, 3), Test set: (1658, 3)\n"]}]},{"cell_type":"markdown","source":["## GPU Activation"],"metadata":{"id":"5d4hGeSAw953"}},{"cell_type":"code","source":["import torch\n","\n","# Check GPU status\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is enabled:\", torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"No GPU found, using CPU.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R0d6nftTwf7T","executionInfo":{"status":"ok","timestamp":1739422222880,"user_tz":-330,"elapsed":4821,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"f3832687-acd5-49b3-fb64-a9a1e059c23a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is enabled: Tesla T4\n"]}]},{"cell_type":"markdown","source":["##GRU - xgb"],"metadata":{"id":"ufX0JKGFDR-c"}},{"cell_type":"markdown","source":["### xgb initial"],"metadata":{"id":"CWWTk6lQM-90"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import xgboost as xgb\n","import pandas as pd\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h0)\n","        return self.fc(out[:, -1, :])  # Take last time step output\n","\n","# Set Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]  # Different GRU layers\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling (helps GRU)\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100   # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# DataFrame to store results\n","columns = [\"Layers\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\"]\n","results_df = pd.DataFrame(columns=columns)\n","\n","# Train multiple GRU models\n","gru_outputs = {}  # Store GRU embeddings for XGBoost later\n","\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining GRU with {num_layers} layers...\")\n","\n","    # Initialize model, loss function, and optimizer\n","    model = GRUModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        train_pred = model(X_train_torch).cpu().numpy()\n","        val_pred = model(X_val_torch).cpu().numpy()\n","        test_pred = model(X_test_torch).cpu().numpy()\n","\n","    # Inverse transform predictions\n","    train_pred_actual = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    test_pred_actual = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","    # Compute metrics for each dataset\n","    metrics_train = compute_metrics(Y_train.values.flatten(), train_pred_actual.flatten())\n","    metrics_val = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","    metrics_test = compute_metrics(Y_test.values.flatten(), test_pred_actual.flatten())\n","\n","    # Append results to DataFrame\n","    results_df = pd.concat([\n","        results_df,\n","        pd.DataFrame([\n","            [num_layers, \"Train\", *metrics_train],\n","            [num_layers, \"Validation\", *metrics_val],\n","            [num_layers, \"Test\", *metrics_test]\n","        ], columns=columns)\n","    ], ignore_index=True)\n","\n","    # Store GRU embeddings for XGBoost\n","    gru_outputs[num_layers] = {\n","        \"train\": train_pred_actual,\n","        \"val\": val_pred_actual,\n","        \"test\": test_pred_actual\n","    }\n","\n","# Find the best GRU model (Lowest Validation MAPE)\n","best_model = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","best_layers = int(best_model[\"Layers\"])\n","\n","# Display GRU Results\n","print(\"\\nGRU Model Performance Comparison (2, 3, and 5 Layers)\\n\")\n","print(results_df.to_string(index=False))\n","\n","print(f\"\\nBest GRU Model: {best_layers} Layers (Based on Lowest Validation MAPE)\\n\")\n","\n","# ---------- XGBoost on Best GRU Embeddings ----------\n","print(\"\\nTraining XGBoost on Best GRU Embeddings...\")\n","\n","# Use the best GRU's output as features for XGBoost\n","X_train_xgb = gru_outputs[best_layers][\"train\"]\n","X_val_xgb = gru_outputs[best_layers][\"val\"]\n","X_test_xgb = gru_outputs[best_layers][\"test\"]\n","\n","# XGBoost Model\n","xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.05)\n","xgb_model.fit(X_train_xgb, Y_train.values)\n","\n","# Predictions\n","train_pred_xgb = xgb_model.predict(X_train_xgb)\n","val_pred_xgb = xgb_model.predict(X_val_xgb)\n","test_pred_xgb = xgb_model.predict(X_test_xgb)\n","\n","# Compute metrics for XGBoost\n","metrics_train_xgb = compute_metrics(Y_train.values.flatten(), train_pred_xgb.flatten())\n","metrics_val_xgb = compute_metrics(Y_val.values.flatten(), val_pred_xgb.flatten())\n","metrics_test_xgb = compute_metrics(Y_test.values.flatten(), test_pred_xgb.flatten())\n","\n","# Append XGBoost results to DataFrame\n","results_df = pd.concat([\n","    results_df,\n","    pd.DataFrame([\n","        [f\"GRU({best_layers}) + XGBoost\", \"Train\", *metrics_train_xgb],\n","        [f\"GRU({best_layers}) + XGBoost\", \"Validation\", *metrics_val_xgb],\n","        [f\"GRU({best_layers}) + XGBoost\", \"Test\", *metrics_test_xgb]\n","    ], columns=columns)\n","], ignore_index=True)\n","\n","# Display Final Results\n","print(\"\\nFinal Model Performance (GRU vs GRU + XGBoost)\\n\")\n","print(results_df.to_string(index=False))\n","\n","# Best Model Selection\n","best_overall = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","print(f\"\\nBest Overall Model: {best_overall['Layers']} (Based on Lowest Validation MAPE)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z641k9nMqbUz","executionInfo":{"status":"ok","timestamp":1739422565516,"user_tz":-330,"elapsed":11646,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"b943bab4-bd97-4b31-994c-79167267bf44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training GRU with 2 layers...\n","Epoch [10/100], Loss: 0.0734\n","Epoch [20/100], Loss: 0.0733\n","Epoch [30/100], Loss: 0.0715\n","Epoch [40/100], Loss: 0.0717\n","Epoch [50/100], Loss: 0.0715\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training GRU with 3 layers...\n","Epoch [10/100], Loss: 0.1090\n","Epoch [20/100], Loss: 0.0729\n","Epoch [30/100], Loss: 0.0734\n","Epoch [40/100], Loss: 0.0724\n","Epoch [50/100], Loss: 0.0716\n","Epoch [60/100], Loss: 0.0716\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training GRU with 5 layers...\n","Epoch [10/100], Loss: 0.0731\n","Epoch [20/100], Loss: 0.0729\n","Epoch [30/100], Loss: 0.0717\n","Epoch [40/100], Loss: 0.0715\n","Epoch [50/100], Loss: 0.0715\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","GRU Model Performance Comparison (2, 3, and 5 Layers)\n","\n","Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","     2      Train 0.327278 0.174978 0.418304    0.000246 124.735262\n","     2 Validation 1.294107 1.680615 1.296385 -284.743881  74.046097\n","     2       Test 1.563597 2.450891 1.565532 -405.108027  77.542481\n","     3      Train 0.327524 0.175527 0.418960   -0.002892 124.749186\n","     3 Validation 1.296273 1.686236 1.298551 -285.699543  74.170142\n","     3       Test 1.566087 2.458699 1.568024 -406.401880  77.666008\n","     5      Train 0.327681 0.174952 0.418272    0.000396 125.092968\n","     5 Validation 1.292370 1.676100 1.294643 -283.976269  73.946799\n","     5       Test 1.561360 2.443877 1.563290 -403.945923  77.431615\n","\n","Best GRU Model: 5 Layers (Based on Lowest Validation MAPE)\n","\n","\n","Training XGBoost on Best GRU Embeddings...\n","\n","Final Model Performance (GRU vs GRU + XGBoost)\n","\n","          Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","               2      Train 0.327278 0.174978 0.418304    0.000246 124.735262\n","               2 Validation 1.294107 1.680615 1.296385 -284.743881  74.046097\n","               2       Test 1.563597 2.450891 1.565532 -405.108027  77.542481\n","               3      Train 0.327524 0.175527 0.418960   -0.002892 124.749186\n","               3 Validation 1.296273 1.686236 1.298551 -285.699543  74.170142\n","               3       Test 1.566087 2.458699 1.568024 -406.401880  77.666008\n","               5      Train 0.327681 0.174952 0.418272    0.000396 125.092968\n","               5 Validation 1.292370 1.676100 1.294643 -283.976269  73.946799\n","               5       Test 1.561360 2.443877 1.563290 -403.945923  77.431615\n","GRU(5) + XGBoost      Train 0.008714 0.000150 0.012258    0.999141   2.868573\n","GRU(5) + XGBoost Validation 0.168893 0.034395 0.185459   -4.847988   9.493946\n","GRU(5) + XGBoost       Test 0.437893 0.197785 0.444730  -31.772628  21.606027\n","\n","Best Overall Model: GRU(5) + XGBoost (Based on Lowest Validation MAPE)\n"]}]},{"cell_type":"code","source":["!pip install xgboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WzUFo6HQEdox","executionInfo":{"status":"ok","timestamp":1739295166453,"user_tz":-330,"elapsed":3293,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"eb8371e1-b6dc-4024-efc5-90803eb2ad5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n"]}]},{"cell_type":"markdown","source":["### xgb optuna"],"metadata":{"id":"63SmLe5aNDh7"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ci560XxnwVt1","executionInfo":{"status":"ok","timestamp":1739424114911,"user_tz":-330,"elapsed":9838,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"a364b1a9-f7fd-43e2-84c5-231de2b8f4aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import xgboost as xgb\n","import pandas as pd\n","import optuna\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h0)\n","        return self.fc(out[:, -1, :])  # Last time step output\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100   # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Optuna GRU Optimization\n","def objective_gru(trial):\n","    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n","    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128)\n","    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n","    num_epochs = 100\n","\n","    # Train GRU\n","    model = GRUModel(input_size=3, hidden_size=hidden_size, num_layers=num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluate GRU\n","    model.eval()\n","    with torch.no_grad():\n","        val_pred = model(X_val_torch).cpu().numpy()\n","\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    mape = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())[-1]  # MAPE\n","\n","    return mape  # Minimize Validation MAPE\n","\n","# Run Optuna for GRU\n","study_gru = optuna.create_study(direction=\"minimize\")\n","study_gru.optimize(objective_gru, n_trials=20)\n","best_gru_params = study_gru.best_params\n","\n","# Train Best GRU Model\n","best_gru_model = GRUModel(input_size=3, hidden_size=best_gru_params[\"hidden_size\"], num_layers=best_gru_params[\"num_layers\"]).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(best_gru_model.parameters(), lr=best_gru_params[\"learning_rate\"])\n","\n","for epoch in range(100):\n","    best_gru_model.train()\n","    outputs = best_gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","# Extract GRU Embeddings\n","best_gru_model.eval()\n","with torch.no_grad():\n","    X_train_xgb = best_gru_model(X_train_torch).cpu().numpy()\n","    X_val_xgb = best_gru_model(X_val_torch).cpu().numpy()\n","    X_test_xgb = best_gru_model(X_test_torch).cpu().numpy()\n","\n","# Optuna XGBoost Optimization\n","def objective_xgb(trial):\n","    params = {\n","        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n","        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n","        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n","        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n","        \"objective\": \"reg:squarederror\"\n","    }\n","\n","    xgb_model = xgb.XGBRegressor(**params)\n","    xgb_model.fit(X_train_xgb, Y_train.values)\n","\n","    val_pred_xgb = xgb_model.predict(X_val_xgb)\n","    mape = compute_metrics(Y_val.values.flatten(), val_pred_xgb.flatten())[-1]  # MAPE\n","\n","    return mape  # Minimize Validation MAPE\n","\n","# Run Optuna for XGBoost\n","study_xgb = optuna.create_study(direction=\"minimize\")\n","study_xgb.optimize(objective_xgb, n_trials=20)\n","best_xgb_params = study_xgb.best_params\n","\n","# Train Best XGBoost Model\n","best_xgb_model = xgb.XGBRegressor(**best_xgb_params)\n","best_xgb_model.fit(X_train_xgb, Y_train.values)\n","\n","# Predictions\n","train_pred_xgb = best_xgb_model.predict(X_train_xgb)\n","val_pred_xgb = best_xgb_model.predict(X_val_xgb)\n","test_pred_xgb = best_xgb_model.predict(X_test_xgb)\n","\n","# Compute final metrics\n","metrics_train_xgb = compute_metrics(Y_train.values.flatten(), train_pred_xgb.flatten())\n","metrics_val_xgb = compute_metrics(Y_val.values.flatten(), val_pred_xgb.flatten())\n","metrics_test_xgb = compute_metrics(Y_test.values.flatten(), test_pred_xgb.flatten())\n","\n","# Print Final Results\n","print(\"\\nFinal Model Performance (GRU + XGBoost):\\n\")\n","print(f\"Training:    MAE={metrics_train_xgb[0]:.4f}, MSE={metrics_train_xgb[1]:.4f}, RMSE={metrics_train_xgb[2]:.4f}, R²={metrics_train_xgb[3]:.4f}, MAPE={metrics_train_xgb[4]:.2f}%\")\n","print(f\"Validation:  MAE={metrics_val_xgb[0]:.4f}, MSE={metrics_val_xgb[1]:.4f}, RMSE={metrics_val_xgb[2]:.4f}, R²={metrics_val_xgb[3]:.4f}, MAPE={metrics_val_xgb[4]:.2f}%\")\n","print(f\"Test:        MAE={metrics_test_xgb[0]:.4f}, MSE={metrics_test_xgb[1]:.4f}, RMSE={metrics_test_xgb[2]:.4f}, R²={metrics_test_xgb[3]:.4f}, MAPE={metrics_test_xgb[4]:.2f}%\")\n","\n","print(\"\\nBest GRU Parameters:\", best_gru_params)\n","print(\"Best XGBoost Parameters:\", best_xgb_params)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6L5gggBwUPz","executionInfo":{"status":"ok","timestamp":1739424176972,"user_tz":-330,"elapsed":58609,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"278f14af-239d-43de-b371-96e9d06fbbe4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 05:22:01,086] A new study created in memory with name: no-name-931e6b07-adda-4472-833b-4dd74f3518b5\n","[I 2025-02-13 05:22:05,075] Trial 0 finished with value: 72.33646716364942 and parameters: {'num_layers': 3, 'hidden_size': 99, 'learning_rate': 0.00024761085506235436}. Best is trial 0 with value: 72.33646716364942.\n","[I 2025-02-13 05:22:07,771] Trial 1 finished with value: 74.14669973103528 and parameters: {'num_layers': 2, 'hidden_size': 105, 'learning_rate': 0.0014432661317700867}. Best is trial 0 with value: 72.33646716364942.\n","[I 2025-02-13 05:22:13,804] Trial 2 finished with value: 73.98521400889024 and parameters: {'num_layers': 3, 'hidden_size': 128, 'learning_rate': 0.0056635588935976125}. Best is trial 0 with value: 72.33646716364942.\n","[I 2025-02-13 05:22:17,059] Trial 3 finished with value: 73.94752857490033 and parameters: {'num_layers': 4, 'hidden_size': 81, 'learning_rate': 0.004318020227720887}. Best is trial 0 with value: 72.33646716364942.\n","[I 2025-02-13 05:22:19,001] Trial 4 finished with value: 73.80764411696799 and parameters: {'num_layers': 5, 'hidden_size': 36, 'learning_rate': 0.0004704845819753353}. Best is trial 0 with value: 72.33646716364942.\n","[I 2025-02-13 05:22:24,891] Trial 5 finished with value: 73.97024010768581 and parameters: {'num_layers': 5, 'hidden_size': 117, 'learning_rate': 0.0034001571243709454}. Best is trial 0 with value: 72.33646716364942.\n","[I 2025-02-13 05:22:27,720] Trial 6 finished with value: 72.5247599196881 and parameters: {'num_layers': 5, 'hidden_size': 75, 'learning_rate': 0.00011970861790291667}. Best is trial 0 with value: 72.33646716364942.\n","[I 2025-02-13 05:22:33,228] Trial 7 finished with value: 73.97411654152712 and parameters: {'num_layers': 5, 'hidden_size': 113, 'learning_rate': 0.0016769354762487445}. Best is trial 0 with value: 72.33646716364942.\n","[I 2025-02-13 05:22:35,584] Trial 8 finished with value: 67.16562358208982 and parameters: {'num_layers': 2, 'hidden_size': 106, 'learning_rate': 0.0001268118559728349}. Best is trial 8 with value: 67.16562358208982.\n","[I 2025-02-13 05:22:37,907] Trial 9 finished with value: 74.04668953528531 and parameters: {'num_layers': 2, 'hidden_size': 104, 'learning_rate': 0.0011174865055849157}. Best is trial 8 with value: 67.16562358208982.\n","[I 2025-02-13 05:22:39,466] Trial 10 finished with value: 71.54715915773211 and parameters: {'num_layers': 2, 'hidden_size': 77, 'learning_rate': 0.00012091506555196748}. Best is trial 8 with value: 67.16562358208982.\n","[I 2025-02-13 05:22:40,924] Trial 11 finished with value: 73.79072094098987 and parameters: {'num_layers': 2, 'hidden_size': 75, 'learning_rate': 0.00011917819875887594}. Best is trial 8 with value: 67.16562358208982.\n","[I 2025-02-13 05:22:42,166] Trial 12 finished with value: 70.4942467966912 and parameters: {'num_layers': 3, 'hidden_size': 59, 'learning_rate': 0.00031622611013685316}. Best is trial 8 with value: 67.16562358208982.\n","[I 2025-02-13 05:22:43,392] Trial 13 finished with value: 72.1990367198804 and parameters: {'num_layers': 3, 'hidden_size': 52, 'learning_rate': 0.0003907790629667332}. Best is trial 8 with value: 67.16562358208982.\n","[I 2025-02-13 05:22:45,012] Trial 14 finished with value: 70.29873649508158 and parameters: {'num_layers': 4, 'hidden_size': 56, 'learning_rate': 0.0002575383340966658}. Best is trial 8 with value: 67.16562358208982.\n","[I 2025-02-13 05:22:48,143] Trial 15 finished with value: 74.02871287844361 and parameters: {'num_layers': 4, 'hidden_size': 90, 'learning_rate': 0.000682162608223932}. Best is trial 8 with value: 67.16562358208982.\n","[I 2025-02-13 05:22:49,728] Trial 16 finished with value: 61.758547375784175 and parameters: {'num_layers': 4, 'hidden_size': 56, 'learning_rate': 0.00020713388255328886}. Best is trial 16 with value: 61.758547375784175.\n","[I 2025-02-13 05:22:50,776] Trial 17 finished with value: 80.5517210418166 and parameters: {'num_layers': 4, 'hidden_size': 33, 'learning_rate': 0.00018548016303111754}. Best is trial 16 with value: 61.758547375784175.\n","[I 2025-02-13 05:22:51,821] Trial 18 finished with value: 73.22087837824779 and parameters: {'num_layers': 3, 'hidden_size': 43, 'learning_rate': 0.0006100670314075539}. Best is trial 16 with value: 61.758547375784175.\n","[I 2025-02-13 05:22:53,322] Trial 19 finished with value: 73.16224749739305 and parameters: {'num_layers': 4, 'hidden_size': 62, 'learning_rate': 0.00019901046822359394}. Best is trial 16 with value: 61.758547375784175.\n","[I 2025-02-13 05:22:54,912] A new study created in memory with name: no-name-d1b3b01f-c0fc-4d81-82a1-3a0c1575cc62\n","[I 2025-02-13 05:22:55,036] Trial 0 finished with value: 8.879212308713894 and parameters: {'n_estimators': 325, 'learning_rate': 0.15500601846781087, 'max_depth': 4, 'subsample': 0.6891668436618892, 'colsample_bytree': 0.568179478866405}. Best is trial 0 with value: 8.879212308713894.\n","[I 2025-02-13 05:22:55,149] Trial 1 finished with value: 11.023709639221103 and parameters: {'n_estimators': 201, 'learning_rate': 0.018275319388168666, 'max_depth': 7, 'subsample': 0.8037045763767623, 'colsample_bytree': 0.5286757683846821}. Best is trial 0 with value: 8.879212308713894.\n","[I 2025-02-13 05:22:55,188] Trial 2 finished with value: 8.885974065938143 and parameters: {'n_estimators': 86, 'learning_rate': 0.19432674420569365, 'max_depth': 4, 'subsample': 0.7021216182630862, 'colsample_bytree': 0.507431011236223}. Best is trial 0 with value: 8.879212308713894.\n","[I 2025-02-13 05:22:55,474] Trial 3 finished with value: 8.851585830766302 and parameters: {'n_estimators': 448, 'learning_rate': 0.04286311555045328, 'max_depth': 8, 'subsample': 0.7141502255571612, 'colsample_bytree': 0.762370906106792}. Best is trial 3 with value: 8.851585830766302.\n","[I 2025-02-13 05:22:55,518] Trial 4 finished with value: 31.59471636640763 and parameters: {'n_estimators': 60, 'learning_rate': 0.01878849331743093, 'max_depth': 9, 'subsample': 0.5549489291369161, 'colsample_bytree': 0.889716712496165}. Best is trial 3 with value: 8.851585830766302.\n","[I 2025-02-13 05:22:55,590] Trial 5 finished with value: 28.91645550666628 and parameters: {'n_estimators': 111, 'learning_rate': 0.011430351812964716, 'max_depth': 4, 'subsample': 0.584032810990301, 'colsample_bytree': 0.6465504426854198}. Best is trial 3 with value: 8.851585830766302.\n","[I 2025-02-13 05:22:56,068] Trial 6 finished with value: 8.95708769702604 and parameters: {'n_estimators': 470, 'learning_rate': 0.014670532869776974, 'max_depth': 8, 'subsample': 0.8592037331989759, 'colsample_bytree': 0.9177612082262993}. Best is trial 3 with value: 8.851585830766302.\n","[I 2025-02-13 05:22:56,275] Trial 7 finished with value: 8.8319704525982 and parameters: {'n_estimators': 400, 'learning_rate': 0.041385804268992535, 'max_depth': 3, 'subsample': 0.5115272160614862, 'colsample_bytree': 0.78922257885683}. Best is trial 7 with value: 8.8319704525982.\n","[I 2025-02-13 05:22:56,341] Trial 8 finished with value: 14.038446600222287 and parameters: {'n_estimators': 112, 'learning_rate': 0.02583965504360978, 'max_depth': 3, 'subsample': 0.7829717090770125, 'colsample_bytree': 0.699341866246745}. Best is trial 7 with value: 8.8319704525982.\n","[I 2025-02-13 05:22:56,623] Trial 9 finished with value: 8.849447693649667 and parameters: {'n_estimators': 389, 'learning_rate': 0.06215080717117445, 'max_depth': 6, 'subsample': 0.9248453907716783, 'colsample_bytree': 0.6998665121421824}. Best is trial 7 with value: 8.8319704525982.\n","[I 2025-02-13 05:22:56,882] Trial 10 finished with value: 8.83556333490495 and parameters: {'n_estimators': 270, 'learning_rate': 0.08103740037126174, 'max_depth': 6, 'subsample': 0.5059584320895016, 'colsample_bytree': 0.8209336204082192}. Best is trial 7 with value: 8.8319704525982.\n","[I 2025-02-13 05:22:57,130] Trial 11 finished with value: 8.813402166265128 and parameters: {'n_estimators': 265, 'learning_rate': 0.07069986471753689, 'max_depth': 6, 'subsample': 0.5034855206996659, 'colsample_bytree': 0.8179152105647931}. Best is trial 11 with value: 8.813402166265128.\n","[I 2025-02-13 05:22:57,511] Trial 12 finished with value: 8.881177147987474 and parameters: {'n_estimators': 305, 'learning_rate': 0.03851092234568866, 'max_depth': 10, 'subsample': 0.6274969367020058, 'colsample_bytree': 0.9945535023050244}. Best is trial 11 with value: 8.813402166265128.\n","[I 2025-02-13 05:22:57,699] Trial 13 finished with value: 8.812773745852061 and parameters: {'n_estimators': 219, 'learning_rate': 0.09826901295532306, 'max_depth': 5, 'subsample': 0.5042389444049517, 'colsample_bytree': 0.8291903158037685}. Best is trial 13 with value: 8.812773745852061.\n","[I 2025-02-13 05:22:57,883] Trial 14 finished with value: 8.889111033468028 and parameters: {'n_estimators': 204, 'learning_rate': 0.10943761369106962, 'max_depth': 5, 'subsample': 0.6201269667183742, 'colsample_bytree': 0.8618292716069201}. Best is trial 13 with value: 8.812773745852061.\n","[I 2025-02-13 05:22:58,067] Trial 15 finished with value: 8.86123741701456 and parameters: {'n_estimators': 237, 'learning_rate': 0.1005488033445136, 'max_depth': 5, 'subsample': 0.9700185585312038, 'colsample_bytree': 0.965590493599816}. Best is trial 13 with value: 8.812773745852061.\n","[I 2025-02-13 05:22:58,278] Trial 16 finished with value: 8.83623996057477 and parameters: {'n_estimators': 173, 'learning_rate': 0.06810459788385055, 'max_depth': 7, 'subsample': 0.5750807040676191, 'colsample_bytree': 0.836618941459369}. Best is trial 13 with value: 8.812773745852061.\n","[I 2025-02-13 05:22:58,559] Trial 17 finished with value: 8.87167816639589 and parameters: {'n_estimators': 348, 'learning_rate': 0.1195078200966864, 'max_depth': 5, 'subsample': 0.5013569497180296, 'colsample_bytree': 0.712033375181414}. Best is trial 13 with value: 8.812773745852061.\n","[I 2025-02-13 05:22:58,834] Trial 18 finished with value: 8.855219310613242 and parameters: {'n_estimators': 260, 'learning_rate': 0.059348940626826524, 'max_depth': 6, 'subsample': 0.6271327435621298, 'colsample_bytree': 0.9207512985475614}. Best is trial 13 with value: 8.812773745852061.\n","[I 2025-02-13 05:22:58,967] Trial 19 finished with value: 9.70303410283111 and parameters: {'n_estimators': 153, 'learning_rate': 0.0306980817086251, 'max_depth': 8, 'subsample': 0.6623444713587665, 'colsample_bytree': 0.6493819968918011}. Best is trial 13 with value: 8.812773745852061.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Final Model Performance (GRU + XGBoost):\n","\n","Training:    MAE=0.0035, MSE=0.0000, RMSE=0.0054, R²=0.9998, MAPE=1.02%\n","Validation:  MAE=0.1570, MSE=0.0305, RMSE=0.1747, R²=-4.1882, MAPE=8.81%\n","Test:        MAE=0.4260, MSE=0.1875, RMSE=0.4330, R²=-30.0683, MAPE=21.01%\n","\n","Best GRU Parameters: {'num_layers': 4, 'hidden_size': 56, 'learning_rate': 0.00020713388255328886}\n","Best XGBoost Parameters: {'n_estimators': 219, 'learning_rate': 0.09826901295532306, 'max_depth': 5, 'subsample': 0.5042389444049517, 'colsample_bytree': 0.8291903158037685}\n"]}]},{"cell_type":"markdown","source":["### xgb bohb"],"metadata":{"id":"DVd0odr_NHIM"}},{"cell_type":"code","source":["!pip install hpbandster ConfigSpace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EMfCdE5VKEfy","executionInfo":{"status":"ok","timestamp":1739424282218,"user_tz":-330,"elapsed":15346,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"928fcd9a-621a-4793-f135-81d3c8a738da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from hpbandster) (1.13.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (3.2.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (10.5.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, ConfigSpace, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=7c2f3d66e39d20e6b1115a0f446be21564c18cab059ad7fb7c33b12837a77803\n","  Stored in directory: /root/.cache/pip/wheels/79/51/18/33d6ba8c55cc8401bffbccb1b87b21e0c68f40edc4ce3c1f99\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=ConfigSpace-1.2.1-py3-none-any.whl size=115943 sha256=ee9f755e884d5ac8a0d5b5cb4644b746ca216d80f157cb43f1274051c333dc02\n","  Stored in directory: /root/.cache/pip/wheels/75/e4/b7/23c23eb4a1c3b1adfeb8bd11366f48c805cbf3ba347237fea6\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp310-cp310-linux_x86_64.whl size=35008 sha256=3899cfe4ed1cd8a8b2b48283dd76775cd0ae42344714cb2ee37e393c227ddd02\n","  Stored in directory: /root/.cache/pip/wheels/48/65/b3/4c4cc6038b81ff21cc9df69f2b6774f5f52e23d3c275ed15aa\n","Successfully built hpbandster ConfigSpace netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, ConfigSpace, hpbandster\n","Successfully installed ConfigSpace-1.2.1 Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import xgboost as xgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# GRU Configurations (2, 3, and 5 layers)\n","gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store GRU feature representations\n","gru_features = {}\n","\n","for num_layers in gru_layers:\n","    print(f\"Training GRU with {num_layers} layers...\")\n","\n","    gru_model = GRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        gru_model.train()\n","        optimizer.zero_grad()\n","        outputs = gru_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Extract Feature Representations\n","    gru_model.eval()\n","    with torch.no_grad():\n","        train_features = gru_model(X_train_torch).numpy()\n","        val_features = gru_model(X_val_torch).numpy()\n","        test_features = gru_model(X_test_torch).numpy()\n","\n","    gru_features[num_layers] = (train_features, val_features, test_features)\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"subsample\", 0.5, 1.0, default_value=0.8))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"colsample_bytree\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for XGBoost\n","class XGBoostWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = xgb.XGBRegressor(\n","            n_estimators=config[\"n_estimators\"],\n","            learning_rate=config[\"learning_rate\"],\n","            max_depth=config[\"max_depth\"],\n","            subsample=config[\"subsample\"],\n","            colsample_bytree=config[\"colsample_bytree\"],\n","            random_state=42\n","        )\n","        model.fit(self.train_features, Y_train)\n","        Y_val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB for each GRU configuration\n","best_models = {}\n","\n","for num_layers in gru_layers:\n","    print(f\"\\nRunning BOHB for GRU ({num_layers} layers) + XGBoost...\")\n","\n","    train_features, val_features, test_features = gru_features[num_layers]\n","\n","    # Start NameServer\n","    NS = hpns.NameServer(run_id=f\"gru_{num_layers}_xgb_bohb\", host=\"127.0.0.1\", port=None)\n","    NS.start()\n","\n","    worker = XGBoostWorker(\n","        train_features=train_features,\n","        val_features=val_features,\n","        nameserver=\"127.0.0.1\",\n","        run_id=f\"gru_{num_layers}_xgb_bohb\"\n","    )\n","    worker.run(background=True)\n","\n","    bohb = BOHB(\n","        configspace=get_config_space(),\n","        run_id=f\"gru_{num_layers}_xgb_bohb\",\n","        nameserver=\"127.0.0.1\",\n","        min_budget=1,\n","        max_budget=3\n","    )\n","\n","    res = bohb.run(n_iterations=50)\n","\n","    # Shutdown BOHB\n","    bohb.shutdown()\n","    NS.shutdown()\n","\n","    # Retrieve Best Configuration\n","    best_config = res.get_incumbent_id()\n","    best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","    # Train Best XGB Model on GRU Features\n","    best_xgb_model = xgb.XGBRegressor(\n","        n_estimators=best_params[\"n_estimators\"],\n","        learning_rate=best_params[\"learning_rate\"],\n","        max_depth=best_params[\"max_depth\"],\n","        subsample=best_params[\"subsample\"],\n","        colsample_bytree=best_params[\"colsample_bytree\"],\n","        random_state=42\n","    )\n","\n","    best_xgb_model.fit(train_features, Y_train)\n","\n","    # Make Predictions\n","    Y_train_pred = best_xgb_model.predict(train_features)\n","    Y_val_pred = best_xgb_model.predict(val_features)\n","    Y_test_pred = best_xgb_model.predict(test_features)\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store best model and metrics\n","    best_models[num_layers] = {\n","        \"params\": best_params,\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics\n","    }\n","\n","    # Print Results\n","    print(f\"\\nBest Parameters for GRU ({num_layers} layers) + XGBoost:\")\n","    print(best_params)\n","\n","    print(\"\\nTraining set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"\\nValidation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"\\nTest set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpOdWrsJxg-S","executionInfo":{"status":"ok","timestamp":1739424555594,"user_tz":-330,"elapsed":146342,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"c7b45c78-4a04-4980-fef3-c6b654104ee2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training GRU with 2 layers...\n","Training GRU with 3 layers...\n","Training GRU with 5 layers...\n","\n","Running BOHB for GRU (2 layers) + XGBoost...\n","\n","Best Parameters for GRU (2 layers) + XGBoost:\n","{'colsample_bytree': 0.616514951443, 'learning_rate': 0.2351215327059, 'max_depth': 9, 'n_estimators': 268, 'subsample': 0.5158514159354}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0055, R²: 0.9998, MAPE: 1.03%\n","\n","Validation set metrics:\n","MAE: 0.1569, MSE: 0.0305, RMSE: 0.1746, R²: -4.1838, MAPE: 8.81%\n","\n","Test set metrics:\n","MAE: 0.4259, MSE: 0.1874, RMSE: 0.4329, R²: -30.0568, MAPE: 21.01%\n","\n","Running BOHB for GRU (3 layers) + XGBoost...\n","\n","Best Parameters for GRU (3 layers) + XGBoost:\n","{'colsample_bytree': 0.5077275834783, 'learning_rate': 0.2281085151797, 'max_depth': 10, 'n_estimators': 97, 'subsample': 0.5234115229348}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0055, R²: 0.9998, MAPE: 1.03%\n","\n","Validation set metrics:\n","MAE: 0.1567, MSE: 0.0304, RMSE: 0.1744, R²: -4.1705, MAPE: 8.79%\n","\n","Test set metrics:\n","MAE: 0.4257, MSE: 0.1872, RMSE: 0.4327, R²: -30.0215, MAPE: 21.00%\n","\n","Running BOHB for GRU (5 layers) + XGBoost...\n","\n","Best Parameters for GRU (5 layers) + XGBoost:\n","{'colsample_bytree': 0.808099498148, 'learning_rate': 0.2334328648543, 'max_depth': 4, 'n_estimators': 347, 'subsample': 0.5058497867929}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0056, R²: 0.9998, MAPE: 1.07%\n","\n","Validation set metrics:\n","MAE: 0.1563, MSE: 0.0303, RMSE: 0.1740, R²: -4.1485, MAPE: 8.77%\n","\n","Test set metrics:\n","MAE: 0.4252, MSE: 0.1869, RMSE: 0.4323, R²: -29.9633, MAPE: 20.98%\n"]}]},{"cell_type":"markdown","source":["## GRU - catboost"],"metadata":{"id":"nW9Vq7ibPQQf"}},{"cell_type":"markdown","source":["### initial"],"metadata":{"id":"mrXDDEWZQrDj"}},{"cell_type":"code","source":["!pip install catboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"39V7oh1sPS0c","executionInfo":{"status":"ok","timestamp":1739424657987,"user_tz":-330,"elapsed":8346,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"bfcd3520-c46e-4929-ee56-ad15b7c90d9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n","Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.7\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h0)\n","        return self.fc(out[:, -1, :])\n","\n","# Set Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# DataFrame to store results\n","columns = [\"Layers\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\"]\n","results_df = pd.DataFrame(columns=columns)\n","\n","# Train multiple GRU models\n","gru_outputs = {}\n","\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining GRU with {num_layers} layers...\")\n","\n","    model = GRUModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        train_pred = model(X_train_torch).cpu().numpy()\n","        val_pred = model(X_val_torch).cpu().numpy()\n","        test_pred = model(X_test_torch).cpu().numpy()\n","\n","    # Inverse transform predictions\n","    train_pred_actual = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    test_pred_actual = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","    # Compute metrics for each dataset\n","    metrics_train = compute_metrics(Y_train.values.flatten(), train_pred_actual.flatten())\n","    metrics_val = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","    metrics_test = compute_metrics(Y_test.values.flatten(), test_pred_actual.flatten())\n","\n","    # Append results to DataFrame\n","    results_df = pd.concat([\n","        results_df,\n","        pd.DataFrame([\n","            [num_layers, \"Train\", *metrics_train],\n","            [num_layers, \"Validation\", *metrics_val],\n","            [num_layers, \"Test\", *metrics_test]\n","        ], columns=columns)\n","    ], ignore_index=True)\n","\n","    # Store GRU embeddings for CatBoost\n","    gru_outputs[num_layers] = {\n","        \"train\": train_pred_actual,\n","        \"val\": val_pred_actual,\n","        \"test\": test_pred_actual\n","    }\n","\n","# Find the best GRU model (Lowest Validation MAPE)\n","best_model = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","best_layers = int(best_model[\"Layers\"])\n","\n","# Display GRU Results\n","print(\"\\nGRU Model Performance Comparison (2, 3, and 5 Layers)\\n\")\n","print(results_df.to_string(index=False))\n","\n","print(f\"\\nBest GRU Model: {best_layers} Layers (Based on Lowest Validation MAPE)\\n\")\n","\n","# ---------- CatBoost on Best GRU Embeddings ----------\n","print(\"\\nTraining CatBoost on Best GRU Embeddings...\")\n","\n","# Use the best GRU's output as features for CatBoost\n","X_train_cat = gru_outputs[best_layers][\"train\"]\n","X_val_cat = gru_outputs[best_layers][\"val\"]\n","X_test_cat = gru_outputs[best_layers][\"test\"]\n","\n","# CatBoost Model\n","cat_model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, loss_function=\"RMSE\", verbose=100)\n","cat_model.fit(X_train_cat, Y_train.values, eval_set=(X_val_cat, Y_val.values), early_stopping_rounds=100)\n","\n","# Predictions\n","train_pred_cat = cat_model.predict(X_train_cat)\n","val_pred_cat = cat_model.predict(X_val_cat)\n","test_pred_cat = cat_model.predict(X_test_cat)\n","\n","# Compute metrics for CatBoost\n","metrics_train_cat = compute_metrics(Y_train.values.flatten(), train_pred_cat.flatten())\n","metrics_val_cat = compute_metrics(Y_val.values.flatten(), val_pred_cat.flatten())\n","metrics_test_cat = compute_metrics(Y_test.values.flatten(), test_pred_cat.flatten())\n","\n","# Append CatBoost results to DataFrame\n","results_df = pd.concat([\n","    results_df,\n","    pd.DataFrame([\n","        [f\"GRU({best_layers}) + CatBoost\", \"Train\", *metrics_train_cat],\n","        [f\"GRU({best_layers}) + CatBoost\", \"Validation\", *metrics_val_cat],\n","        [f\"GRU({best_layers}) + CatBoost\", \"Test\", *metrics_test_cat]\n","    ], columns=columns)\n","], ignore_index=True)\n","\n","# Display Final Results\n","print(\"\\nFinal Model Performance (GRU vs GRU + CatBoost)\\n\")\n","print(results_df.to_string(index=False))\n","\n","# Best Model Selection\n","best_overall = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","print(f\"\\nBest Overall Model: {best_overall['Layers']} (Based on Lowest Validation MAPE)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYlgkIo5zYqs","executionInfo":{"status":"ok","timestamp":1739424913329,"user_tz":-330,"elapsed":8442,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"59bf6fc0-d54d-420d-fcec-7e9cc632c538"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training GRU with 2 layers...\n","Epoch [10/100], Loss: 0.0899\n","Epoch [20/100], Loss: 0.0746\n","Epoch [30/100], Loss: 0.0731\n","Epoch [40/100], Loss: 0.0719\n","Epoch [50/100], Loss: 0.0716\n","Epoch [60/100], Loss: 0.0716\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training GRU with 3 layers...\n","Epoch [10/100], Loss: 0.0729\n","Epoch [20/100], Loss: 0.0732\n","Epoch [30/100], Loss: 0.0716\n","Epoch [40/100], Loss: 0.0716\n","Epoch [50/100], Loss: 0.0715\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training GRU with 5 layers...\n","Epoch [10/100], Loss: 0.0770\n","Epoch [20/100], Loss: 0.0742\n","Epoch [30/100], Loss: 0.0716\n","Epoch [40/100], Loss: 0.0717\n","Epoch [50/100], Loss: 0.0715\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","GRU Model Performance Comparison (2, 3, and 5 Layers)\n","\n","Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","     2      Train 0.326843 0.174467 0.417692    0.003166 124.564755\n","     2 Validation 1.292852 1.677371 1.295133 -284.192280  73.974025\n","     2       Test 1.562404 2.447166 1.564342 -404.490862  77.483181\n","     3      Train 0.327504 0.174942 0.418261    0.000450 124.952909\n","     3 Validation 1.292901 1.677479 1.295175 -284.210755  73.977114\n","     3       Test 1.562101 2.446203 1.564034 -404.331310  77.468323\n","     5      Train 0.327072 0.174712 0.417986    0.001766 124.703659\n","     5 Validation 1.292322 1.675970 1.294593 -283.954094  73.944150\n","     5       Test 1.561173 2.443288 1.563102 -403.848318  77.422413\n","\n","Best GRU Model: 5 Layers (Based on Lowest Validation MAPE)\n","\n","\n","Training CatBoost on Best GRU Embeddings...\n","0:\tlearn: 0.3980528\ttest: 1.2474579\tbest: 1.2474579 (0)\ttotal: 47.6ms\tremaining: 47.5s\n","100:\tlearn: 0.0086793\ttest: 0.1987533\tbest: 0.1987533 (100)\ttotal: 174ms\tremaining: 1.55s\n","200:\tlearn: 0.0065292\ttest: 0.1773565\tbest: 0.1773565 (200)\ttotal: 294ms\tremaining: 1.17s\n","300:\tlearn: 0.0060343\ttest: 0.1760661\tbest: 0.1760661 (300)\ttotal: 418ms\tremaining: 970ms\n","400:\tlearn: 0.0058515\ttest: 0.1756422\tbest: 0.1756422 (400)\ttotal: 553ms\tremaining: 825ms\n","500:\tlearn: 0.0057531\ttest: 0.1754522\tbest: 0.1754451 (494)\ttotal: 681ms\tremaining: 679ms\n","600:\tlearn: 0.0056959\ttest: 0.1753222\tbest: 0.1753222 (600)\ttotal: 804ms\tremaining: 533ms\n","700:\tlearn: 0.0056572\ttest: 0.1752244\tbest: 0.1752244 (700)\ttotal: 927ms\tremaining: 395ms\n","800:\tlearn: 0.0056318\ttest: 0.1751770\tbest: 0.1751770 (800)\ttotal: 1.05s\tremaining: 262ms\n","900:\tlearn: 0.0056169\ttest: 0.1751484\tbest: 0.1751437 (872)\ttotal: 1.18s\tremaining: 130ms\n","999:\tlearn: 0.0056055\ttest: 0.1751280\tbest: 0.1751194 (967)\ttotal: 1.31s\tremaining: 0us\n","\n","bestTest = 0.1751193886\n","bestIteration = 967\n","\n","Shrink model to first 968 iterations.\n","\n","Final Model Performance (GRU vs GRU + CatBoost)\n","\n","           Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","                2      Train 0.326843 0.174467 0.417692    0.003166 124.564755\n","                2 Validation 1.292852 1.677371 1.295133 -284.192280  73.974025\n","                2       Test 1.562404 2.447166 1.564342 -404.490862  77.483181\n","                3      Train 0.327504 0.174942 0.418261    0.000450 124.952909\n","                3 Validation 1.292901 1.677479 1.295175 -284.210755  73.977114\n","                3       Test 1.562101 2.446203 1.564034 -404.331310  77.468323\n","                5      Train 0.327072 0.174712 0.417986    0.001766 124.703659\n","                5 Validation 1.292322 1.675970 1.294593 -283.954094  73.944150\n","                5       Test 1.561173 2.443288 1.563102 -403.848318  77.422413\n","GRU(5) + CatBoost      Train 0.003711 0.000031 0.005609    0.999820   1.071461\n","GRU(5) + CatBoost Validation 0.157495 0.030667 0.175119   -4.214074   8.840230\n","GRU(5) + CatBoost       Test 0.426470 0.187912 0.433488  -30.136687  21.038448\n","\n","Best Overall Model: GRU(5) + CatBoost (Based on Lowest Validation MAPE)\n"]}]},{"cell_type":"markdown","source":["### optuna"],"metadata":{"id":"0XWZweeUQs5E"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","import optuna\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h0)\n","        return self.fc(out[:, -1, :])\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Optuna objective function\n","def objective(trial):\n","    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128])\n","    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n","    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n","\n","    model = GRUModel(input_size=3, hidden_size=hidden_size, num_layers=num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(50):  # Reduce epochs for faster tuning\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    with torch.no_grad():\n","        val_pred = model(X_val_torch).cpu().numpy()\n","\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    _, _, _, _, val_mape = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","\n","    return val_mape\n","\n","# Run Optuna optimization\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=20)\n","\n","# Get best hyperparameters\n","best_params = study.best_params\n","print(f\"\\nBest GRU Hyperparameters: {best_params}\")\n","\n","# Train Best GRU Model\n","best_gru = GRUModel(input_size=3, hidden_size=best_params[\"hidden_size\"], num_layers=best_params[\"num_layers\"]).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(best_gru.parameters(), lr=best_params[\"learning_rate\"])\n","\n","for epoch in range(100):\n","    best_gru.train()\n","    outputs = best_gru(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","# Evaluate Best GRU Model\n","best_gru.eval()\n","with torch.no_grad():\n","    train_pred = best_gru(X_train_torch).cpu().numpy()\n","    val_pred = best_gru(X_val_torch).cpu().numpy()\n","    test_pred = best_gru(X_test_torch).cpu().numpy()\n","\n","# Inverse transform predictions\n","train_pred_actual = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","test_pred_actual = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","# Compute metrics\n","metrics_train = compute_metrics(Y_train.values.flatten(), train_pred_actual.flatten())\n","metrics_val = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","metrics_test = compute_metrics(Y_test.values.flatten(), test_pred_actual.flatten())\n","\n","# Store GRU embeddings\n","gru_outputs = {\n","    \"train\": train_pred_actual,\n","    \"val\": val_pred_actual,\n","    \"test\": test_pred_actual\n","}\n","\n","# Display GRU Results\n","print(\"\\nGRU Model Performance (Best Hyperparameters):\\n\")\n","print(f\"Train: {metrics_train}\\nValidation: {metrics_val}\\nTest: {metrics_test}\")\n","\n","# ---------- CatBoost on Best GRU Embeddings ----------\n","print(\"\\nTraining CatBoost on Best GRU Embeddings...\")\n","\n","# Use GRU embeddings as input for CatBoost\n","X_train_cat = gru_outputs[\"train\"]\n","X_val_cat = gru_outputs[\"val\"]\n","X_test_cat = gru_outputs[\"test\"]\n","\n","# CatBoost Model\n","cat_model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, loss_function=\"RMSE\", verbose=100)\n","cat_model.fit(X_train_cat, Y_train.values, eval_set=(X_val_cat, Y_val.values), early_stopping_rounds=100)\n","\n","# Predictions\n","train_pred_cat = cat_model.predict(X_train_cat)\n","val_pred_cat = cat_model.predict(X_val_cat)\n","test_pred_cat = cat_model.predict(X_test_cat)\n","\n","# Compute metrics for CatBoost\n","metrics_train_cat = compute_metrics(Y_train.values.flatten(), train_pred_cat.flatten())\n","metrics_val_cat = compute_metrics(Y_val.values.flatten(), val_pred_cat.flatten())\n","metrics_test_cat = compute_metrics(Y_test.values.flatten(), test_pred_cat.flatten())\n","\n","# Display Final Results\n","print(\"\\nFinal Model Performance (GRU vs GRU + CatBoost)\\n\")\n","print(f\"Train: {metrics_train_cat}\\nValidation: {metrics_val_cat}\\nTest: {metrics_test_cat}\")\n","\n","# Best Model Selection\n","best_model = \"GRU + CatBoost\" if metrics_val_cat[4] < metrics_val[4] else \"GRU Only\"\n","print(f\"\\nBest Overall Model: {best_model} (Based on Lowest Validation MAPE)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3GpRHR00Taq","executionInfo":{"status":"ok","timestamp":1739425193902,"user_tz":-330,"elapsed":55991,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"9c6cc94e-7d41-4cf3-ee3e-d970f14f7ac8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 05:39:00,191] A new study created in memory with name: no-name-705da87e-27e6-4ac4-98cb-c90333f0609c\n","[I 2025-02-13 05:39:03,948] Trial 0 finished with value: 75.25414628370612 and parameters: {'hidden_size': 128, 'num_layers': 4, 'learning_rate': 0.00043776742528689987}. Best is trial 0 with value: 75.25414628370612.\n","[I 2025-02-13 05:39:04,379] Trial 1 finished with value: 71.58392829532345 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.0003291705614879124}. Best is trial 1 with value: 71.58392829532345.\n","[I 2025-02-13 05:39:05,868] Trial 2 finished with value: 73.45893080145923 and parameters: {'hidden_size': 64, 'num_layers': 5, 'learning_rate': 0.0009869375397354883}. Best is trial 1 with value: 71.58392829532345.\n","[I 2025-02-13 05:39:06,422] Trial 3 finished with value: 73.42811734123924 and parameters: {'hidden_size': 32, 'num_layers': 4, 'learning_rate': 0.0008073318783337196}. Best is trial 1 with value: 71.58392829532345.\n","[I 2025-02-13 05:39:06,843] Trial 4 finished with value: 74.61575748469343 and parameters: {'hidden_size': 32, 'num_layers': 2, 'learning_rate': 0.002812394709700696}. Best is trial 1 with value: 71.58392829532345.\n","[I 2025-02-13 05:39:09,590] Trial 5 finished with value: 73.31773525922031 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.0007474763261293842}. Best is trial 1 with value: 71.58392829532345.\n","[I 2025-02-13 05:39:10,323] Trial 6 finished with value: 73.02897887439656 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.001138538516979496}. Best is trial 1 with value: 71.58392829532345.\n","[I 2025-02-13 05:39:10,951] Trial 7 finished with value: 79.93710669861079 and parameters: {'hidden_size': 32, 'num_layers': 5, 'learning_rate': 0.00012897838405819185}. Best is trial 1 with value: 71.58392829532345.\n","[I 2025-02-13 05:39:14,531] Trial 8 finished with value: 67.88875780668091 and parameters: {'hidden_size': 128, 'num_layers': 4, 'learning_rate': 0.00017896288657616265}. Best is trial 8 with value: 67.88875780668091.\n","[I 2025-02-13 05:39:15,251] Trial 9 finished with value: 77.26537618957565 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0002700927835167638}. Best is trial 8 with value: 67.88875780668091.\n","[I 2025-02-13 05:39:18,023] Trial 10 finished with value: 73.78455217776643 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.007907369756770139}. Best is trial 8 with value: 67.88875780668091.\n","[I 2025-02-13 05:39:18,585] Trial 11 finished with value: 83.5353946435856 and parameters: {'hidden_size': 32, 'num_layers': 4, 'learning_rate': 0.00010632343647007451}. Best is trial 8 with value: 67.88875780668091.\n","[I 2025-02-13 05:39:21,369] Trial 12 finished with value: 65.02444220804819 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.00029492397206169177}. Best is trial 12 with value: 65.02444220804819.\n","[I 2025-02-13 05:39:24,175] Trial 13 finished with value: 75.24788412239077 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.00017578609022204656}. Best is trial 12 with value: 65.02444220804819.\n","[I 2025-02-13 05:39:27,818] Trial 14 finished with value: 63.546448953517654 and parameters: {'hidden_size': 128, 'num_layers': 4, 'learning_rate': 0.00022159920806409506}. Best is trial 14 with value: 63.546448953517654.\n","[I 2025-02-13 05:39:30,639] Trial 15 finished with value: 74.51980446042268 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.00047507605591810614}. Best is trial 14 with value: 63.546448953517654.\n","[I 2025-02-13 05:39:35,140] Trial 16 finished with value: 74.28521188421458 and parameters: {'hidden_size': 128, 'num_layers': 5, 'learning_rate': 0.0026024761198320517}. Best is trial 14 with value: 63.546448953517654.\n","[I 2025-02-13 05:39:38,832] Trial 17 finished with value: 64.72612585087283 and parameters: {'hidden_size': 128, 'num_layers': 4, 'learning_rate': 0.00023573721184784986}. Best is trial 14 with value: 63.546448953517654.\n","[I 2025-02-13 05:39:42,506] Trial 18 finished with value: 73.82057888897309 and parameters: {'hidden_size': 128, 'num_layers': 4, 'learning_rate': 0.0019649806718961466}. Best is trial 14 with value: 63.546448953517654.\n","[I 2025-02-13 05:39:47,077] Trial 19 finished with value: 67.87823130752518 and parameters: {'hidden_size': 128, 'num_layers': 5, 'learning_rate': 0.00019195483650826352}. Best is trial 14 with value: 63.546448953517654.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Best GRU Hyperparameters: {'hidden_size': 128, 'num_layers': 4, 'learning_rate': 0.00022159920806409506}\n","\n","GRU Model Performance (Best Hyperparameters):\n","\n","Train: (0.3253094109411231, 0.17286601109499716, 0.4157715852424227, 0.01231278033712535, 124.0138336851776)\n","Validation: (1.2859226419819814, 1.6594217853177897, 1.2881854623142546, -281.1405305423759, 73.57781479240901)\n","Test: (1.5537205315550298, 2.4200290598228467, 1.555644258763181, -399.9943385967086, 77.05267364610829)\n","\n","Training CatBoost on Best GRU Embeddings...\n","0:\tlearn: 0.3980547\ttest: 1.2474612\tbest: 1.2474612 (0)\ttotal: 1.65ms\tremaining: 1.64s\n","100:\tlearn: 0.0093264\ttest: 0.1989873\tbest: 0.1989873 (100)\ttotal: 124ms\tremaining: 1.1s\n","200:\tlearn: 0.0073850\ttest: 0.1775091\tbest: 0.1775091 (200)\ttotal: 247ms\tremaining: 981ms\n","300:\tlearn: 0.0069603\ttest: 0.1762935\tbest: 0.1762873 (299)\ttotal: 367ms\tremaining: 851ms\n","400:\tlearn: 0.0067976\ttest: 0.1757961\tbest: 0.1757961 (400)\ttotal: 491ms\tremaining: 733ms\n","500:\tlearn: 0.0066976\ttest: 0.1756314\tbest: 0.1756308 (499)\ttotal: 616ms\tremaining: 614ms\n","600:\tlearn: 0.0066403\ttest: 0.1755017\tbest: 0.1755017 (600)\ttotal: 738ms\tremaining: 490ms\n","700:\tlearn: 0.0066044\ttest: 0.1754018\tbest: 0.1754018 (700)\ttotal: 873ms\tremaining: 372ms\n","800:\tlearn: 0.0065794\ttest: 0.1753740\tbest: 0.1753734 (796)\ttotal: 1s\tremaining: 249ms\n","900:\tlearn: 0.0065670\ttest: 0.1753191\tbest: 0.1753156 (888)\ttotal: 1.13s\tremaining: 124ms\n","999:\tlearn: 0.0065564\ttest: 0.1752647\tbest: 0.1752647 (999)\ttotal: 1.25s\tremaining: 0us\n","\n","bestTest = 0.1752646943\n","bestIteration = 999\n","\n","\n","Final Model Performance (GRU vs GRU + CatBoost)\n","\n","Train: (0.004409946684561705, 4.298658487470994e-05, 0.006556415550795262, 0.9997543918539639, 1.3045029086811144)\n","Validation: (0.15765537715871633, 0.030717713159801555, 0.17526469456168733, -4.222729968134776, 8.84940352660724)\n","Test: (0.42663204065582533, 0.1880499685051519, 0.4336472858270324, -30.159531922883307, 21.046478553813632)\n","\n","Best Overall Model: GRU + CatBoost (Based on Lowest Validation MAPE)\n"]}]},{"cell_type":"markdown","source":["### bohb\n"],"metadata":{"id":"11fjBsEN1A98"}},{"cell_type":"code","source":["import numpy as np\n","import catboost as cb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# GRU Configurations (2, 3, and 5 layers)\n","gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store GRU feature representations\n","gru_features = {}\n","\n","for num_layers in gru_layers:\n","    print(f\"Training GRU with {num_layers} layers...\")\n","\n","    gru_model = GRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        gru_model.train()\n","        optimizer.zero_grad()\n","        outputs = gru_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Extract Feature Representations\n","    gru_model.eval()\n","    with torch.no_grad():\n","        train_features = gru_model(X_train_torch).numpy()\n","        val_features = gru_model(X_val_torch).numpy()\n","        test_features = gru_model(X_test_torch).numpy()\n","\n","    gru_features[num_layers] = (train_features, val_features, test_features)\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"iterations\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"bagging_temperature\", 0.0, 1.0, default_value=0.8))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"colsample_bylevel\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for CatBoost\n","class CatBoostWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = cb.CatBoostRegressor(\n","            iterations=config[\"iterations\"],\n","            learning_rate=config[\"learning_rate\"],\n","            depth=config[\"depth\"],\n","            bagging_temperature=config[\"bagging_temperature\"],\n","            colsample_bylevel=config[\"colsample_bylevel\"],\n","            random_seed=42,\n","            verbose=0\n","        )\n","        model.fit(self.train_features, Y_train)\n","        Y_val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB for each GRU configuration\n","best_models = {}\n","\n","for num_layers in gru_layers:\n","    print(f\"\\nRunning BOHB for GRU ({num_layers} layers) + CatBoost...\")\n","\n","    train_features, val_features, test_features = gru_features[num_layers]\n","\n","    # Start NameServer\n","    NS = hpns.NameServer(run_id=f\"gru_{num_layers}_catboost_bohb\", host=\"127.0.0.1\", port=None)\n","    NS.start()\n","\n","    worker = CatBoostWorker(\n","        train_features=train_features,\n","        val_features=val_features,\n","        nameserver=\"127.0.0.1\",\n","        run_id=f\"gru_{num_layers}_catboost_bohb\"\n","    )\n","    worker.run(background=True)\n","\n","    bohb = BOHB(\n","        configspace=get_config_space(),\n","        run_id=f\"gru_{num_layers}_catboost_bohb\",\n","        nameserver=\"127.0.0.1\",\n","        min_budget=1,\n","        max_budget=3\n","    )\n","\n","    res = bohb.run(n_iterations=50)\n","\n","    # Shutdown BOHB\n","    bohb.shutdown()\n","    NS.shutdown()\n","\n","    # Retrieve Best Configuration\n","    best_config = res.get_incumbent_id()\n","    best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","    # Train Best CatBoost Model on GRU Features\n","    best_catboost_model = cb.CatBoostRegressor(\n","        iterations=best_params[\"iterations\"],\n","        learning_rate=best_params[\"learning_rate\"],\n","        depth=best_params[\"depth\"],\n","        bagging_temperature=best_params[\"bagging_temperature\"],\n","        colsample_bylevel=best_params[\"colsample_bylevel\"],\n","        random_seed=42,\n","        verbose=0\n","    )\n","\n","    best_catboost_model.fit(train_features, Y_train)\n","\n","    # Make Predictions\n","    Y_train_pred = best_catboost_model.predict(train_features)\n","    Y_val_pred = best_catboost_model.predict(val_features)\n","    Y_test_pred = best_catboost_model.predict(test_features)\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store best model and metrics\n","    best_models[num_layers] = {\n","        \"params\": best_params,\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics\n","    }\n","\n","    # Print Results\n","    print(f\"\\nBest Parameters for GRU ({num_layers} layers) + CatBoost:\")\n","    print(best_params)\n","\n","    print(\"\\nTraining set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"\\nValidation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"\\nTest set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K01t5TlEb4dS","outputId":"4fcb2109-a6d0-475f-86d2-2bbf2322e5b7","executionInfo":{"status":"ok","timestamp":1739437250664,"user_tz":-330,"elapsed":592248,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training GRU with 2 layers...\n","Training GRU with 3 layers...\n","Training GRU with 5 layers...\n","\n","Running BOHB for GRU (2 layers) + CatBoost...\n","\n","Best Parameters for GRU (2 layers) + CatBoost:\n","{'bagging_temperature': 0.0685761918533, 'colsample_bylevel': 0.983520067695, 'depth': 9, 'iterations': 496, 'learning_rate': 0.2875706042572}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0055, R²: 0.9998, MAPE: 1.03%\n","\n","Validation set metrics:\n","MAE: 0.1575, MSE: 0.0307, RMSE: 0.1751, R²: -4.2144, MAPE: 8.84%\n","\n","Test set metrics:\n","MAE: 0.4265, MSE: 0.1879, RMSE: 0.4335, R²: -30.1376, MAPE: 21.04%\n","\n","Running BOHB for GRU (3 layers) + CatBoost...\n","\n","Best Parameters for GRU (3 layers) + CatBoost:\n","{'bagging_temperature': 0.3787428249561, 'colsample_bylevel': 0.9942424487158, 'depth': 8, 'iterations': 416, 'learning_rate': 0.1835957608924}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0055, R²: 0.9998, MAPE: 1.05%\n","\n","Validation set metrics:\n","MAE: 0.1575, MSE: 0.0307, RMSE: 0.1752, R²: -4.2160, MAPE: 8.84%\n","\n","Test set metrics:\n","MAE: 0.4265, MSE: 0.1879, RMSE: 0.4335, R²: -30.1418, MAPE: 21.04%\n","\n","Running BOHB for GRU (5 layers) + CatBoost...\n","\n","Best Parameters for GRU (5 layers) + CatBoost:\n","{'bagging_temperature': 0.7506602787068, 'colsample_bylevel': 0.9860977117869, 'depth': 9, 'iterations': 475, 'learning_rate': 0.2958145229455}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0055, R²: 0.9998, MAPE: 1.04%\n","\n","Validation set metrics:\n","MAE: 0.1575, MSE: 0.0307, RMSE: 0.1751, R²: -4.2145, MAPE: 8.84%\n","\n","Test set metrics:\n","MAE: 0.4265, MSE: 0.1879, RMSE: 0.4335, R²: -30.1379, MAPE: 21.04%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"J-Y8TKPNdV8r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GRU - lightboost"],"metadata":{"id":"lXDdsg6VZm8f"}},{"cell_type":"code","source":["!pip install lightgbm"],"metadata":{"id":"NBEoXkf5aT4p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### initial"],"metadata":{"id":"AoMJJT3BqcII"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import lightgbm as lgb\n","import pandas as pd\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h0)\n","        return self.fc(out[:, -1, :])  # Take last time step output\n","\n","# Set Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]  # Different GRU layers\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100  # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# DataFrame to store results\n","columns = [\"Layers\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\"]\n","results_df = pd.DataFrame(columns=columns)\n","\n","# Train multiple GRU models\n","gru_outputs = {}  # Store GRU embeddings for LGBM\n","\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining GRU with {num_layers} layers...\")\n","\n","    # Initialize model, loss function, and optimizer\n","    model = GRUModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        train_pred = model(X_train_torch).cpu().numpy()\n","        val_pred = model(X_val_torch).cpu().numpy()\n","        test_pred = model(X_test_torch).cpu().numpy()\n","\n","    # Inverse transform predictions\n","    train_pred_actual = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","    test_pred_actual = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","    # Compute metrics for each dataset\n","    metrics_train = compute_metrics(Y_train.values.flatten(), train_pred_actual.flatten())\n","    metrics_val = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","    metrics_test = compute_metrics(Y_test.values.flatten(), test_pred_actual.flatten())\n","\n","    # Append results to DataFrame\n","    results_df = pd.concat([\n","        results_df,\n","        pd.DataFrame([\n","            [num_layers, \"Train\", *metrics_train],\n","            [num_layers, \"Validation\", *metrics_val],\n","            [num_layers, \"Test\", *metrics_test]\n","        ], columns=columns)\n","    ], ignore_index=True)\n","\n","    # Store GRU embeddings for LGBM\n","    gru_outputs[num_layers] = {\n","        \"train\": train_pred_actual,\n","        \"val\": val_pred_actual,\n","        \"test\": test_pred_actual\n","    }\n","\n","# Find the best GRU model (Lowest Validation MAPE)\n","best_model = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","best_layers = int(best_model[\"Layers\"])\n","\n","# Display GRU Results\n","print(\"\\nGRU Model Performance Comparison (2, 3, and 5 Layers)\\n\")\n","print(results_df.to_string(index=False))\n","\n","print(f\"\\nBest GRU Model: {best_layers} Layers (Based on Lowest Validation MAPE)\\n\")\n","\n","# ---------- LightGBM on Best GRU Embeddings ----------\n","print(\"\\nTraining LightGBM on Best GRU Embeddings...\")\n","\n","# Use the best GRU's output as features for LGBM\n","X_train_lgb = gru_outputs[best_layers][\"train\"]\n","X_val_lgb = gru_outputs[best_layers][\"val\"]\n","X_test_lgb = gru_outputs[best_layers][\"test\"]\n","\n","# LightGBM Dataset\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","\n","# LGBM Parameters\n","lgb_params = {\n","    \"objective\": \"regression\",\n","    \"metric\": \"rmse\",\n","    \"boosting_type\": \"gbdt\",\n","    \"learning_rate\": 0.05,\n","    \"num_leaves\": 31\n","}\n","\n","# Train LGBM\n","lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","\n","# Predictions\n","train_pred_lgb = lgb_model.predict(X_train_lgb)\n","val_pred_lgb = lgb_model.predict(X_val_lgb)\n","test_pred_lgb = lgb_model.predict(X_test_lgb)\n","\n","# Compute metrics for LGBM\n","metrics_train_lgb = compute_metrics(Y_train.values.flatten(), train_pred_lgb.flatten())\n","metrics_val_lgb = compute_metrics(Y_val.values.flatten(), val_pred_lgb.flatten())\n","metrics_test_lgb = compute_metrics(Y_test.values.flatten(), test_pred_lgb.flatten())\n","\n","# Append LGBM results to DataFrame\n","results_df = pd.concat([\n","    results_df,\n","    pd.DataFrame([\n","        [f\"GRU({best_layers}) + LGBM\", \"Train\", *metrics_train_lgb],\n","        [f\"GRU({best_layers}) + LGBM\", \"Validation\", *metrics_val_lgb],\n","        [f\"GRU({best_layers}) + LGBM\", \"Test\", *metrics_test_lgb]\n","    ], columns=columns)\n","], ignore_index=True)\n","\n","# Display Final Results\n","print(\"\\nFinal Model Performance (GRU vs GRU + LGBM)\\n\")\n","print(results_df.to_string(index=False))\n","\n","# Best Model Selection\n","best_overall = results_df[results_df[\"Dataset\"] == \"Validation\"].sort_values(\"MAPE\").iloc[0]\n","print(f\"\\nBest Overall Model: {best_overall['Layers']} (Based on Lowest Validation MAPE)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6nO9F2oPyku","executionInfo":{"status":"ok","timestamp":1739433389448,"user_tz":-330,"elapsed":220161,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"26b0b633-a9e7-4183-e72d-d16eb2706ac5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training GRU with 2 layers...\n","Epoch [10/100], Loss: 0.1396\n","Epoch [20/100], Loss: 0.0852\n","Epoch [30/100], Loss: 0.0803\n","Epoch [40/100], Loss: 0.0740\n","Epoch [50/100], Loss: 0.0726\n","Epoch [60/100], Loss: 0.0716\n","Epoch [70/100], Loss: 0.0716\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training GRU with 3 layers...\n","Epoch [10/100], Loss: 0.0831\n","Epoch [20/100], Loss: 0.0739\n","Epoch [30/100], Loss: 0.0716\n","Epoch [40/100], Loss: 0.0720\n","Epoch [50/100], Loss: 0.0715\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","Training GRU with 5 layers...\n","Epoch [10/100], Loss: 0.0738\n","Epoch [20/100], Loss: 0.0734\n","Epoch [30/100], Loss: 0.0717\n","Epoch [40/100], Loss: 0.0716\n","Epoch [50/100], Loss: 0.0716\n","Epoch [60/100], Loss: 0.0715\n","Epoch [70/100], Loss: 0.0715\n","Epoch [80/100], Loss: 0.0715\n","Epoch [90/100], Loss: 0.0715\n","Epoch [100/100], Loss: 0.0715\n","\n","GRU Model Performance Comparison (2, 3, and 5 Layers)\n","\n","Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","     2      Train 0.330532 0.176675 0.420328   -0.009453 126.379309\n","     2 Validation 1.307226 1.715130 1.309630 -290.612172  74.792352\n","     2       Test 1.586232 2.522648 1.588285 -416.998093  78.661559\n","     3      Train 0.327263 0.175122 0.418476   -0.000579 124.696645\n","     3 Validation 1.294427 1.681435 1.296702 -284.883327  74.064547\n","     3       Test 1.563786 2.451476 1.565719 -405.205096  77.551931\n","     5      Train 0.327849 0.175167 0.418530   -0.000834 125.143863\n","     5 Validation 1.293210 1.678278 1.295484 -284.346540  73.994873\n","     5       Test 1.562341 2.446949 1.564273 -404.454906  77.480301\n","\n","Best GRU Model: 5 Layers (Based on Lowest Validation MAPE)\n","\n","\n","Training LightGBM on Best GRU Embeddings...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000505 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0334049\tvalid_1's rmse: 0.261976\n","[100]\ttraining's rmse: 0.00902579\tvalid_1's rmse: 0.183162\n","[150]\ttraining's rmse: 0.00867477\tvalid_1's rmse: 0.177193\n","[200]\ttraining's rmse: 0.00867198\tvalid_1's rmse: 0.176696\n","\n","Final Model Performance (GRU vs GRU + LGBM)\n","\n","       Layers    Dataset      MAE      MSE     RMSE          R²       MAPE\n","            2      Train 0.330532 0.176675 0.420328   -0.009453 126.379309\n","            2 Validation 1.307226 1.715130 1.309630 -290.612172  74.792352\n","            2       Test 1.586232 2.522648 1.588285 -416.998093  78.661559\n","            3      Train 0.327263 0.175122 0.418476   -0.000579 124.696645\n","            3 Validation 1.294427 1.681435 1.296702 -284.883327  74.064547\n","            3       Test 1.563786 2.451476 1.565719 -405.205096  77.551931\n","            5      Train 0.327849 0.175167 0.418530   -0.000834 125.143863\n","            5 Validation 1.293210 1.678278 1.295484 -284.346540  73.994873\n","            5       Test 1.562341 2.446949 1.564273 -404.454906  77.480301\n","GRU(5) + LGBM      Train 0.005509 0.000075 0.008672    0.999570   1.647413\n","GRU(5) + LGBM Validation 0.159232 0.031221 0.176696   -4.308379   8.939771\n","GRU(5) + LGBM       Test 0.428222 0.189409 0.435212  -30.384783  21.125497\n","\n","Best Overall Model: GRU(5) + LGBM (Based on Lowest Validation MAPE)\n"]}]},{"cell_type":"markdown","source":["### optuna"],"metadata":{"id":"772VFiL1qd8f"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w4V4-lMYUhJA","executionInfo":{"status":"ok","timestamp":1739433595411,"user_tz":-330,"elapsed":5158,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"b33bcecf-ba79-47aa-9548-fec7ebcce556"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import lightgbm as lgb\n","import pandas as pd\n","import optuna\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h0)\n","        return self.fc(out[:, -1, :])\n","\n","# Function to compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# ----------- OPTUNA OPTIMIZATION FUNCTION -----------\n","\n","def objective(trial):\n","    # Sample GRU hyperparameters\n","    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128, step=16)\n","    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n","\n","    # Initialize model, loss function, and optimizer\n","    model = GRUModel(input_size=3, hidden_size=hidden_size, num_layers=num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    num_epochs = 50\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        val_pred = model(X_val_torch).cpu().numpy()\n","\n","    # Inverse transform predictions\n","    val_pred_actual = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","\n","    # Compute MAPE (minimization objective)\n","    _, _, _, _, mape = compute_metrics(Y_val.values.flatten(), val_pred_actual.flatten())\n","\n","    return mape  # Optuna minimizes MAPE\n","\n","# Run Optuna for GRU\n","study_gru = optuna.create_study(direction=\"minimize\")\n","study_gru.optimize(objective, n_trials=20)\n","\n","# Best GRU Model Parameters\n","best_gru_params = study_gru.best_params\n","print(\"\\nBest GRU Model:\", best_gru_params)\n","\n","# ----------- Train Best GRU and Get Embeddings -----------\n","\n","best_gru = GRUModel(input_size=3, hidden_size=best_gru_params[\"hidden_size\"], num_layers=best_gru_params[\"num_layers\"]).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(best_gru.parameters(), lr=best_gru_params[\"learning_rate\"])\n","\n","# Train Best GRU\n","for epoch in range(50):\n","    best_gru.train()\n","    outputs = best_gru(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","# Get GRU embeddings\n","best_gru.eval()\n","with torch.no_grad():\n","    train_pred = best_gru(X_train_torch).cpu().numpy()\n","    val_pred = best_gru(X_val_torch).cpu().numpy()\n","    test_pred = best_gru(X_test_torch).cpu().numpy()\n","\n","# Inverse transform predictions\n","X_train_lgb = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","X_val_lgb = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","X_test_lgb = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","# ----------- OPTUNA OPTIMIZATION FOR LIGHTGBM -----------\n","\n","def objective_lgb(trial):\n","    params = {\n","        \"objective\": \"regression\",\n","        \"metric\": \"rmse\",\n","        \"boosting_type\": \"gbdt\",\n","        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 50),\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n","        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 30)\n","    }\n","\n","    lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","    lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","\n","    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","\n","    val_pred = model.predict(X_val_lgb)\n","    _, _, _, _, mape = compute_metrics(Y_val.values.flatten(), val_pred.flatten())\n","\n","    return mape  # Optuna minimizes MAPE\n","\n","# Run Optuna for LGBM\n","study_lgb = optuna.create_study(direction=\"minimize\")\n","study_lgb.optimize(objective_lgb, n_trials=20)\n","\n","# Best LGBM Model Parameters\n","best_lgb_params = study_lgb.best_params\n","print(\"\\nBest LightGBM Model:\", best_lgb_params)\n","\n","# Train Final LightGBM Model\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","\n","final_lgb = lgb.train(best_lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","\n","# Predictions\n","train_pred_lgb = final_lgb.predict(X_train_lgb)\n","val_pred_lgb = final_lgb.predict(X_val_lgb)\n","test_pred_lgb = final_lgb.predict(X_test_lgb)\n","\n","# Compute metrics\n","metrics_train_lgb = compute_metrics(Y_train.values.flatten(), train_pred_lgb.flatten())\n","metrics_val_lgb = compute_metrics(Y_val.values.flatten(), val_pred_lgb.flatten())\n","metrics_test_lgb = compute_metrics(Y_test.values.flatten(), test_pred_lgb.flatten())\n","\n","print(\"\\nFinal GRU + LightGBM Performance:\")\n","print(\"Train:\", metrics_train_lgb)\n","print(\"Validation:\", metrics_val_lgb)\n","print(\"Test:\", metrics_test_lgb)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJleAoAES88J","executionInfo":{"status":"ok","timestamp":1739434459880,"user_tz":-330,"elapsed":839964,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"ac1c98ca-b439-485b-9c53-5ad0e40c860f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:00:22,495] A new study created in memory with name: no-name-7e72d292-957d-41ed-9c7a-4903017cb99a\n","[I 2025-02-13 08:00:58,796] Trial 0 finished with value: 73.88033346772791 and parameters: {'hidden_size': 64, 'num_layers': 4, 'learning_rate': 0.00629976609866542}. Best is trial 0 with value: 73.88033346772791.\n","[I 2025-02-13 08:01:40,251] Trial 1 finished with value: 74.86668047947921 and parameters: {'hidden_size': 96, 'num_layers': 4, 'learning_rate': 0.0017233513072134636}. Best is trial 0 with value: 73.88033346772791.\n","[I 2025-02-13 08:02:23,469] Trial 2 finished with value: 73.26963817216257 and parameters: {'hidden_size': 48, 'num_layers': 4, 'learning_rate': 0.0019326251186914116}. Best is trial 2 with value: 73.26963817216257.\n","[I 2025-02-13 08:02:58,134] Trial 3 finished with value: 73.31971603797183 and parameters: {'hidden_size': 64, 'num_layers': 4, 'learning_rate': 0.0030998377124945363}. Best is trial 2 with value: 73.26963817216257.\n","[I 2025-02-13 08:03:38,380] Trial 4 finished with value: 82.74867599040053 and parameters: {'hidden_size': 64, 'num_layers': 5, 'learning_rate': 0.00014883494730166515}. Best is trial 2 with value: 73.26963817216257.\n","[I 2025-02-13 08:04:10,256] Trial 5 finished with value: 72.98538771470847 and parameters: {'hidden_size': 48, 'num_layers': 3, 'learning_rate': 0.005177074354494777}. Best is trial 5 with value: 72.98538771470847.\n","[I 2025-02-13 08:04:44,846] Trial 6 finished with value: 72.95027354912544 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.002229561779762639}. Best is trial 6 with value: 72.95027354912544.\n","[I 2025-02-13 08:05:41,248] Trial 7 finished with value: 71.82128468516092 and parameters: {'hidden_size': 128, 'num_layers': 5, 'learning_rate': 0.00020643919395418254}. Best is trial 7 with value: 71.82128468516092.\n","[I 2025-02-13 08:06:35,670] Trial 8 finished with value: 74.74936892207154 and parameters: {'hidden_size': 112, 'num_layers': 5, 'learning_rate': 0.00034200969673289674}. Best is trial 7 with value: 71.82128468516092.\n","[I 2025-02-13 08:07:19,083] Trial 9 finished with value: 71.08000081335533 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.00032233123572281976}. Best is trial 9 with value: 71.08000081335533.\n","[I 2025-02-13 08:08:01,415] Trial 10 finished with value: 75.25210871955633 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0006255573849175739}. Best is trial 9 with value: 71.08000081335533.\n","[I 2025-02-13 08:08:47,027] Trial 11 finished with value: 74.25610606579119 and parameters: {'hidden_size': 128, 'num_layers': 3, 'learning_rate': 0.00011112305182898824}. Best is trial 9 with value: 71.08000081335533.\n","[I 2025-02-13 08:09:22,609] Trial 12 finished with value: 66.47222763998366 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0002974606806431787}. Best is trial 12 with value: 66.47222763998366.\n","[I 2025-02-13 08:09:58,391] Trial 13 finished with value: 73.09451284707411 and parameters: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0005347889497613645}. Best is trial 12 with value: 66.47222763998366.\n","[I 2025-02-13 08:10:38,326] Trial 14 finished with value: 69.11875671212269 and parameters: {'hidden_size': 96, 'num_layers': 3, 'learning_rate': 0.00029808274528201473}. Best is trial 12 with value: 66.47222763998366.\n","[I 2025-02-13 08:11:16,115] Trial 15 finished with value: 73.41099326947652 and parameters: {'hidden_size': 80, 'num_layers': 3, 'learning_rate': 0.0010050879680987283}. Best is trial 12 with value: 66.47222763998366.\n","[I 2025-02-13 08:11:55,208] Trial 16 finished with value: 71.31258119879453 and parameters: {'hidden_size': 80, 'num_layers': 3, 'learning_rate': 0.0002368273495648763}. Best is trial 12 with value: 66.47222763998366.\n","[I 2025-02-13 08:12:32,757] Trial 17 finished with value: 74.14310602580929 and parameters: {'hidden_size': 112, 'num_layers': 2, 'learning_rate': 0.0007957883150687763}. Best is trial 12 with value: 66.47222763998366.\n","[I 2025-02-13 08:13:03,424] Trial 18 finished with value: 75.95177265047282 and parameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.0003968645013526727}. Best is trial 12 with value: 66.47222763998366.\n","[I 2025-02-13 08:13:41,025] Trial 19 finished with value: 73.3667237311217 and parameters: {'hidden_size': 112, 'num_layers': 2, 'learning_rate': 0.0010246691633324586}. Best is trial 12 with value: 66.47222763998366.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Best GRU Model: {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0002974606806431787}\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:17,361] A new study created in memory with name: no-name-a1596d54-627f-4d76-87a0-69410628f2eb\n","[I 2025-02-13 08:14:17,480] Trial 0 finished with value: 9.284707649008874 and parameters: {'num_leaves': 21, 'learning_rate': 0.024559450904080706, 'max_depth': 5, 'min_data_in_leaf': 29}. Best is trial 0 with value: 9.284707649008874.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000128 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.121061\tvalid_1's rmse: 0.507117\n","[100]\ttraining's rmse: 0.0353741\tvalid_1's rmse: 0.270387\n","[150]\ttraining's rmse: 0.0114053\tvalid_1's rmse: 0.201863\n","[200]\ttraining's rmse: 0.00602047\tvalid_1's rmse: 0.182164\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.00690072\tvalid_1's rmse: 0.186297\n","[100]\ttraining's rmse: 0.00526441\tvalid_1's rmse: 0.174212\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:17,644] Trial 1 finished with value: 8.774870806580424 and parameters: {'num_leaves': 23, 'learning_rate': 0.08699143020849764, 'max_depth': 5, 'min_data_in_leaf': 29}. Best is trial 1 with value: 8.774870806580424.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00526166\tvalid_1's rmse: 0.174084\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.0052606\tvalid_1's rmse: 0.174082\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000124 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[50]\ttraining's rmse: 0.0202\tvalid_1's rmse: 0.245083\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[100]\ttraining's rmse: 0.0054162\tvalid_1's rmse: 0.178263\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:17,838] Trial 2 finished with value: 8.783026034208394 and parameters: {'num_leaves': 27, 'learning_rate': 0.06062983439633724, 'max_depth': 3, 'min_data_in_leaf': 17}. Best is trial 1 with value: 8.774870806580424.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00532204\tvalid_1's rmse: 0.174435\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00530908\tvalid_1's rmse: 0.174211\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000122 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[50]\ttraining's rmse: 0.0292675\tvalid_1's rmse: 0.273141\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[100]\ttraining's rmse: 0.00574755\tvalid_1's rmse: 0.189846\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00536464\tvalid_1's rmse: 0.183145\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:18,043] Trial 3 finished with value: 9.31894420880134 and parameters: {'num_leaves': 31, 'learning_rate': 0.05305921810180837, 'max_depth': 3, 'min_data_in_leaf': 30}. Best is trial 1 with value: 8.774870806580424.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00535433\tvalid_1's rmse: 0.182707\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[50]\ttraining's rmse: 0.0658092\tvalid_1's rmse: 0.360832\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[100]\ttraining's rmse: 0.0115753\tvalid_1's rmse: 0.204619\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00550657\tvalid_1's rmse: 0.179029\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:18,265] Trial 4 finished with value: 8.825320068877827 and parameters: {'num_leaves': 25, 'learning_rate': 0.036509663439804094, 'max_depth': 4, 'min_data_in_leaf': 6}. Best is trial 1 with value: 8.774870806580424.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00526779\tvalid_1's rmse: 0.174883\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000122 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0178593\tvalid_1's rmse: 0.217473\n","[100]\ttraining's rmse: 0.00530577\tvalid_1's rmse: 0.175843\n","[150]\ttraining's rmse: 0.00525977\tvalid_1's rmse: 0.174151\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:18,473] Trial 5 finished with value: 8.774699090367243 and parameters: {'num_leaves': 46, 'learning_rate': 0.06200051037688981, 'max_depth': 10, 'min_data_in_leaf': 10}. Best is trial 5 with value: 8.774699090367243.\n","[I 2025-02-13 08:14:18,609] Trial 6 finished with value: 8.77626998649586 and parameters: {'num_leaves': 27, 'learning_rate': 0.052658649654771725, 'max_depth': 7, 'min_data_in_leaf': 21}. Best is trial 5 with value: 8.774699090367243.\n"]},{"output_type":"stream","name":"stdout","text":["[200]\ttraining's rmse: 0.00525969\tvalid_1's rmse: 0.174079\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000123 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0285643\tvalid_1's rmse: 0.249444\n","[100]\ttraining's rmse: 0.00558834\tvalid_1's rmse: 0.179103\n","[150]\ttraining's rmse: 0.00526166\tvalid_1's rmse: 0.174436\n","[200]\ttraining's rmse: 0.00525981\tvalid_1's rmse: 0.174104\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000214 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:18,755] Trial 7 finished with value: 8.85427073294276 and parameters: {'num_leaves': 31, 'learning_rate': 0.033359213030683905, 'max_depth': 10, 'min_data_in_leaf': 5}. Best is trial 5 with value: 8.774699090367243.\n"]},{"output_type":"stream","name":"stdout","text":["[50]\ttraining's rmse: 0.0769913\tvalid_1's rmse: 0.380441\n","[100]\ttraining's rmse: 0.0150548\tvalid_1's rmse: 0.211027\n","[150]\ttraining's rmse: 0.00586372\tvalid_1's rmse: 0.180874\n","[200]\ttraining's rmse: 0.00528153\tvalid_1's rmse: 0.175342\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000125 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.150936\tvalid_1's rmse: 0.573999\n","[100]\ttraining's rmse: 0.0546485\tvalid_1's rmse: 0.315288\n","[150]\ttraining's rmse: 0.0203081\tvalid_1's rmse: 0.224024\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:18,922] Trial 8 finished with value: 9.895610287227246 and parameters: {'num_leaves': 46, 'learning_rate': 0.0202004221782512, 'max_depth': 6, 'min_data_in_leaf': 18}. Best is trial 5 with value: 8.774699090367243.\n"]},{"output_type":"stream","name":"stdout","text":["[200]\ttraining's rmse: 0.00881497\tvalid_1's rmse: 0.19189\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000186 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0301913\tvalid_1's rmse: 0.251642\n","[100]\ttraining's rmse: 0.00566883\tvalid_1's rmse: 0.179533\n","[150]\ttraining's rmse: 0.00526191\tvalid_1's rmse: 0.174474\n","[200]\ttraining's rmse: 0.0052597\tvalid_1's rmse: 0.174105\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:19,089] Trial 9 finished with value: 8.776317955428594 and parameters: {'num_leaves': 37, 'learning_rate': 0.05153823313364949, 'max_depth': 8, 'min_data_in_leaf': 22}. Best is trial 5 with value: 8.774699090367243.\n","[I 2025-02-13 08:14:19,294] Trial 10 finished with value: 14.240342787674273 and parameters: {'num_leaves': 50, 'learning_rate': 0.012377138790418293, 'max_depth': 10, 'min_data_in_leaf': 11}. Best is trial 5 with value: 8.774699090367243.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000119 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.224526\tvalid_1's rmse: 0.772978\n","[100]\ttraining's rmse: 0.120559\tvalid_1's rmse: 0.492069\n","[150]\ttraining's rmse: 0.0648425\tvalid_1's rmse: 0.342428\n","[200]\ttraining's rmse: 0.0350759\tvalid_1's rmse: 0.263058\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000214 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.00646222\tvalid_1's rmse: 0.183668\n","[100]\ttraining's rmse: 0.00525988\tvalid_1's rmse: 0.174168\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00525969\tvalid_1's rmse: 0.174077\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00525969\tvalid_1's rmse: 0.174076\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:19,537] Trial 11 finished with value: 8.774508473755759 and parameters: {'num_leaves': 40, 'learning_rate': 0.09001021067402358, 'max_depth': 8, 'min_data_in_leaf': 12}. Best is trial 11 with value: 8.774508473755759.\n","[I 2025-02-13 08:14:19,746] Trial 12 finished with value: 8.774498773612361 and parameters: {'num_leaves': 41, 'learning_rate': 0.09273856181583953, 'max_depth': 9, 'min_data_in_leaf': 12}. Best is trial 12 with value: 8.774498773612361.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000119 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.00617302\tvalid_1's rmse: 0.182306\n","[100]\ttraining's rmse: 0.00525979\tvalid_1's rmse: 0.174147\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00525969\tvalid_1's rmse: 0.174076\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00525969\tvalid_1's rmse: 0.174076\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:19,947] Trial 13 finished with value: 8.774514755670513 and parameters: {'num_leaves': 39, 'learning_rate': 0.0804746502143998, 'max_depth': 8, 'min_data_in_leaf': 12}. Best is trial 12 with value: 8.774498773612361.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.00822244\tvalid_1's rmse: 0.190218\n","[100]\ttraining's rmse: 0.00526066\tvalid_1's rmse: 0.17433\n","[150]\ttraining's rmse: 0.0052597\tvalid_1's rmse: 0.17408\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00525969\tvalid_1's rmse: 0.174076\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000123 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:20,167] Trial 14 finished with value: 8.774511737718617 and parameters: {'num_leaves': 41, 'learning_rate': 0.09251837122823287, 'max_depth': 8, 'min_data_in_leaf': 14}. Best is trial 12 with value: 8.774498773612361.\n"]},{"output_type":"stream","name":"stdout","text":["[50]\ttraining's rmse: 0.00619365\tvalid_1's rmse: 0.182346\n","[100]\ttraining's rmse: 0.00525982\tvalid_1's rmse: 0.174147\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.0052597\tvalid_1's rmse: 0.174076\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00525969\tvalid_1's rmse: 0.174076\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000124 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:20,422] Trial 15 finished with value: 8.774497539715709 and parameters: {'num_leaves': 43, 'learning_rate': 0.09866766139024867, 'max_depth': 9, 'min_data_in_leaf': 8}. Best is trial 15 with value: 8.774497539715709.\n"]},{"output_type":"stream","name":"stdout","text":["[50]\ttraining's rmse: 0.00575226\tvalid_1's rmse: 0.179972\n","[100]\ttraining's rmse: 0.00525974\tvalid_1's rmse: 0.174113\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's rmse: 0.00525969\tvalid_1's rmse: 0.174076\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00525969\tvalid_1's rmse: 0.174076\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:20,617] Trial 16 finished with value: 16.48861040800895 and parameters: {'num_leaves': 44, 'learning_rate': 0.01068319956236507, 'max_depth': 9, 'min_data_in_leaf': 8}. Best is trial 15 with value: 8.774497539715709.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.244607\tvalid_1's rmse: 0.828217\n","[100]\ttraining's rmse: 0.143062\tvalid_1's rmse: 0.553592\n","[150]\ttraining's rmse: 0.0837438\tvalid_1's rmse: 0.393357\n","[200]\ttraining's rmse: 0.0491429\tvalid_1's rmse: 0.300762\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000122 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:20,790] Trial 17 finished with value: 8.79172021843407 and parameters: {'num_leaves': 34, 'learning_rate': 0.040824840574976415, 'max_depth': 9, 'min_data_in_leaf': 15}. Best is trial 15 with value: 8.774497539715709.\n"]},{"output_type":"stream","name":"stdout","text":["[50]\ttraining's rmse: 0.0523915\tvalid_1's rmse: 0.312149\n","[100]\ttraining's rmse: 0.00835908\tvalid_1's rmse: 0.190975\n","[150]\ttraining's rmse: 0.00532202\tvalid_1's rmse: 0.176198\n","[200]\ttraining's rmse: 0.00526069\tvalid_1's rmse: 0.17435\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000120 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0103513\tvalid_1's rmse: 0.196534\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:21,010] Trial 18 finished with value: 8.774513905713668 and parameters: {'num_leaves': 50, 'learning_rate': 0.07410499478552804, 'max_depth': 7, 'min_data_in_leaf': 8}. Best is trial 15 with value: 8.774497539715709.\n"]},{"output_type":"stream","name":"stdout","text":["[100]\ttraining's rmse: 0.00526324\tvalid_1's rmse: 0.174557\n","[150]\ttraining's rmse: 0.0052597\tvalid_1's rmse: 0.174086\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's rmse: 0.00525969\tvalid_1's rmse: 0.174076\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000124 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.177125\tvalid_1's rmse: 0.646086\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-02-13 08:14:21,224] Trial 19 finished with value: 10.911566432374446 and parameters: {'num_leaves': 43, 'learning_rate': 0.017056505991077046, 'max_depth': 9, 'min_data_in_leaf': 24}. Best is trial 15 with value: 8.774497539715709.\n"]},{"output_type":"stream","name":"stdout","text":["[100]\ttraining's rmse: 0.075118\tvalid_1's rmse: 0.370402\n","[150]\ttraining's rmse: 0.0321484\tvalid_1's rmse: 0.255734\n","[200]\ttraining's rmse: 0.0144174\tvalid_1's rmse: 0.208243\n","\n","Best LightGBM Model: {'num_leaves': 43, 'learning_rate': 0.09866766139024867, 'max_depth': 9, 'min_data_in_leaf': 8}\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000123 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's l2: 3.30885e-05\tvalid_1's l2: 0.0323899\n","[100]\ttraining's l2: 2.76648e-05\tvalid_1's l2: 0.0303152\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[150]\ttraining's l2: 2.76643e-05\tvalid_1's l2: 0.0303024\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[200]\ttraining's l2: 2.76643e-05\tvalid_1's l2: 0.0303023\n","\n","Final GRU + LightGBM Performance:\n","Train: (0.0034931986437253893, 2.7664292305943547e-05, 0.005259685571015015, 0.9998419373028942, 1.0065571208703437)\n","Validation: (0.15634806656129713, 0.03030230019659937, 0.17407555887200066, -4.152100044586075, 8.774497539715709)\n","Test: (0.42530850619478844, 0.18692239583264042, 0.43234522760479316, -29.972695216853605, 20.98071155347049)\n"]}]},{"cell_type":"markdown","source":["### bohb"],"metadata":{"id":"QbsT7a3yqiVz"}},{"cell_type":"code","source":["!pip install hpbandster ConfigSpace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cPguaSU6beS_","executionInfo":{"status":"ok","timestamp":1739436635889,"user_tz":-330,"elapsed":16806,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"e923c72c-1628-479c-f846-e77eba6b476c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.13.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, ConfigSpace, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=f01094cda43c15986d8dfc5df8cee503fccf33a05ac71fac6ae609ffefa66b36\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=ConfigSpace-1.2.1-py3-none-any.whl size=115943 sha256=4c872af4359ea2d55e6abd4460e9b4e806113a723b81a6963dad8799185cbce5\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35183 sha256=f0b703d444ad7725ee96d093b1ce9cd5c97b1d3f603fa70fa9785a9b7f8744e9\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster ConfigSpace netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, ConfigSpace, hpbandster\n","Successfully installed ConfigSpace-1.2.1 Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import lightgbm as lgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# GRU Configurations (2, 3, and 5 layers)\n","gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store GRU feature representations\n","gru_features = {}\n","\n","for num_layers in gru_layers:\n","    print(f\"Training GRU with {num_layers} layers...\")\n","\n","    gru_model = GRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        gru_model.train()\n","        optimizer.zero_grad()\n","        outputs = gru_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Extract Feature Representations\n","    gru_model.eval()\n","    with torch.no_grad():\n","        train_features = gru_model(X_train_torch).numpy()\n","        val_features = gru_model(X_val_torch).numpy()\n","        test_features = gru_model(X_test_torch).numpy()\n","\n","    gru_features[num_layers] = (train_features, val_features, test_features)\n","\n","# Define ConfigSpace for BOHB (LightGBM)\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_leaves\", 20, 300, default_value=50))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 12, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"feature_fraction\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for LightGBM\n","class LightGBMWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = lgb.LGBMRegressor(\n","            num_leaves=config[\"num_leaves\"],\n","            max_depth=config[\"max_depth\"],\n","            learning_rate=config[\"learning_rate\"],\n","            feature_fraction=config[\"feature_fraction\"],\n","            random_state=42\n","        )\n","        model.fit(self.train_features, Y_train)\n","        Y_val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB for each GRU configuration\n","best_models = {}\n","\n","for num_layers in gru_layers:\n","    print(f\"\\nRunning BOHB for GRU ({num_layers} layers) + LightGBM...\")\n","\n","    train_features, val_features, test_features = gru_features[num_layers]\n","\n","    # Start NameServer\n","    NS = hpns.NameServer(run_id=f\"gru_{num_layers}_lgb_bohb\", host=\"127.0.0.1\", port=None)\n","    NS.start()\n","\n","    worker = LightGBMWorker(\n","        train_features=train_features,\n","        val_features=val_features,\n","        nameserver=\"127.0.0.1\",\n","        run_id=f\"gru_{num_layers}_lgb_bohb\"\n","    )\n","    worker.run(background=True)\n","\n","    bohb = BOHB(\n","        configspace=get_config_space(),\n","        run_id=f\"gru_{num_layers}_lgb_bohb\",\n","        nameserver=\"127.0.0.1\",\n","        min_budget=1,\n","        max_budget=3\n","    )\n","\n","    res = bohb.run(n_iterations=50)\n","\n","    # Shutdown BOHB\n","    bohb.shutdown()\n","    NS.shutdown()\n","\n","    # Retrieve Best Configuration\n","    best_config = res.get_incumbent_id()\n","    best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","    # Train Best LightGBM Model on GRU Features\n","    best_lgb_model = lgb.LGBMRegressor(\n","        num_leaves=best_params[\"num_leaves\"],\n","        max_depth=best_params[\"max_depth\"],\n","        learning_rate=best_params[\"learning_rate\"],\n","        feature_fraction=best_params[\"feature_fraction\"],\n","        random_state=42\n","    )\n","\n","    best_lgb_model.fit(train_features, Y_train)\n","\n","    # Make Predictions\n","    Y_train_pred = best_lgb_model.predict(train_features)\n","    Y_val_pred = best_lgb_model.predict(val_features)\n","    Y_test_pred = best_lgb_model.predict(test_features)\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store best model and metrics\n","    best_models[num_layers] = {\n","        \"params\": best_params,\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics\n","    }\n","\n","    # Print Results\n","    print(f\"\\nBest Parameters for GRU ({num_layers} layers) + LightGBM:\")\n","    print(best_params)\n","\n","    print(\"\\nTraining set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"\\nValidation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"\\nTest set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"On6djYZ4Uqu5","executionInfo":{"status":"ok","timestamp":1739435691232,"user_tz":-330,"elapsed":180899,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"3709ba1f-3323-4b38-a5da-5c342a3a5911"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training GRU with 2 layers...\n","Training GRU with 3 layers...\n","Training GRU with 5 layers...\n","\n","Running BOHB for GRU (2 layers) + LightGBM...\n","[LightGBM] [Warning] feature_fraction is set=0.5162360818571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5162360818571\n","[LightGBM] [Warning] feature_fraction is set=0.5162360818571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5162360818571\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000202 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.5162360818571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5162360818571\n","[LightGBM] [Warning] feature_fraction is set=0.5162360818571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5162360818571\n","[LightGBM] [Warning] feature_fraction is set=0.5162360818571, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5162360818571\n","\n","Best Parameters for GRU (2 layers) + LightGBM:\n","{'feature_fraction': 0.5162360818571, 'learning_rate': 0.2994566217115, 'max_depth': 11, 'num_leaves': 186}\n","\n","Training set metrics:\n","MAE: 0.0037, MSE: 0.0000, RMSE: 0.0055, R²: 0.9998, MAPE: 1.07%\n","\n","Validation set metrics:\n","MAE: 0.1565, MSE: 0.0303, RMSE: 0.1742, R²: -4.1586, MAPE: 8.78%\n","\n","Test set metrics:\n","MAE: 0.4254, MSE: 0.1870, RMSE: 0.4325, R²: -29.9898, MAPE: 20.99%\n","\n","Running BOHB for GRU (3 layers) + LightGBM...\n","[LightGBM] [Warning] feature_fraction is set=0.6593984010549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6593984010549\n","[LightGBM] [Warning] feature_fraction is set=0.6593984010549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6593984010549\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001520 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.6593984010549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6593984010549\n","[LightGBM] [Warning] feature_fraction is set=0.6593984010549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6593984010549\n","[LightGBM] [Warning] feature_fraction is set=0.6593984010549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6593984010549\n","\n","Best Parameters for GRU (3 layers) + LightGBM:\n","{'feature_fraction': 0.6593984010549, 'learning_rate': 0.2839254990202, 'max_depth': 11, 'num_leaves': 145}\n","\n","Training set metrics:\n","MAE: 0.0035, MSE: 0.0000, RMSE: 0.0053, R²: 0.9998, MAPE: 1.01%\n","\n","Validation set metrics:\n","MAE: 0.1563, MSE: 0.0303, RMSE: 0.1741, R²: -4.1521, MAPE: 8.77%\n","\n","Test set metrics:\n","MAE: 0.4253, MSE: 0.1869, RMSE: 0.4323, R²: -29.9727, MAPE: 20.98%\n","\n","Running BOHB for GRU (5 layers) + LightGBM...\n","[LightGBM] [Warning] feature_fraction is set=0.961781340168, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.961781340168\n","[LightGBM] [Warning] feature_fraction is set=0.961781340168, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.961781340168\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000193 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.961781340168, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.961781340168\n","[LightGBM] [Warning] feature_fraction is set=0.961781340168, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.961781340168\n","[LightGBM] [Warning] feature_fraction is set=0.961781340168, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.961781340168\n","\n","Best Parameters for GRU (5 layers) + LightGBM:\n","{'feature_fraction': 0.961781340168, 'learning_rate': 0.2445583270804, 'max_depth': 11, 'num_leaves': 299}\n","\n","Training set metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0055, R²: 0.9998, MAPE: 1.05%\n","\n","Validation set metrics:\n","MAE: 0.1563, MSE: 0.0303, RMSE: 0.1741, R²: -4.1521, MAPE: 8.77%\n","\n","Test set metrics:\n","MAE: 0.4253, MSE: 0.1869, RMSE: 0.4323, R²: -29.9727, MAPE: 20.98%\n"]}]}]}