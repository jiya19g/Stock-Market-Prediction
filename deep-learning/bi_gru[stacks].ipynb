{"cells":[{"cell_type":"markdown","metadata":{"id":"b3zZmLiLkbT4"},"source":["##Initial Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7gOgVN8kW8r"},"outputs":[],"source":["# Importing necessary libraries for data analysis and manipulation\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","# For handling warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oFuOd4x5kfMG","outputId":"90989ddb-170c-4185-e8e3-c0fd0bd94fa1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orOZDFCjlf5a"},"outputs":[],"source":["\n","df_aapl = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stocks/AAPL.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-r0_rYh1lwPD"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import boxcox\n","\n","df_aapl['Close_log'] = np.log(df_aapl['Close'] + 1)\n","df_aapl['Close_sqrt'] = np.sqrt(df_aapl['Close'])\n","df_aapl['Close_boxcox'], _ = boxcox(df_aapl['Close'] + 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNuStzUZlyqT","outputId":"0f424966-d4c1-404a-cf80-d721bd3267b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Skewness: 2.5045276102319933\n","Log Transformation Skewness: 0.8535555176510303\n","Square Root Transformation Skewness: 1.6211545809555206\n","Box-Cox Transformation Skewness: 0.43527466713563334\n"]}],"source":["\n","skew_original = df_aapl['Close'].skew()\n","skew_log = df_aapl['Close_log'].skew()\n","skew_sqrt = df_aapl['Close_sqrt'].skew()\n","skew_boxcox = pd.Series(df_aapl['Close_boxcox']).skew()\n","\n","print(f\"Original Skewness: {skew_original}\")\n","print(f\"Log Transformation Skewness: {skew_log}\")\n","print(f\"Square Root Transformation Skewness: {skew_sqrt}\")\n","print(f\"Box-Cox Transformation Skewness: {skew_boxcox}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNrMNRaKnRvc"},"outputs":[],"source":["\n","df_aapl['Open_log'] = np.log(df_aapl['Open'])\n","df_aapl['High_log'] = np.log(df_aapl['High'])\n","df_aapl['Low_log'] = np.log(df_aapl['Low'])\n","df_aapl['Adj Close_log'] = np.log(df_aapl['Adj Close'])\n","df_aapl['Volume_log'] = np.log(df_aapl['Volume'])\n","\n","\n","df_aapl['Open_sqrt'] = np.sqrt(df_aapl['Open'])\n","df_aapl['High_sqrt'] = np.sqrt(df_aapl['High'])\n","df_aapl['Low_sqrt'] = np.sqrt(df_aapl['Low'])\n","df_aapl['Adj Close_sqrt'] = np.sqrt(df_aapl['Adj Close'])\n","df_aapl['Volume_sqrt'] = np.sqrt(df_aapl['Volume'])\n","\n","from scipy.stats import boxcox\n","df_aapl['Open_boxcox'], _ = boxcox(df_aapl['Open'])\n","df_aapl['High_boxcox'], _ = boxcox(df_aapl['High'])\n","df_aapl['Low_boxcox'], _ = boxcox(df_aapl['Low'])\n","df_aapl['Adj Close_boxcox'], _ = boxcox(df_aapl['Adj Close'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ky1SQb1-nUAl","outputId":"d8120ecc-ee77-4930-bc37-3b1732319a75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Skewness Before Transformation:\n"," Open         2.504632\n","High         2.502208\n","Low          2.506714\n","Adj Close    2.550677\n","Volume       3.565699\n","dtype: float64\n","\n","Skewness After Transformation:\n"," Open_log            0.482872\n","High_log            0.481997\n","Low_log             0.484246\n","Adj Close_log       0.494009\n","Open_sqrt           1.620771\n","High_sqrt           1.621456\n","Low_sqrt            1.620661\n","Adj Close_sqrt      1.679402\n","Volume_sqrt         1.299776\n","Open_boxcox         0.181226\n","High_boxcox         0.179749\n","Low_boxcox          0.182882\n","Adj Close_boxcox    0.180085\n","dtype: float64\n"]}],"source":["\n","skewness_before = df_aapl[['Open', 'High', 'Low', 'Adj Close', 'Volume']].skew()\n","skewness_after = df_aapl[['Open_log', 'High_log', 'Low_log', 'Adj Close_log',\n","                          'Open_sqrt', 'High_sqrt', 'Low_sqrt', 'Adj Close_sqrt', 'Volume_sqrt',\n","                          'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox']].skew()\n","\n","print(\"Skewness Before Transformation:\\n\", skewness_before)\n","print(\"\\nSkewness After Transformation:\\n\", skewness_after)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"48zmxKq7nV39","outputId":"80813ec5-d1a9-44ba-e165-f9d6a9b64d6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Skewness After Box-Cox Transformation:\n","Open_boxcox         0.435237\n","High_boxcox         0.433381\n","Low_boxcox          0.437331\n","Adj Close_boxcox    0.458762\n","Close_boxcox        0.435275\n","dtype: float64\n"]}],"source":["from scipy import stats\n","\n","df_aapl['Open_boxcox'], _ = stats.boxcox(df_aapl['Open'] + 1)\n","df_aapl['High_boxcox'], _ = stats.boxcox(df_aapl['High'] + 1)\n","df_aapl['Low_boxcox'], _ = stats.boxcox(df_aapl['Low'] + 1)\n","df_aapl['Adj Close_boxcox'], _ = stats.boxcox(df_aapl['Adj Close'] + 1)\n","df_aapl['Close_boxcox'], _ = stats.boxcox(df_aapl['Close'] + 1)\n","\n","skewness_after_boxcox = df_aapl[['Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox', 'Close_boxcox']].skew()\n","\n","print(\"Skewness After Box-Cox Transformation:\")\n","print(skewness_after_boxcox)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"giiXoL8snYA1","outputId":"b430fa4d-87e2-450a-82e6-75b000e55494"},"outputs":[{"name":"stdout","output_type":"stream","text":["         Date      Open      High       Low  Adj Close     Close     Volume  \\\n","0  1980-12-12  0.128348  0.128906  0.128348   0.098943  0.128348  469033600   \n","1  1980-12-15  0.122210  0.122210  0.121652   0.093781  0.121652  175884800   \n","2  1980-12-16  0.113281  0.113281  0.112723   0.086898  0.112723  105728000   \n","3  1980-12-17  0.115513  0.116071  0.115513   0.089049  0.115513   86441600   \n","4  1980-12-18  0.118862  0.119420  0.118862   0.091630  0.118862   73449600   \n","\n","   Open_boxcox  High_boxcox  Low_boxcox  Adj Close_boxcox  Close_boxcox  \n","0     0.117689     0.118173    0.117674          0.092374      0.117689  \n","1     0.112503     0.112516    0.112016          0.087857      0.112030  \n","2     0.104886     0.104897    0.104395          0.081785      0.104407  \n","3     0.106798     0.107287    0.106786          0.083688      0.106798  \n","4     0.109657     0.110145    0.109644          0.085966      0.109657  \n"]}],"source":["\n","df_aapl_cleaned = df_aapl[['Date', 'Open', 'High', 'Low', 'Adj Close', 'Close', 'Volume',\n","                           'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox',\n","                           'Close_boxcox']]\n","\n","print(df_aapl_cleaned.head())\n"]},{"cell_type":"markdown","metadata":{"id":"kMvxyuvvnbxW"},"source":["##Train Validation Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJpYQOmAnZ2u","outputId":"023ce2b2-447c-47af-de94-6b6a97062217"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set: (7736, 3), Validation set: (1658, 3), Test set: (1658, 3)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","X = df_aapl_cleaned[['Open_boxcox', 'High_boxcox', 'Low_boxcox']]\n","Y = df_aapl_cleaned['Close_boxcox']\n","\n","X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, shuffle=False)\n","X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, shuffle=False)\n","\n","print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n"]},{"cell_type":"markdown","metadata":{"id":"vcdgzKxCxxvY"},"source":["#BI-Gru"]},{"cell_type":"markdown","metadata":{"id":"5UR4JmmbypjK"},"source":["##initial"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CnmpyYEYx3OU","outputId":"20791411-1dc1-4dff-9c6a-c446602dfee1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training Bi-GRU with 2 layers...\n","\n","Bi-GRU (2 layers) Metrics:\n","Training Time: 18.7199 sec, Validation Time: 0.0129 sec, Testing Time: 0.0112 sec\n","Training set metrics:\n","MAE: 0.0065, MSE: 0.0001, RMSE: 0.0083, R²: 0.9996, MAPE: 4.03%\n","Validation set metrics:\n","MAE: 0.0075, MSE: 0.0001, RMSE: 0.0089, R²: 0.9864, MAPE: 0.42%\n","Test set metrics:\n","MAE: 0.0329, MSE: 0.0012, RMSE: 0.0342, R²: 0.8067, MAPE: 1.61%\n","\n","Training Bi-GRU with 3 layers...\n","\n","Bi-GRU (3 layers) Metrics:\n","Training Time: 26.1354 sec, Validation Time: 0.0181 sec, Testing Time: 0.0170 sec\n","Training set metrics:\n","MAE: 0.0091, MSE: 0.0001, RMSE: 0.0117, R²: 0.9992, MAPE: 5.69%\n","Validation set metrics:\n","MAE: 0.0146, MSE: 0.0003, RMSE: 0.0170, R²: 0.9509, MAPE: 0.82%\n","Test set metrics:\n","MAE: 0.0580, MSE: 0.0036, RMSE: 0.0600, R²: 0.4036, MAPE: 2.85%\n","\n","Training Bi-GRU with 5 layers...\n","\n","Bi-GRU (5 layers) Metrics:\n","Training Time: 46.0388 sec, Validation Time: 0.0335 sec, Testing Time: 0.0307 sec\n","Training set metrics:\n","MAE: 0.0162, MSE: 0.0004, RMSE: 0.0207, R²: 0.9975, MAPE: 8.46%\n","Validation set metrics:\n","MAE: 0.1381, MSE: 0.0204, RMSE: 0.1429, R²: -2.4712, MAPE: 7.83%\n","Test set metrics:\n","MAE: 0.2879, MSE: 0.0852, RMSE: 0.2918, R²: -13.1109, MAPE: 14.21%\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiGRUModel, self).__init__()\n","        self.bigru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-GRU has 2x hidden_dim output\n","\n","    def forward(self, x):\n","        out, _ = self.bigru(x)\n","        out = self.fc(out[:, -1, :])  # Take the last time step output\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Bi-GRU Configurations (2, 3, and 5 layers)\n","bigru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store results\n","bigru_results = {}\n","\n","for num_layers in bigru_layers:\n","    print(f\"\\nTraining Bi-GRU with {num_layers} layers...\")\n","    model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_train = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    end_train = time.time()\n","    train_time = end_train - start_train\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        start_val = time.time()\n","        Y_train_pred = model(X_train_torch).numpy()\n","        end_val = time.time()\n","        train_eval_time = end_val - start_val\n","\n","        start_val = time.time()\n","        Y_val_pred = model(X_val_torch).numpy()\n","        end_val = time.time()\n","        val_time = end_val - start_val\n","\n","        start_test = time.time()\n","        Y_test_pred = model(X_test_torch).numpy()\n","        end_test = time.time()\n","        test_time = end_test - start_test\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store results\n","    bigru_results[num_layers] = {\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics,\n","        \"train_time\": train_time,\n","        \"val_time\": val_time,\n","        \"test_time\": test_time\n","    }\n","\n","    # Print Results\n","    print(f\"\\nBi-GRU ({num_layers} layers) Metrics:\")\n","    print(f\"Training Time: {train_time:.4f} sec, Validation Time: {val_time:.4f} sec, Testing Time: {test_time:.4f} sec\")\n","\n","    print(\"Training set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"Validation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"Test set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"0iE1oF16ysWG"},"source":["##optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_snAMw0mzrON","outputId":"49eaac3d-a84a-40b1-cdf3-6b0099ff1afd"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 20:19:46,155] A new study created in memory with name: no-name-68db40db-a73f-4b4d-bbc9-de3d8e2dbd32\n"]},{"name":"stdout","output_type":"stream","text":["\n","Optimizing Bi-GRU with 2 layers using Optuna...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 20:20:06,031] Trial 0 finished with value: 0.04276478155783232 and parameters: {'hidden_dim': 128, 'lr': 0.00047385684669914047}. Best is trial 0 with value: 0.04276478155783232.\n","[I 2025-03-04 20:20:16,481] Trial 1 finished with value: 0.04748089197538464 and parameters: {'hidden_dim': 95, 'lr': 0.0008655915347467487}. Best is trial 0 with value: 0.04276478155783232.\n","[I 2025-03-04 20:20:38,375] Trial 2 finished with value: 0.28997280043091755 and parameters: {'hidden_dim': 107, 'lr': 0.00023826385618674396}. Best is trial 0 with value: 0.04276478155783232.\n","[I 2025-03-04 20:20:41,407] Trial 3 finished with value: 0.030883545493066784 and parameters: {'hidden_dim': 39, 'lr': 0.0014561955409058717}. Best is trial 3 with value: 0.030883545493066784.\n","[I 2025-03-04 20:20:49,996] Trial 4 finished with value: 0.0063229084341492724 and parameters: {'hidden_dim': 89, 'lr': 0.0009974013501968004}. Best is trial 4 with value: 0.0063229084341492724.\n","[I 2025-03-04 20:20:56,557] Trial 5 finished with value: 1.8515597867676798 and parameters: {'hidden_dim': 49, 'lr': 0.00011906723596858083}. Best is trial 4 with value: 0.0063229084341492724.\n","[I 2025-03-04 20:20:58,298] Trial 6 finished with value: 0.012956571228328126 and parameters: {'hidden_dim': 43, 'lr': 0.003968578774601371}. Best is trial 4 with value: 0.0063229084341492724.\n","[I 2025-03-04 20:21:02,571] Trial 7 finished with value: 0.005933983659876792 and parameters: {'hidden_dim': 102, 'lr': 0.006534894128857922}. Best is trial 7 with value: 0.005933983659876792.\n","[I 2025-03-04 20:21:04,031] Trial 8 finished with value: 0.08300691183865205 and parameters: {'hidden_dim': 38, 'lr': 0.003975857366330145}. Best is trial 7 with value: 0.005933983659876792.\n","[I 2025-03-04 20:21:06,906] Trial 9 finished with value: 0.058739028941438794 and parameters: {'hidden_dim': 42, 'lr': 0.0017298774737769462}. Best is trial 7 with value: 0.005933983659876792.\n"]},{"name":"stdout","output_type":"stream","text":["Best Params for 2 layers - Hidden Dim: 102, LR: 0.006534894128857922\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 20:21:12,051] A new study created in memory with name: no-name-afa2f484-006e-4f60-affe-bbc1853fd904\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping at epoch 14\n","\n","Bi-GRU (2 layers) Metrics:\n","Training Time: 4.9525 sec, Validation Time: 0.0245 sec, Testing Time: 0.0243 sec\n","Training set metrics:\n","MAE: 0.0542, MSE: 0.0034, RMSE: 0.0582, R²: 0.9806, MAPE: 23.49%\n","Validation set metrics:\n","MAE: 0.1188, MSE: 0.0141, RMSE: 0.1188, R²: -1.3998, MAPE: 6.81%\n","Test set metrics:\n","MAE: 0.1092, MSE: 0.0120, RMSE: 0.1094, R²: -0.9823, MAPE: 5.44%\n","\n","Optimizing Bi-GRU with 3 layers using Optuna...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 20:21:22,155] Trial 0 finished with value: 1.3185904859291566 and parameters: {'hidden_dim': 43, 'lr': 0.00027307363471523646}. Best is trial 0 with value: 1.3185904859291566.\n","[I 2025-03-04 20:21:29,372] Trial 1 finished with value: 0.03068797487805392 and parameters: {'hidden_dim': 124, 'lr': 0.0036765062962728526}. Best is trial 1 with value: 0.03068797487805392.\n","[I 2025-03-04 20:21:45,903] Trial 2 finished with value: 1.8772052928880374 and parameters: {'hidden_dim': 68, 'lr': 0.0001298861131032277}. Best is trial 1 with value: 0.03068797487805392.\n","[I 2025-03-04 20:21:53,117] Trial 3 finished with value: 0.15707647172471972 and parameters: {'hidden_dim': 78, 'lr': 0.0013300825855939353}. Best is trial 1 with value: 0.03068797487805392.\n","[I 2025-03-04 20:21:59,988] Trial 4 finished with value: 0.1983305395873398 and parameters: {'hidden_dim': 59, 'lr': 0.0013122209839461702}. Best is trial 1 with value: 0.03068797487805392.\n","[I 2025-03-04 20:22:08,614] Trial 5 finished with value: 0.3203927610703391 and parameters: {'hidden_dim': 39, 'lr': 0.0004619103995184617}. Best is trial 1 with value: 0.03068797487805392.\n","[I 2025-03-04 20:22:18,548] Trial 6 finished with value: 0.02116658778703296 and parameters: {'hidden_dim': 119, 'lr': 0.001683849608453156}. Best is trial 6 with value: 0.02116658778703296.\n","[I 2025-03-04 20:22:24,595] Trial 7 finished with value: 0.07583858441870954 and parameters: {'hidden_dim': 82, 'lr': 0.002542700632642131}. Best is trial 6 with value: 0.02116658778703296.\n","[I 2025-03-04 20:22:36,183] Trial 8 finished with value: 0.0875083652682644 and parameters: {'hidden_dim': 90, 'lr': 0.0008387553896539327}. Best is trial 6 with value: 0.02116658778703296.\n","[I 2025-03-04 20:23:00,750] Trial 9 finished with value: 0.16193852643201218 and parameters: {'hidden_dim': 105, 'lr': 0.00034847466924835144}. Best is trial 6 with value: 0.02116658778703296.\n"]},{"name":"stdout","output_type":"stream","text":["Best Params for 3 layers - Hidden Dim: 119, LR: 0.001683849608453156\n","Early stopping at epoch 38\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 20:23:33,693] A new study created in memory with name: no-name-249fc6bd-224d-4090-bee1-9302a41a6300\n"]},{"name":"stdout","output_type":"stream","text":["\n","Bi-GRU (3 layers) Metrics:\n","Training Time: 32.5477 sec, Validation Time: 0.0511 sec, Testing Time: 0.0519 sec\n","Training set metrics:\n","MAE: 0.0411, MSE: 0.0029, RMSE: 0.0535, R²: 0.9836, MAPE: 16.98%\n","Validation set metrics:\n","MAE: 0.1749, MSE: 0.0309, RMSE: 0.1757, R²: -4.2476, MAPE: 10.00%\n","Test set metrics:\n","MAE: 0.2406, MSE: 0.0583, RMSE: 0.2415, R²: -8.6664, MAPE: 11.91%\n","\n","Optimizing Bi-GRU with 5 layers using Optuna...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 20:24:33,495] Trial 0 finished with value: 0.0003153825384677211 and parameters: {'hidden_dim': 107, 'lr': 0.0003241094593134622}. Best is trial 0 with value: 0.0003153825384677211.\n","[I 2025-03-04 20:25:18,335] Trial 1 finished with value: 0.6301005758473104 and parameters: {'hidden_dim': 108, 'lr': 0.0002560374555748744}. Best is trial 0 with value: 0.0003153825384677211.\n","[I 2025-03-04 20:25:25,651] Trial 2 finished with value: 0.0036489375491680584 and parameters: {'hidden_dim': 66, 'lr': 0.008083945652754354}. Best is trial 0 with value: 0.0003153825384677211.\n","[I 2025-03-04 20:25:43,041] Trial 3 finished with value: 0.0027482306434478327 and parameters: {'hidden_dim': 115, 'lr': 0.0043033707116304455}. Best is trial 0 with value: 0.0003153825384677211.\n","[I 2025-03-04 20:26:03,401] Trial 4 finished with value: 0.0014118529737819937 and parameters: {'hidden_dim': 98, 'lr': 0.0022106799902418864}. Best is trial 0 with value: 0.0003153825384677211.\n","[I 2025-03-04 20:26:22,005] Trial 5 finished with value: 0.47847972984396325 and parameters: {'hidden_dim': 122, 'lr': 0.0011190617024419112}. Best is trial 0 with value: 0.0003153825384677211.\n","[I 2025-03-04 20:26:32,551] Trial 6 finished with value: 0.021577076945182093 and parameters: {'hidden_dim': 124, 'lr': 0.005096226146766021}. Best is trial 0 with value: 0.0003153825384677211.\n","[I 2025-03-04 20:26:35,087] Trial 7 finished with value: 0.6561505235054956 and parameters: {'hidden_dim': 32, 'lr': 0.0032991753529822607}. Best is trial 0 with value: 0.0003153825384677211.\n","[I 2025-03-04 20:26:52,046] Trial 8 finished with value: 0.5535019879158055 and parameters: {'hidden_dim': 115, 'lr': 0.0012576631724731395}. Best is trial 0 with value: 0.0003153825384677211.\n","[I 2025-03-04 20:27:01,726] Trial 9 finished with value: 0.5287108448712767 and parameters: {'hidden_dim': 82, 'lr': 0.0018035268242490268}. Best is trial 0 with value: 0.0003153825384677211.\n"]},{"name":"stdout","output_type":"stream","text":["Best Params for 5 layers - Hidden Dim: 107, LR: 0.0003241094593134622\n","Early stopping at epoch 64\n","\n","Bi-GRU (5 layers) Metrics:\n","Training Time: 78.5571 sec, Validation Time: 0.0783 sec, Testing Time: 0.0773 sec\n","Training set metrics:\n","MAE: 0.0591, MSE: 0.0044, RMSE: 0.0660, R²: 0.9751, MAPE: 18.94%\n","Validation set metrics:\n","MAE: 0.1790, MSE: 0.0323, RMSE: 0.1796, R²: -4.4870, MAPE: 10.23%\n","Test set metrics:\n","MAE: 0.2128, MSE: 0.0453, RMSE: 0.2128, R²: -6.5069, MAPE: 10.56%\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import optuna\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiGRUModel, self).__init__()\n","        self.bigru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-GRU has 2x hidden_dim output\n","\n","    def forward(self, x):\n","        out, _ = self.bigru(x)\n","        out = self.fc(out[:, -1, :])  # Take the last time step output\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Bi-GRU Configurations (2, 3, and 5 layers)\n","bigru_layers = [2, 3, 5]\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Optuna Optimization Function\n","def objective(trial, num_layers):\n","    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n","    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n","\n","    model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    num_epochs = 50  # Reduce epochs for tuning\n","    patience = 5  # Early stopping patience\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","\n","    # Training\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).numpy()\n","            val_loss = mean_squared_error(Y_val, Y_val_pred)\n","\n","        # Early Stopping Check\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= patience:\n","            break  # Stop training\n","\n","    return best_val_loss  # Return MSE as the objective\n","\n","# Dictionary to store results\n","bigru_results = {}\n","\n","for num_layers in bigru_layers:\n","    print(f\"\\nOptimizing Bi-GRU with {num_layers} layers using Optuna...\")\n","\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective(trial, num_layers), n_trials=10)\n","\n","    best_params = study.best_params\n","    hidden_dim = best_params[\"hidden_dim\"]\n","    lr = best_params[\"lr\"]\n","\n","    print(f\"Best Params for {num_layers} layers - Hidden Dim: {hidden_dim}, LR: {lr}\")\n","\n","    # Train final model with best params\n","    model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    num_epochs = 100\n","    patience = 10  # Early stopping patience\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","\n","    start_train = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).numpy()\n","            val_loss = mean_squared_error(Y_val, Y_val_pred)\n","\n","        # Early Stopping Check\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= patience:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break  # Stop training\n","\n","    end_train = time.time()\n","    train_time = end_train - start_train\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        start_val = time.time()\n","        Y_train_pred = model(X_train_torch).numpy()\n","        end_val = time.time()\n","        train_eval_time = end_val - start_val\n","\n","        start_val = time.time()\n","        Y_val_pred = model(X_val_torch).numpy()\n","        end_val = time.time()\n","        val_time = end_val - start_val\n","\n","        start_test = time.time()\n","        Y_test_pred = model(X_test_torch).numpy()\n","        end_test = time.time()\n","        test_time = end_test - start_test\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store results\n","    bigru_results[num_layers] = {\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics,\n","        \"train_time\": train_time,\n","        \"val_time\": val_time,\n","        \"test_time\": test_time\n","    }\n","\n","    # Print Results\n","    print(f\"\\nBi-GRU ({num_layers} layers) Metrics:\")\n","    print(f\"Training Time: {train_time:.4f} sec, Validation Time: {val_time:.4f} sec, Testing Time: {test_time:.4f} sec\")\n","\n","    print(\"Training set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"Validation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"Test set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"ae-oHV_v2IZT"},"source":["##bohb"]},{"cell_type":"code","source":[],"metadata":{"id":"7FXQtT7U9R5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiGRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Bi-GRU Configuration\n","num_layers = 2\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training Bi-GRU with {num_layers} layers...\")\n","\n","gru_model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","# Train Bi-GRU\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","gru_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = gru_model(X_train_torch).cpu().numpy()\n","    val_features = gru_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = gru_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for Bi-GRU\n","class GRUWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = BiGRUModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"bigru_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = GRUWorker(nameserver=\"127.0.0.2\", run_id=\"bigru_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"bigru_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best Bi-GRU Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_gru_model = BiGRUModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_gru_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_gru_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_gru_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_gru_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_gru_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-61RbvFAxqk0","outputId":"169e5ff1-4d6c-437e-a0ca-9efcb0b424e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training Bi-GRU with 2 layers...\n","Train Metrics: (0.005007714012645372, 4.116270561462868e-05, 0.0064158168314431075, 0.9997648127702792, 2.2535764955229745) Time: 1.331376075744629\n","Validation Metrics: (0.0033320840040154723, 1.4516831285590805e-05, 0.003810095968028995, 0.997531799017616, 0.1912401845152856) Time: 0.4534919261932373\n","Test Metrics: (0.0016764540180921577, 5.003406773665728e-06, 0.0022368296255338106, 0.999170944753001, 0.08511241731595702) Time: 0.021201610565185547\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiGRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Bi-GRU Configuration\n","num_layers = 3\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training Bi-GRU with {num_layers} layers...\")\n","\n","gru_model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","# Train Bi-GRU\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","gru_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = gru_model(X_train_torch).cpu().numpy()\n","    val_features = gru_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = gru_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for Bi-GRU\n","class GRUWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = BiGRUModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"bigru_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = GRUWorker(nameserver=\"127.0.0.2\", run_id=\"bigru_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"bigru_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best Bi-GRU Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_gru_model = BiGRUModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_gru_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_gru_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_gru_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_gru_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_gru_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JvfkMXn9xrje","outputId":"a2d7ee17-6efc-4055-c6ec-8f6d49662acf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training Bi-GRU with 3 layers...\n","Train Metrics: (0.005091470353882686, 4.243519109054671e-05, 0.006514229892362312, 0.999757542297421, 2.0870381641472653) Time: 1.7144381999969482\n","Validation Metrics: (0.001934198954771722, 6.690547274931037e-06, 0.0025866092234682527, 0.9988624504182905, 0.11237856178622142) Time: 0.7629313468933105\n","Test Metrics: (0.00381437389523579, 1.8933714231941666e-05, 0.004351288801256665, 0.9968627185757132, 0.18627158840023805) Time: 0.00475621223449707\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiGRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Bi-GRU Configuration\n","num_layers = 5\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training Bi-GRU with {num_layers} layers...\")\n","\n","gru_model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","# Train Bi-GRU\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","gru_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = gru_model(X_train_torch).cpu().numpy()\n","    val_features = gru_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = gru_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for Bi-GRU\n","class GRUWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = BiGRUModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"bigru_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = GRUWorker(nameserver=\"127.0.0.2\", run_id=\"bigru_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"bigru_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best Bi-GRU Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_gru_model = BiGRUModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_gru_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_gru_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_gru_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_gru_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_gru_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ixGT2kfsyLMM","outputId":"a83c6640-7cf2-4c2e-ee46-021603a37eda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training Bi-GRU with 5 layers...\n","Train Metrics: (0.004611744691708896, 3.379035251465287e-05, 0.0058129469733219545, 0.9998069354460416, 2.3584148188619536) Time: 3.2453372478485107\n","Validation Metrics: (0.002378742380399711, 8.82542432370317e-06, 0.0029707615730151034, 0.9984994713682909, 0.1367844193764174) Time: 0.8540260791778564\n","Test Metrics: (0.01125220103949969, 0.00014544060327252548, 0.01205987575692741, 0.975900761076546, 0.551014345291094) Time: 0.0076694488525390625\n"]}]},{"cell_type":"markdown","metadata":{"id":"gS33VlPQ8Bo3"},"source":["#GRU"]},{"cell_type":"markdown","metadata":{"id":"i9YG7pl38Gp2"},"source":["##initial"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hFUkim18kZi","outputId":"66db34e5-ecb7-4047-9a43-e13858f35f41"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training GRU with 2 layers...\n","\n","GRU (2 layers) Metrics:\n","Training Time: 9.9894 sec, Validation Time: 0.0057 sec, Testing Time: 0.0050 sec\n","Training set metrics:\n","MAE: 0.0103, MSE: 0.0001, RMSE: 0.0121, R²: 0.9992, MAPE: 4.89%\n","Validation set metrics:\n","MAE: 0.0167, MSE: 0.0003, RMSE: 0.0170, R²: 0.9511, MAPE: 0.96%\n","Test set metrics:\n","MAE: 0.0053, MSE: 0.0000, RMSE: 0.0067, R²: 0.9925, MAPE: 0.27%\n","\n","Training GRU with 3 layers...\n","\n","GRU (3 layers) Metrics:\n","Training Time: 11.1716 sec, Validation Time: 0.0084 sec, Testing Time: 0.0075 sec\n","Training set metrics:\n","MAE: 0.0136, MSE: 0.0003, RMSE: 0.0183, R²: 0.9981, MAPE: 9.37%\n","Validation set metrics:\n","MAE: 0.0167, MSE: 0.0004, RMSE: 0.0195, R²: 0.9354, MAPE: 0.93%\n","Test set metrics:\n","MAE: 0.0671, MSE: 0.0048, RMSE: 0.0695, R²: 0.2004, MAPE: 3.30%\n","\n","Training GRU with 5 layers...\n","\n","GRU (5 layers) Metrics:\n","Training Time: 18.1904 sec, Validation Time: 0.0149 sec, Testing Time: 0.0134 sec\n","Training set metrics:\n","MAE: 0.0246, MSE: 0.0010, RMSE: 0.0310, R²: 0.9945, MAPE: 13.77%\n","Validation set metrics:\n","MAE: 0.1735, MSE: 0.0321, RMSE: 0.1791, R²: -4.4566, MAPE: 9.84%\n","Test set metrics:\n","MAE: 0.3500, MSE: 0.1255, RMSE: 0.3543, R²: -19.7942, MAPE: 17.28%\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])  # Take the last time step output\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# GRU Configurations (2, 3, and 5 layers)\n","gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store results\n","gru_results = {}\n","\n","for num_layers in gru_layers:\n","    print(f\"\\nTraining GRU with {num_layers} layers...\")\n","    model = GRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_train = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    end_train = time.time()\n","    train_time = end_train - start_train\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        start_val = time.time()\n","        Y_train_pred = model(X_train_torch).numpy()\n","        end_val = time.time()\n","        train_eval_time = end_val - start_val\n","\n","        start_val = time.time()\n","        Y_val_pred = model(X_val_torch).numpy()\n","        end_val = time.time()\n","        val_time = end_val - start_val\n","\n","        start_test = time.time()\n","        Y_test_pred = model(X_test_torch).numpy()\n","        end_test = time.time()\n","        test_time = end_test - start_test\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store results\n","    gru_results[num_layers] = {\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics,\n","        \"train_time\": train_time,\n","        \"val_time\": val_time,\n","        \"test_time\": test_time\n","    }\n","\n","    # Print Results\n","    print(f\"\\nGRU ({num_layers} layers) Metrics:\")\n","    print(f\"Training Time: {train_time:.4f} sec, Validation Time: {val_time:.4f} sec, Testing Time: {test_time:.4f} sec\")\n","\n","    print(\"Training set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"Validation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"Test set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"xhUGYyfL8PpO"},"source":["##optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6dGeIVfQ83aV","outputId":"b79c9444-818a-4ab0-e39c-227dfc757213"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 20:58:42,464] A new study created in memory with name: no-name-63aa4ac4-d752-4afd-acf9-dc72c2f2a6c9\n"]},{"name":"stdout","output_type":"stream","text":["\n","Optimizing GRU with 2 layers using Optuna...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 20:58:49,573] Trial 0 finished with value: 1.0887247383243146 and parameters: {'hidden_dim': 58, 'lr': 0.0004365971741478734}. Best is trial 0 with value: 1.0887247383243146.\n","[I 2025-03-04 20:58:53,965] Trial 1 finished with value: 2.8231522831859666 and parameters: {'hidden_dim': 67, 'lr': 0.00010251456258287766}. Best is trial 0 with value: 1.0887247383243146.\n","[I 2025-03-04 20:58:55,029] Trial 2 finished with value: 0.053504192020766096 and parameters: {'hidden_dim': 63, 'lr': 0.007600775894121467}. Best is trial 2 with value: 0.053504192020766096.\n","[I 2025-03-04 20:59:06,089] Trial 3 finished with value: 0.5498890948784043 and parameters: {'hidden_dim': 103, 'lr': 0.0004268690475335824}. Best is trial 2 with value: 0.053504192020766096.\n","[I 2025-03-04 20:59:10,518] Trial 4 finished with value: 0.4443727174215173 and parameters: {'hidden_dim': 67, 'lr': 0.0005375158309609654}. Best is trial 2 with value: 0.053504192020766096.\n","[I 2025-03-04 20:59:12,055] Trial 5 finished with value: 0.05181705236216426 and parameters: {'hidden_dim': 44, 'lr': 0.0035174356404429133}. Best is trial 5 with value: 0.05181705236216426.\n","[I 2025-03-04 20:59:22,223] Trial 6 finished with value: 0.09342023330942417 and parameters: {'hidden_dim': 118, 'lr': 0.00047599827507897944}. Best is trial 5 with value: 0.05181705236216426.\n","[I 2025-03-04 20:59:33,865] Trial 7 finished with value: 2.07855102582763 and parameters: {'hidden_dim': 121, 'lr': 0.00012518493524926586}. Best is trial 5 with value: 0.05181705236216426.\n","[I 2025-03-04 20:59:36,340] Trial 8 finished with value: 0.013525917232595571 and parameters: {'hidden_dim': 92, 'lr': 0.0034761110548771684}. Best is trial 8 with value: 0.013525917232595571.\n","[I 2025-03-04 20:59:45,821] Trial 9 finished with value: 0.08781676804561592 and parameters: {'hidden_dim': 106, 'lr': 0.0005768647196035838}. Best is trial 8 with value: 0.013525917232595571.\n"]},{"name":"stdout","output_type":"stream","text":["Best Params for 2 layers - Hidden Dim: 92, LR: 0.0034761110548771684\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 20:59:51,804] A new study created in memory with name: no-name-f0a5c9ea-2315-4ceb-b28b-9fc18e744e16\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping at epoch 44\n","\n","GRU (2 layers) Metrics:\n","Training Time: 5.8670 sec, Validation Time: 0.0145 sec, Testing Time: 0.0135 sec\n","Training set metrics:\n","MAE: 0.0278, MSE: 0.0010, RMSE: 0.0322, R²: 0.9941, MAPE: 14.50%\n","Validation set metrics:\n","MAE: 0.0730, MSE: 0.0054, RMSE: 0.0734, R²: 0.0828, MAPE: 4.17%\n","Test set metrics:\n","MAE: 0.1092, MSE: 0.0121, RMSE: 0.1099, R²: -1.0011, MAPE: 5.40%\n","\n","Optimizing GRU with 3 layers using Optuna...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 21:00:00,041] Trial 0 finished with value: 0.22416892200849467 and parameters: {'hidden_dim': 95, 'lr': 0.0009663983849811578}. Best is trial 0 with value: 0.22416892200849467.\n","[I 2025-03-04 21:00:01,592] Trial 1 finished with value: 0.21875738801578462 and parameters: {'hidden_dim': 68, 'lr': 0.006383513532426767}. Best is trial 1 with value: 0.21875738801578462.\n","[I 2025-03-04 21:00:12,435] Trial 2 finished with value: 0.18972829069943353 and parameters: {'hidden_dim': 94, 'lr': 0.0006319151127244248}. Best is trial 2 with value: 0.18972829069943353.\n","[I 2025-03-04 21:00:23,199] Trial 3 finished with value: 2.787831725228161 and parameters: {'hidden_dim': 85, 'lr': 0.00012640158006290098}. Best is trial 2 with value: 0.18972829069943353.\n","[I 2025-03-04 21:00:27,611] Trial 4 finished with value: 0.9380279766680859 and parameters: {'hidden_dim': 46, 'lr': 0.0005912196846296438}. Best is trial 2 with value: 0.18972829069943353.\n","[I 2025-03-04 21:00:31,026] Trial 5 finished with value: 0.0003288184109610975 and parameters: {'hidden_dim': 44, 'lr': 0.0020756744713393807}. Best is trial 5 with value: 0.0003288184109610975.\n","[I 2025-03-04 21:00:33,444] Trial 6 finished with value: 0.001468540388597611 and parameters: {'hidden_dim': 56, 'lr': 0.008825459890565495}. Best is trial 5 with value: 0.0003288184109610975.\n","[I 2025-03-04 21:00:46,483] Trial 7 finished with value: 0.2548580558602678 and parameters: {'hidden_dim': 102, 'lr': 0.0004132273409180906}. Best is trial 5 with value: 0.0003288184109610975.\n","[I 2025-03-04 21:00:53,466] Trial 8 finished with value: 2.817573556741135 and parameters: {'hidden_dim': 59, 'lr': 0.0001903582465902115}. Best is trial 5 with value: 0.0003288184109610975.\n","[I 2025-03-04 21:00:56,414] Trial 9 finished with value: 0.12463189667581248 and parameters: {'hidden_dim': 108, 'lr': 0.004337612006846535}. Best is trial 5 with value: 0.0003288184109610975.\n"]},{"name":"stdout","output_type":"stream","text":["Best Params for 3 layers - Hidden Dim: 44, LR: 0.0020756744713393807\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 21:00:59,012] A new study created in memory with name: no-name-779a7e1f-f581-4619-a514-570d0cf51955\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping at epoch 31\n","\n","GRU (3 layers) Metrics:\n","Training Time: 2.5442 sec, Validation Time: 0.0064 sec, Testing Time: 0.0059 sec\n","Training set metrics:\n","MAE: 0.1817, MSE: 0.0525, RMSE: 0.2291, R²: 0.7002, MAPE: 70.98%\n","Validation set metrics:\n","MAE: 0.7229, MSE: 0.5249, RMSE: 0.7245, R²: -88.2471, MAPE: 41.35%\n","Test set metrics:\n","MAE: 0.8987, MSE: 0.8103, RMSE: 0.9002, R²: -133.2642, MAPE: 44.55%\n","\n","Optimizing GRU with 5 layers using Optuna...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 21:01:06,884] Trial 0 finished with value: 0.5464111248523909 and parameters: {'hidden_dim': 80, 'lr': 0.001237332019126339}. Best is trial 0 with value: 0.5464111248523909.\n","[I 2025-03-04 21:01:12,682] Trial 1 finished with value: 0.7107227658942579 and parameters: {'hidden_dim': 94, 'lr': 0.0022369337814471014}. Best is trial 0 with value: 0.5464111248523909.\n","[I 2025-03-04 21:01:18,987] Trial 2 finished with value: 0.7845136049453029 and parameters: {'hidden_dim': 69, 'lr': 0.001743064638692117}. Best is trial 0 with value: 0.5464111248523909.\n","[I 2025-03-04 21:01:21,771] Trial 3 finished with value: 0.7462748065506668 and parameters: {'hidden_dim': 69, 'lr': 0.005577133069270635}. Best is trial 0 with value: 0.5464111248523909.\n","[I 2025-03-04 21:01:26,933] Trial 4 finished with value: 0.6878861455426525 and parameters: {'hidden_dim': 72, 'lr': 0.001839004351255331}. Best is trial 0 with value: 0.5464111248523909.\n","[I 2025-03-04 21:01:32,272] Trial 5 finished with value: 0.33559391192520005 and parameters: {'hidden_dim': 113, 'lr': 0.005088443266682438}. Best is trial 5 with value: 0.33559391192520005.\n","[I 2025-03-04 21:01:34,065] Trial 6 finished with value: 0.8431763279977857 and parameters: {'hidden_dim': 37, 'lr': 0.004411464452652811}. Best is trial 5 with value: 0.33559391192520005.\n","[I 2025-03-04 21:01:36,775] Trial 7 finished with value: 0.9337678647466627 and parameters: {'hidden_dim': 44, 'lr': 0.002686343987428838}. Best is trial 5 with value: 0.33559391192520005.\n","[I 2025-03-04 21:01:38,761] Trial 8 finished with value: 0.9028857531241578 and parameters: {'hidden_dim': 61, 'lr': 0.008340268365514282}. Best is trial 5 with value: 0.33559391192520005.\n","[I 2025-03-04 21:01:43,912] Trial 9 finished with value: 0.21056051387377728 and parameters: {'hidden_dim': 111, 'lr': 0.007526532122427158}. Best is trial 9 with value: 0.21056051387377728.\n"]},{"name":"stdout","output_type":"stream","text":["Best Params for 5 layers - Hidden Dim: 111, LR: 0.007526532122427158\n","Early stopping at epoch 21\n","\n","GRU (5 layers) Metrics:\n","Training Time: 11.1137 sec, Validation Time: 0.0556 sec, Testing Time: 0.0544 sec\n","Training set metrics:\n","MAE: 0.0956, MSE: 0.0158, RMSE: 0.1257, R²: 0.9097, MAPE: 63.77%\n","Validation set metrics:\n","MAE: 0.3774, MSE: 0.1467, RMSE: 0.3830, R²: -23.9459, MAPE: 21.49%\n","Test set metrics:\n","MAE: 0.6167, MSE: 0.3854, RMSE: 0.6208, R²: -62.8631, MAPE: 30.51%\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import optuna\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# GRU Configurations (2, 3, and 5 layers)\n","gru_layers = [2, 3, 5]\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Optuna Optimization Function\n","def objective(trial, num_layers):\n","    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n","    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n","\n","    model = GRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    num_epochs = 50\n","    patience = 5\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).numpy()\n","            val_loss = mean_squared_error(Y_val, Y_val_pred)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= patience:\n","            break\n","\n","    return best_val_loss\n","\n","# Dictionary to store results\n","gru_results = {}\n","\n","for num_layers in gru_layers:\n","    print(f\"\\nOptimizing GRU with {num_layers} layers using Optuna...\")\n","\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective(trial, num_layers), n_trials=10)\n","\n","    best_params = study.best_params\n","    hidden_dim = best_params[\"hidden_dim\"]\n","    lr = best_params[\"lr\"]\n","\n","    print(f\"Best Params for {num_layers} layers - Hidden Dim: {hidden_dim}, LR: {lr}\")\n","\n","    model = GRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    num_epochs = 100\n","    patience = 10\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","\n","    start_train = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).numpy()\n","            val_loss = mean_squared_error(Y_val, Y_val_pred)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= patience:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    end_train = time.time()\n","    train_time = end_train - start_train\n","\n","    model.eval()\n","    with torch.no_grad():\n","        start_val = time.time()\n","        Y_train_pred = model(X_train_torch).numpy()\n","        end_val = time.time()\n","        train_eval_time = end_val - start_val\n","\n","        start_val = time.time()\n","        Y_val_pred = model(X_val_torch).numpy()\n","        end_val = time.time()\n","        val_time = end_val - start_val\n","\n","        start_test = time.time()\n","        Y_test_pred = model(X_test_torch).numpy()\n","        end_test = time.time()\n","        test_time = end_test - start_test\n","\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    gru_results[num_layers] = {\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics,\n","        \"train_time\": train_time,\n","        \"val_time\": val_time,\n","        \"test_time\": test_time\n","    }\n","\n","    print(f\"\\nGRU ({num_layers} layers) Metrics:\")\n","    print(f\"Training Time: {train_time:.4f} sec, Validation Time: {val_time:.4f} sec, Testing Time: {test_time:.4f} sec\")\n","\n","    print(\"Training set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"Validation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"Test set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"_WXw8fNI83yS"},"source":["##bohb"]},{"cell_type":"code","source":[],"metadata":{"id":"9802VNoj9G92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# GRU Configuration\n","num_layers = 2\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training GRU with {num_layers} layers...\")\n","\n","gru_model = GRUModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","# Train GRU\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","gru_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = gru_model(X_train_torch).cpu().numpy()\n","    val_features = gru_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = gru_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for GRU\n","class GRUWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = GRUModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"gru_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = GRUWorker(nameserver=\"127.0.0.2\", run_id=\"gru_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"gru_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best GRU Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_gru_model = GRUModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_gru_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_gru_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_gru_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_gru_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_gru_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5Tx-V-UyOW4","outputId":"8b4b20eb-cd4d-43cf-e021-4873b8cc7a90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training GRU with 2 layers...\n","Train Metrics: (0.005511521856629336, 4.7527138560193234e-05, 0.006893992932995597, 0.9997284489469868, 2.5668805456825) Time: 0.5485134124755859\n","Validation Metrics: (0.009529767790479763, 9.691288716634495e-05, 0.00984443432434515, 0.9835225416205624, 0.5500259798068133) Time: 0.2537407875061035\n","Test Metrics: (0.003295677504039564, 1.5574333188991714e-05, 0.00394643297028997, 0.9974193618002828, 0.16648237714472428) Time: 0.001786947250366211\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# GRU Configuration\n","num_layers = 3\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training GRU with {num_layers} layers...\")\n","\n","gru_model = GRUModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","# Train GRU\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","gru_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = gru_model(X_train_torch).cpu().numpy()\n","    val_features = gru_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = gru_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for GRU\n","class GRUWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = GRUModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"gru_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = GRUWorker(nameserver=\"127.0.0.2\", run_id=\"gru_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"gru_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best GRU Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_gru_model = GRUModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_gru_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_gru_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_gru_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_gru_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_gru_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6zQrPDGizL8j","outputId":"ff94da95-a6dc-4171-ed14-f7e036df3276"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training GRU with 3 layers...\n","Train Metrics: (0.008872168524849726, 0.00013556922206821897, 0.01164341968960232, 0.9992254117095186, 5.802069227750733) Time: 0.841616153717041\n","Validation Metrics: (0.008790081069104307, 0.00011008569995896415, 0.01049217327148976, 0.9812828552292371, 0.4910651846079351) Time: 0.3597114086151123\n","Test Metrics: (0.0396050589084133, 0.0016991380376535797, 0.041220602102026355, 0.7184559702587692, 1.9456672526222827) Time: 0.0024461746215820312\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# GRU Configuration\n","num_layers = 5\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training GRU with {num_layers} layers...\")\n","\n","gru_model = GRUModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","# Train GRU\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","gru_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = gru_model(X_train_torch).cpu().numpy()\n","    val_features = gru_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = gru_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for GRU\n","class GRUWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = GRUModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"gru_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = GRUWorker(nameserver=\"127.0.0.2\", run_id=\"gru_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"gru_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best GRU Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_gru_model = GRUModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_gru_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_gru_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_gru_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_gru_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_gru_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_gru_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_gru_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWtzM57kzODd","outputId":"8f0b7bb7-90dd-464c-8bca-a02843e700a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training GRU with 5 layers...\n","Train Metrics: (0.003974773970864016, 2.7689223676390275e-05, 0.005262055081086692, 0.9998417948550191, 1.5159134311271476) Time: 1.4235813617706299\n","Validation Metrics: (0.022333713747533547, 0.0005512016538959118, 0.023477684168075687, 0.9062828218587926, 1.2636151050673052) Time: 0.6080272197723389\n","Test Metrics: (0.05380460771715651, 0.0030109898146017575, 0.054872486863652876, 0.5010845575069076, 2.652524697440376) Time: 0.004075050354003906\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"b5qw6EZdzP5A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-IPkYtel-lC5"},"source":["#Bi-lstm"]},{"cell_type":"markdown","metadata":{"id":"EdZpwuGF-qKm"},"source":["##initial"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NiESa1I5_D_9","outputId":"abbe3847-54a3-4305-a911-5ab2176795a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training Bi-LSTM with 2 layers...\n","\n","Bi-LSTM (2 layers) Metrics:\n","Training Time: 43.4122 sec, Validation Time: 0.0422 sec, Testing Time: 0.0252 sec\n","Training set metrics:\n","MAE: 0.0276, MSE: 0.0011, RMSE: 0.0326, R²: 0.9939, MAPE: 12.00%\n","Validation set metrics:\n","MAE: 0.1028, MSE: 0.0108, RMSE: 0.1039, R²: -0.8359, MAPE: 5.86%\n","Test set metrics:\n","MAE: 0.1471, MSE: 0.0218, RMSE: 0.1475, R²: -2.6041, MAPE: 7.29%\n","\n","Training Bi-LSTM with 3 layers...\n","\n","Bi-LSTM (3 layers) Metrics:\n","Training Time: 66.2933 sec, Validation Time: 0.0378 sec, Testing Time: 0.0350 sec\n","Training set metrics:\n","MAE: 0.0354, MSE: 0.0020, RMSE: 0.0449, R²: 0.9885, MAPE: 21.42%\n","Validation set metrics:\n","MAE: 0.0608, MSE: 0.0037, RMSE: 0.0608, R²: 0.3706, MAPE: 3.48%\n","Test set metrics:\n","MAE: 0.0470, MSE: 0.0023, RMSE: 0.0478, R²: 0.6219, MAPE: 2.35%\n","\n","Training Bi-LSTM with 5 layers...\n","\n","Bi-LSTM (5 layers) Metrics:\n","Training Time: 103.7288 sec, Validation Time: 0.0620 sec, Testing Time: 0.0599 sec\n","Training set metrics:\n","MAE: 0.0370, MSE: 0.0020, RMSE: 0.0444, R²: 0.9887, MAPE: 20.82%\n","Validation set metrics:\n","MAE: 0.1920, MSE: 0.0395, RMSE: 0.1986, R²: -5.7076, MAPE: 10.89%\n","Test set metrics:\n","MAE: 0.3886, MSE: 0.1547, RMSE: 0.3933, R²: -24.6327, MAPE: 19.19%\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-LSTM has 2x hidden_dim output\n","\n","    def forward(self, x):\n","        out, _ = self.bilstm(x)\n","        out = self.fc(out[:, -1, :])  # Take the last time step output\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Bi-LSTM Configurations (2, 3, and 5 layers)\n","bilstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store results\n","bilstm_results = {}\n","\n","for num_layers in bilstm_layers:\n","    print(f\"\\nTraining Bi-LSTM with {num_layers} layers...\")\n","    model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_train = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    end_train = time.time()\n","    train_time = end_train - start_train\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        start_val = time.time()\n","        Y_train_pred = model(X_train_torch).numpy()\n","        end_val = time.time()\n","        train_eval_time = end_val - start_val\n","\n","        start_val = time.time()\n","        Y_val_pred = model(X_val_torch).numpy()\n","        end_val = time.time()\n","        val_time = end_val - start_val\n","\n","        start_test = time.time()\n","        Y_test_pred = model(X_test_torch).numpy()\n","        end_test = time.time()\n","        test_time = end_test - start_test\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store results\n","    bilstm_results[num_layers] = {\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics,\n","        \"train_time\": train_time,\n","        \"val_time\": val_time,\n","        \"test_time\": test_time\n","    }\n","\n","    # Print Results\n","    print(f\"\\nBi-LSTM ({num_layers} layers) Metrics:\")\n","    print(f\"Training Time: {train_time:.4f} sec, Validation Time: {val_time:.4f} sec, Testing Time: {test_time:.4f} sec\")\n","\n","    print(\"Training set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"Validation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"Test set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"QK2CKcHi-sYT"},"source":["##optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awP3S5tk_amm","outputId":"a0cd96c9-24df-4808-8e4b-2efc342c22ca"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 21:11:19,312] A new study created in memory with name: no-name-b94eaaec-c6f0-4c2c-a000-51f47cd6f2fd\n"]},{"name":"stdout","output_type":"stream","text":["\n","Optimizing Bi-LSTM with 2 layers using Optuna...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 21:11:35,094] Trial 0 finished with value: 0.021657882054178547 and parameters: {'hidden_dim': 83, 'lr': 0.0018788960595807064}. Best is trial 0 with value: 0.021657882054178547.\n","[I 2025-03-04 21:11:44,863] Trial 1 finished with value: 2.71708527400641 and parameters: {'hidden_dim': 41, 'lr': 0.00016212157902592884}. Best is trial 0 with value: 0.021657882054178547.\n","[I 2025-03-04 21:11:55,911] Trial 2 finished with value: 0.005672096632300654 and parameters: {'hidden_dim': 102, 'lr': 0.004753394127329592}. Best is trial 2 with value: 0.005672096632300654.\n","[I 2025-03-04 21:12:11,846] Trial 3 finished with value: 0.007206656892891607 and parameters: {'hidden_dim': 126, 'lr': 0.003921063567428989}. Best is trial 2 with value: 0.005672096632300654.\n","[I 2025-03-04 21:12:34,054] Trial 4 finished with value: 2.2093088084274832 and parameters: {'hidden_dim': 61, 'lr': 0.0002990756062878816}. Best is trial 2 with value: 0.005672096632300654.\n","[I 2025-03-04 21:13:01,767] Trial 5 finished with value: 1.7194081716819771 and parameters: {'hidden_dim': 76, 'lr': 0.0003002204758729381}. Best is trial 2 with value: 0.005672096632300654.\n","[I 2025-03-04 21:13:26,779] Trial 6 finished with value: 0.07561225682390203 and parameters: {'hidden_dim': 104, 'lr': 0.0011577822756247923}. Best is trial 2 with value: 0.005672096632300654.\n","[I 2025-03-04 21:13:43,502] Trial 7 finished with value: 0.8663388587802392 and parameters: {'hidden_dim': 53, 'lr': 0.0005782927520966509}. Best is trial 2 with value: 0.005672096632300654.\n","[I 2025-03-04 21:13:54,675] Trial 8 finished with value: 0.100015094765062 and parameters: {'hidden_dim': 60, 'lr': 0.0019120937697356649}. Best is trial 2 with value: 0.005672096632300654.\n","[I 2025-03-04 21:14:22,377] Trial 9 finished with value: 1.9811691687100546 and parameters: {'hidden_dim': 80, 'lr': 0.0002367576964827358}. Best is trial 2 with value: 0.005672096632300654.\n"]},{"name":"stdout","output_type":"stream","text":["Best Params for 2 layers - Hidden Dim: 102, LR: 0.004753394127329592\n","Early stopping at epoch 18\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 21:14:37,963] A new study created in memory with name: no-name-f086ee1e-30a2-4963-8981-9f270db14b0a\n"]},{"name":"stdout","output_type":"stream","text":["\n","Bi-LSTM (2 layers) Metrics:\n","Training Time: 15.1255 sec, Validation Time: 0.0504 sec, Testing Time: 0.0505 sec\n","Training set metrics:\n","MAE: 0.0431, MSE: 0.0031, RMSE: 0.0552, R²: 0.9826, MAPE: 16.41%\n","Validation set metrics:\n","MAE: 0.0893, MSE: 0.0080, RMSE: 0.0894, R²: -0.3581, MAPE: 5.13%\n","Test set metrics:\n","MAE: 0.0959, MSE: 0.0092, RMSE: 0.0959, R²: -0.5249, MAPE: 4.76%\n","\n","Optimizing Bi-LSTM with 3 layers using Optuna...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 21:14:52,564] Trial 0 finished with value: 0.1741900518420259 and parameters: {'hidden_dim': 113, 'lr': 0.005678784885241791}. Best is trial 0 with value: 0.1741900518420259.\n","[I 2025-03-04 21:15:03,689] Trial 1 finished with value: 0.07687405039074743 and parameters: {'hidden_dim': 94, 'lr': 0.007311084202160114}. Best is trial 1 with value: 0.07687405039074743.\n","[I 2025-03-04 21:16:11,539] Trial 2 finished with value: 2.589936548033051 and parameters: {'hidden_dim': 93, 'lr': 0.0001626689050523497}. Best is trial 1 with value: 0.07687405039074743.\n","[I 2025-03-04 21:16:31,860] Trial 3 finished with value: 0.16345330987056506 and parameters: {'hidden_dim': 109, 'lr': 0.0025899541069476363}. Best is trial 1 with value: 0.07687405039074743.\n","[I 2025-03-04 21:17:05,439] Trial 4 finished with value: 2.7269363369896493 and parameters: {'hidden_dim': 55, 'lr': 0.0001056798702665625}. Best is trial 1 with value: 0.07687405039074743.\n","[I 2025-03-04 21:18:19,006] Trial 5 finished with value: 0.2215801102432172 and parameters: {'hidden_dim': 122, 'lr': 0.0004669184190107706}. Best is trial 1 with value: 0.07687405039074743.\n","[I 2025-03-04 21:18:28,326] Trial 6 finished with value: 0.0011109912174441948 and parameters: {'hidden_dim': 39, 'lr': 0.00893593651214843}. Best is trial 6 with value: 0.0011109912174441948.\n","[I 2025-03-04 21:18:39,971] Trial 7 finished with value: 0.00017139694252578353 and parameters: {'hidden_dim': 49, 'lr': 0.005508618684517529}. Best is trial 7 with value: 0.00017139694252578353.\n","[I 2025-03-04 21:19:15,117] Trial 8 finished with value: 0.29188967566369267 and parameters: {'hidden_dim': 98, 'lr': 0.0009383125209467539}. Best is trial 7 with value: 0.00017139694252578353.\n","[I 2025-03-04 21:19:24,871] Trial 9 finished with value: 0.0006954629960233997 and parameters: {'hidden_dim': 56, 'lr': 0.005704240492806344}. Best is trial 7 with value: 0.00017139694252578353.\n"]},{"name":"stdout","output_type":"stream","text":["Best Params for 3 layers - Hidden Dim: 49, LR: 0.005508618684517529\n","Early stopping at epoch 38\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 21:19:41,658] A new study created in memory with name: no-name-44f12ac1-47b7-4350-96f5-e90fb1080c19\n"]},{"name":"stdout","output_type":"stream","text":["\n","Bi-LSTM (3 layers) Metrics:\n","Training Time: 16.5129 sec, Validation Time: 0.0310 sec, Testing Time: 0.0295 sec\n","Training set metrics:\n","MAE: 0.0360, MSE: 0.0028, RMSE: 0.0527, R²: 0.9841, MAPE: 18.08%\n","Validation set metrics:\n","MAE: 0.3063, MSE: 0.0965, RMSE: 0.3106, R²: -15.4018, MAPE: 17.44%\n","Test set metrics:\n","MAE: 0.5024, MSE: 0.2559, RMSE: 0.5059, R²: -41.4073, MAPE: 24.84%\n","\n","Optimizing Bi-LSTM with 5 layers using Optuna...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 21:20:16,472] Trial 0 finished with value: 2.791982098146895 and parameters: {'hidden_dim': 39, 'lr': 0.0002283692143369438}. Best is trial 0 with value: 2.791982098146895.\n","[I 2025-03-04 21:20:31,863] Trial 1 finished with value: 0.8072431487809635 and parameters: {'hidden_dim': 64, 'lr': 0.003215649017675925}. Best is trial 1 with value: 0.8072431487809635.\n","[I 2025-03-04 21:23:00,582] Trial 2 finished with value: 2.1036569524272783 and parameters: {'hidden_dim': 124, 'lr': 0.0001537110391373099}. Best is trial 1 with value: 0.8072431487809635.\n","[I 2025-03-04 21:24:25,867] Trial 3 finished with value: 2.530542542366618 and parameters: {'hidden_dim': 88, 'lr': 0.00016602362260273494}. Best is trial 1 with value: 0.8072431487809635.\n","[I 2025-03-04 21:25:44,127] Trial 4 finished with value: 2.5715916100906338 and parameters: {'hidden_dim': 80, 'lr': 0.00020557521300527593}. Best is trial 1 with value: 0.8072431487809635.\n","[I 2025-03-04 21:26:29,814] Trial 5 finished with value: 0.7768899033716322 and parameters: {'hidden_dim': 57, 'lr': 0.00047749649588198543}. Best is trial 5 with value: 0.7768899033716322.\n","[I 2025-03-04 21:26:37,275] Trial 6 finished with value: 0.9970328556222326 and parameters: {'hidden_dim': 34, 'lr': 0.005342466469213497}. Best is trial 5 with value: 0.7768899033716322.\n","[I 2025-03-04 21:27:33,704] Trial 7 finished with value: 0.6381381022901044 and parameters: {'hidden_dim': 100, 'lr': 0.0008938587782179752}. Best is trial 7 with value: 0.6381381022901044.\n","[I 2025-03-04 21:27:55,961] Trial 8 finished with value: 0.004577348596122318 and parameters: {'hidden_dim': 69, 'lr': 0.007939780411485705}. Best is trial 8 with value: 0.004577348596122318.\n","[I 2025-03-04 21:28:10,237] Trial 9 finished with value: 0.8358336899209018 and parameters: {'hidden_dim': 61, 'lr': 0.004861417987330634}. Best is trial 8 with value: 0.004577348596122318.\n"]},{"name":"stdout","output_type":"stream","text":["Best Params for 5 layers - Hidden Dim: 69, LR: 0.007939780411485705\n","Early stopping at epoch 32\n","\n","Bi-LSTM (5 layers) Metrics:\n","Training Time: 44.1649 sec, Validation Time: 0.0843 sec, Testing Time: 0.0851 sec\n","Training set metrics:\n","MAE: 0.0961, MSE: 0.0152, RMSE: 0.1235, R²: 0.9129, MAPE: 44.87%\n","Validation set metrics:\n","MAE: 0.3223, MSE: 0.1095, RMSE: 0.3309, R²: -17.6119, MAPE: 18.30%\n","Test set metrics:\n","MAE: 0.5863, MSE: 0.3496, RMSE: 0.5912, R²: -56.9231, MAPE: 28.98%\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import optuna\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-LSTM has 2x hidden_dim output\n","\n","    def forward(self, x):\n","        out, _ = self.bilstm(x)\n","        out = self.fc(out[:, -1, :])  # Take the last time step output\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Bi-LSTM Configurations (2, 3, and 5 layers)\n","bilstm_layers = [2, 3, 5]\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Optuna Optimization Function\n","def objective(trial, num_layers):\n","    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n","    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n","\n","    model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    num_epochs = 50  # Reduce epochs for tuning\n","    patience = 5  # Early stopping patience\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","\n","    # Training\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).numpy()\n","            val_loss = mean_squared_error(Y_val, Y_val_pred)\n","\n","        # Early Stopping Check\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= patience:\n","            break  # Stop training\n","\n","    return best_val_loss  # Return MSE as the objective\n","\n","# Dictionary to store results\n","bilstm_results = {}\n","\n","for num_layers in bilstm_layers:\n","    print(f\"\\nOptimizing Bi-LSTM with {num_layers} layers using Optuna...\")\n","\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective(trial, num_layers), n_trials=10)\n","\n","    best_params = study.best_params\n","    hidden_dim = best_params[\"hidden_dim\"]\n","    lr = best_params[\"lr\"]\n","\n","    print(f\"Best Params for {num_layers} layers - Hidden Dim: {hidden_dim}, LR: {lr}\")\n","\n","    # Train final model with best params\n","    model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    num_epochs = 100\n","    patience = 10  # Early stopping patience\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","\n","    start_train = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).numpy()\n","            val_loss = mean_squared_error(Y_val, Y_val_pred)\n","\n","        # Early Stopping Check\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= patience:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break  # Stop training\n","\n","    end_train = time.time()\n","    train_time = end_train - start_train\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        start_val = time.time()\n","        Y_train_pred = model(X_train_torch).numpy()\n","        end_val = time.time()\n","        train_eval_time = end_val - start_val\n","\n","        start_val = time.time()\n","        Y_val_pred = model(X_val_torch).numpy()\n","        end_val = time.time()\n","        val_time = end_val - start_val\n","\n","        start_test = time.time()\n","        Y_test_pred = model(X_test_torch).numpy()\n","        end_test = time.time()\n","        test_time = end_test - start_test\n","\n","    # Calculate Metrics\n","    train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","    val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","    test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","    # Store results\n","    bilstm_results[num_layers] = {\n","        \"train_metrics\": train_metrics,\n","        \"val_metrics\": val_metrics,\n","        \"test_metrics\": test_metrics,\n","        \"train_time\": train_time,\n","        \"val_time\": val_time,\n","        \"test_time\": test_time\n","    }\n","\n","    # Print Results\n","    print(f\"\\nBi-LSTM ({num_layers} layers) Metrics:\")\n","    print(f\"Training Time: {train_time:.4f} sec, Validation Time: {val_time:.4f} sec, Testing Time: {test_time:.4f} sec\")\n","\n","    print(\"Training set metrics:\")\n","    print(f\"MAE: {train_metrics[0]:.4f}, MSE: {train_metrics[1]:.4f}, RMSE: {train_metrics[2]:.4f}, R²: {train_metrics[3]:.4f}, MAPE: {train_metrics[4]:.2f}%\")\n","\n","    print(\"Validation set metrics:\")\n","    print(f\"MAE: {val_metrics[0]:.4f}, MSE: {val_metrics[1]:.4f}, RMSE: {val_metrics[2]:.4f}, R²: {val_metrics[3]:.4f}, MAPE: {val_metrics[4]:.2f}%\")\n","\n","    print(\"Test set metrics:\")\n","    print(f\"MAE: {test_metrics[0]:.4f}, MSE: {test_metrics[1]:.4f}, RMSE: {test_metrics[2]:.4f}, R²: {test_metrics[3]:.4f}, MAPE: {test_metrics[4]:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"ct7uj0ts-u10"},"source":["##bohb"]},{"cell_type":"code","source":[],"metadata":{"id":"lOHMmBBFxNl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to device\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Bi-LSTM Configuration\n","num_layers = 2\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training Bi-LSTM with {num_layers} layers on {device}...\")\n","\n","lstm_model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","lstm_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = lstm_model(X_train_torch).cpu().numpy()\n","    val_features = lstm_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = lstm_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for Bi-LSTM\n","class LSTMWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = BiLSTMModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"bilstm_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = LSTMWorker(nameserver=\"127.0.0.2\", run_id=\"bilstm_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"bilstm_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best Bi-LSTM Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_lstm_model = BiLSTMModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_lstm_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_lstm_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_lstm_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_lstm_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_lstm_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XF65kjAXsd_c","outputId":"bf2f7ec6-54e5-467d-af16-b95d495e6fce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Bi-LSTM with 2 layers on cuda...\n","Train Metrics: (0.013367075669218386, 0.00023547239764059268, 0.015345109893402285, 0.998654604937895, 6.27791259514441) Time: 1.8598740100860596\n","Validation Metrics: (0.007389678979745142, 7.599191734494195e-05, 0.008717334302694944, 0.9870795959976334, 0.43431138031895966) Time: 0.6154749393463135\n","Test Metrics: (0.021312206616370958, 0.0005659440443041224, 0.023789578481009755, 0.9062241187529002, 1.038403966800185) Time: 0.004046440124511719\n"]}]},{"cell_type":"code","source":["!pip cache purge import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to device\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Bi-LSTM Configuration\n","num_layers = 3\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training Bi-LSTM with {num_layers} layers on {device}...\")\n","\n","lstm_model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","lstm_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = lstm_model(X_train_torch).cpu().numpy()\n","    val_features = lstm_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = lstm_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for Bi-LSTM\n","class LSTMWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = BiLSTMModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"bilstm_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = LSTMWorker(nameserver=\"127.0.0.2\", run_id=\"bilstm_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"bilstm_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best Bi-LSTM Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_lstm_model = BiLSTMModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_lstm_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_lstm_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_lstm_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_lstm_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_lstm_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHMnIDiPsjRp","outputId":"f31e15a8-c8d9-493a-e735-0e08af375730"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Too many arguments\u001b[0m\u001b[31m\n","\u001b[0mTraining Bi-LSTM with 3 layers on cuda...\n","Train Metrics: (0.007692476869820237, 9.184445922314318e-05, 0.009583551493217072, 0.9994752375091152, 4.438875855072742) Time: 2.431035041809082\n","Validation Metrics: (0.003712549472991549, 2.0555465002813692e-05, 0.004533813516545833, 0.9965050899941461, 0.21256213505118338) Time: 0.9235081672668457\n","Test Metrics: (0.0220077348137445, 0.0005496802590819483, 0.02344526090880518, 0.9089189978791542, 1.0778910712062117) Time: 0.006041288375854492\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to device\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","# Bi-LSTM Configuration\n","num_layers = 5\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","print(f\"Training Bi-LSTM with {num_layers} layers on {device}...\")\n","\n","lstm_model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","num_epochs = 100\n","\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","train_time = time.time() - start_time\n","\n","# Extract Feature Representations\n","lstm_model.eval()\n","with torch.no_grad():\n","    val_start = time.time()\n","    train_features = lstm_model(X_train_torch).cpu().numpy()\n","    val_features = lstm_model(X_val_torch).cpu().numpy()\n","    val_time = time.time() - val_start\n","\n","    test_start = time.time()\n","    test_features = lstm_model(X_test_torch).cpu().numpy()\n","    test_time = time.time() - test_start\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"hidden_dim\", 32, 128, default_value=64))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_layers\", 1, 5, default_value=2))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.0001, 0.01, default_value=0.001, log=True))\n","    return cs\n","\n","# BOHB Worker for Bi-LSTM\n","class LSTMWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = BiLSTMModel(input_dim, config[\"hidden_dim\"], config[\"num_layers\"], output_dim).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","        num_epochs = 100\n","\n","        for epoch in range(num_epochs):\n","            model.train()\n","            optimizer.zero_grad()\n","            outputs = model(X_train_torch)\n","            loss = criterion(outputs, Y_train_torch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            Y_val_pred = model(X_val_torch).cpu().numpy()\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"bilstm_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = LSTMWorker(nameserver=\"127.0.0.2\", run_id=\"bilstm_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"bilstm_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=25)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best Bi-LSTM Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_lstm_model = BiLSTMModel(input_dim, best_params[\"hidden_dim\"], best_params[\"num_layers\"], output_dim).to(device)\n","optimizer = optim.Adam(best_lstm_model.parameters(), lr=best_params[\"learning_rate\"])\n","criterion = nn.MSELoss()\n","\n","for epoch in range(100):\n","    best_lstm_model.train()\n","    optimizer.zero_grad()\n","    outputs = best_lstm_model(X_train_torch)\n","    loss = criterion(outputs, Y_train_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Predictions\n","best_lstm_model.eval()\n","with torch.no_grad():\n","    Y_train_pred = best_lstm_model(X_train_torch).cpu().numpy()\n","    Y_val_pred = best_lstm_model(X_val_torch).cpu().numpy()\n","    Y_test_pred = best_lstm_model(X_test_torch).cpu().numpy()\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdidLZ2Fs6Nz","outputId":"5775f532-70da-4901-fc9a-6514af30dfd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Bi-LSTM with 5 layers on cuda...\n","Train Metrics: (0.009170177659641829, 0.00012204579677572613, 0.011047433945298163, 0.9993026791506011, 4.801268081962811) Time: 4.422967195510864\n","Validation Metrics: (0.004050040822916552, 2.47695520018734e-05, 0.004976901847723481, 0.9957885966033843, 0.22884409916595305) Time: 1.1460261344909668\n","Test Metrics: (0.024649320135596536, 0.0006809122342708871, 0.026094295052192674, 0.8871741023457591, 1.208198731926681) Time: 0.010097503662109375\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"303ErWdxvpjX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U1zQVmEXxmBM"},"source":["# STACKED MODELS - BI-GRU"]},{"cell_type":"markdown","metadata":{"id":"GaUtio4BA-bB"},"source":["#XGBoost"]},{"cell_type":"markdown","metadata":{"id":"IQyCKpKoKBcV"},"source":["## Initial"]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, GRU, Dense\n","import xgboost as xgb\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Enable GPU for TensorFlow\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train a Bi-GRU model on GPU\n","def train_bi_gru(X_train, Y_train, X_val, Y_val, layers):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(Bidirectional(GRU(64, return_sequences=(layers > 1)), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(Bidirectional(GRU(64, return_sequences=(_ < layers - 2))))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        start_time = time.time()\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","        train_time = time.time() - start_time\n","        return model, train_time\n","\n","# Reshaping input for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train Bi-GRU models\n","total_train_time = 0\n","total_val_time = 0\n","total_test_time = 0\n","bi_gru_models = {}\n","bi_gru_predictions = {}\n","\n","times = {}\n","\n","for layers in [2, 3, 5]:\n","    model, train_time = train_bi_gru(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    total_train_time += train_time\n","    Y_train_pred = model.predict(X_train_r)\n","    Y_val_pred = model.predict(X_val_r)\n","    Y_test_pred = model.predict(X_test_r)\n","\n","    bi_gru_models[layers] = model\n","    bi_gru_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","\n","times['Total Bi-GRU Train Time'] = total_train_time\n","\n","# Prepare input for XGBoost\n","X_train_xgb = np.column_stack([bi_gru_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_xgb = np.column_stack([bi_gru_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_xgb = np.column_stack([bi_gru_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train XGBoost model with GPU\n","start_time = time.time()\n","xgb_model = xgb.XGBRegressor(objective='reg:squarederror',\n","                             n_estimators=100,\n","                             learning_rate=0.05,\n","                             max_depth=3,\n","                             tree_method='gpu_hist')\n","xgb_model.fit(X_train_xgb, Y_train)\n","total_train_time += time.time() - start_time\n","\n","times['Total Train Time'] = total_train_time\n","\n","# Predictions from XGBoost\n","start_time = time.time()\n","Y_train_pred_xgb = xgb_model.predict(X_train_xgb)\n","total_val_time += time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_xgb = xgb_model.predict(X_val_xgb)\n","total_val_time += time.time() - start_time\n","\n","times['Total Validation Time'] = total_val_time\n","\n","start_time = time.time()\n","Y_test_pred_xgb = xgb_model.predict(X_test_xgb)\n","total_test_time += time.time() - start_time\n","\n","times['Total Test Time'] = total_test_time\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_xgb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_xgb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_xgb)\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print training times\n","print(\"Execution Times:\")\n","for model, t in times.items():\n","    print(f\"{model}: {t:.2f} seconds\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fuWNuWaKqzud","outputId":"c07fde9b-d673-4c76-c682-ee415437d4ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU activated for TensorFlow!\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","Train Metrics: MAE=0.0042, MSE=0.0000, RMSE=0.0059, R²=0.9998, MAPE=1.51%\n","Validation Metrics: MAE=0.1712, MSE=0.0352, RMSE=0.1876, R²=-4.9858, MAPE=9.63%\n","Test Metrics: MAE=0.4403, MSE=0.1999, RMSE=0.4471, R²=-32.1200, MAPE=21.72%\n","Execution Times:\n","Total Bi-GRU Train Time: 418.21 seconds\n","Total Train Time: 418.73 seconds\n","Total Validation Time: 0.09 seconds\n","Total Test Time: 0.00 seconds\n"]}]},{"cell_type":"markdown","metadata":{"id":"2DTldgzST4fp"},"source":["##Optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IHSMBlypT8Ao","outputId":"114aa595-140d-438c-d660-07c40790e195"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"]}],"source":["!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OZq_53b8Lvji","outputId":"e5dbcdeb-94ee-480e-d067-823802a0effd"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:28:06,313] A new study created in memory with name: no-name-4597e469-6c36-468a-9acc-08df7b8dee2e\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:28:46,793] Trial 0 finished with value: 0.002042252303076748 and parameters: {'gru_units': 64, 'gru_layers': 2, 'gru_learning_rate': 0.004967482323376196, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.25008179909034406, 'xgb_max_depth': 6}. Best is trial 0 with value: 0.002042252303076748.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 38.98 seconds\n","XGBoost Training Time: 0.10 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:29:17,584] Trial 1 finished with value: 0.0020513506840643923 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.0005134501377456693, 'gru_batch_size': 32, 'gru_epochs': 10, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.14497215569106672, 'xgb_max_depth': 6}. Best is trial 0 with value: 0.002042252303076748.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 29.31 seconds\n","XGBoost Training Time: 0.10 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:29:59,753] Trial 2 finished with value: 0.0021081826763026433 and parameters: {'gru_units': 96, 'gru_layers': 2, 'gru_learning_rate': 0.004912791197306664, 'gru_batch_size': 32, 'gru_epochs': 20, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.1502447735940948, 'xgb_max_depth': 3}. Best is trial 0 with value: 0.002042252303076748.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 41.21 seconds\n","XGBoost Training Time: 0.10 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:30:20,647] Trial 3 finished with value: 0.0020167295102244244 and parameters: {'gru_units': 112, 'gru_layers': 3, 'gru_learning_rate': 0.0001242443444070061, 'gru_batch_size': 64, 'gru_epochs': 20, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.08384609499711615, 'xgb_max_depth': 9}. Best is trial 3 with value: 0.0020167295102244244.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 19.26 seconds\n","XGBoost Training Time: 0.26 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:30:35,586] Trial 4 finished with value: 0.0020074486697286755 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.00018160092419953764, 'gru_batch_size': 64, 'gru_epochs': 50, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.18836100588439267, 'xgb_max_depth': 5}. Best is trial 4 with value: 0.0020074486697286755.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 13.97 seconds\n","XGBoost Training Time: 0.10 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:31:05,073] Trial 5 finished with value: 0.001985162817778148 and parameters: {'gru_units': 80, 'gru_layers': 5, 'gru_learning_rate': 0.0006915195305731017, 'gru_batch_size': 64, 'gru_epochs': 40, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.047100882278382224, 'xgb_max_depth': 8}. Best is trial 5 with value: 0.001985162817778148.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 26.51 seconds\n","XGBoost Training Time: 0.31 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:31:50,540] Trial 6 finished with value: 0.0020579677111685893 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.0008443879019906944, 'gru_batch_size': 32, 'gru_epochs': 40, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.13253902289943317, 'xgb_max_depth': 3}. Best is trial 5 with value: 0.001985162817778148.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 43.94 seconds\n","XGBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:32:16,171] Trial 7 finished with value: 0.0020074610177590892 and parameters: {'gru_units': 128, 'gru_layers': 3, 'gru_learning_rate': 0.0008105939691009926, 'gru_batch_size': 64, 'gru_epochs': 30, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.15281905081830335, 'xgb_max_depth': 4}. Best is trial 5 with value: 0.001985162817778148.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 24.34 seconds\n","XGBoost Training Time: 0.08 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:33:11,418] Trial 8 finished with value: 0.0021550773192271933 and parameters: {'gru_units': 96, 'gru_layers': 3, 'gru_learning_rate': 0.0024896773890388073, 'gru_batch_size': 32, 'gru_epochs': 20, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.24450013030193898, 'xgb_max_depth': 3}. Best is trial 5 with value: 0.001985162817778148.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 53.82 seconds\n","XGBoost Training Time: 0.06 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:33:45,128] Trial 9 finished with value: 0.0020191060665390617 and parameters: {'gru_units': 80, 'gru_layers': 2, 'gru_learning_rate': 0.00022149057035828525, 'gru_batch_size': 16, 'gru_epochs': 50, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.24745983240917158, 'xgb_max_depth': 9}. Best is trial 5 with value: 0.001985162817778148.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 32.19 seconds\n","XGBoost Training Time: 0.17 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:34:22,395] Trial 10 finished with value: 0.003637468266525108 and parameters: {'gru_units': 48, 'gru_layers': 5, 'gru_learning_rate': 0.0020222676011281823, 'gru_batch_size': 64, 'gru_epochs': 40, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.02170116474958867, 'xgb_max_depth': 8}. Best is trial 5 with value: 0.001985162817778148.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 34.63 seconds\n","XGBoost Training Time: 0.34 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:35:35,176] Trial 11 finished with value: 0.002410046481355915 and parameters: {'gru_units': 32, 'gru_layers': 5, 'gru_learning_rate': 0.00029171421197942175, 'gru_batch_size': 64, 'gru_epochs': 50, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.02734143369127244, 'xgb_max_depth': 7}. Best is trial 5 with value: 0.001985162817778148.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 69.76 seconds\n","XGBoost Training Time: 0.32 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:36:40,662] Trial 12 finished with value: 0.001964149685998495 and parameters: {'gru_units': 32, 'gru_layers': 5, 'gru_learning_rate': 0.0003173836234393223, 'gru_batch_size': 64, 'gru_epochs': 40, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.04265304144507192, 'xgb_max_depth': 5}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 63.42 seconds\n","XGBoost Training Time: 0.27 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:37:16,712] Trial 13 finished with value: 0.002010729652842778 and parameters: {'gru_units': 48, 'gru_layers': 5, 'gru_learning_rate': 0.0004149078623240371, 'gru_batch_size': 64, 'gru_epochs': 40, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.04152007657869579, 'xgb_max_depth': 10}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 33.04 seconds\n","XGBoost Training Time: 0.32 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:38:08,854] Trial 14 finished with value: 0.013804119629023778 and parameters: {'gru_units': 48, 'gru_layers': 5, 'gru_learning_rate': 0.0015921983443382465, 'gru_batch_size': 64, 'gru_epochs': 40, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.01162713826656797, 'xgb_max_depth': 7}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 50.08 seconds\n","XGBoost Training Time: 0.31 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:38:41,858] Trial 15 finished with value: 0.001993708205694781 and parameters: {'gru_units': 64, 'gru_layers': 5, 'gru_learning_rate': 0.0005457767103853479, 'gru_batch_size': 64, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.05729394988269217, 'xgb_max_depth': 5}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 29.78 seconds\n","XGBoost Training Time: 0.48 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:40:29,414] Trial 16 finished with value: 0.0020089504277190904 and parameters: {'gru_units': 128, 'gru_layers': 5, 'gru_learning_rate': 0.00010156839851887379, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.06233886286084899, 'xgb_max_depth': 8}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 105.43 seconds\n","XGBoost Training Time: 0.21 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:41:11,130] Trial 17 finished with value: 0.0019934647507630957 and parameters: {'gru_units': 64, 'gru_layers': 5, 'gru_learning_rate': 0.0011640123501592645, 'gru_batch_size': 64, 'gru_epochs': 50, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.036161451154368536, 'xgb_max_depth': 5}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 38.70 seconds\n","XGBoost Training Time: 0.32 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:41:42,088] Trial 18 finished with value: 0.008133623825301245 and parameters: {'gru_units': 96, 'gru_layers': 5, 'gru_learning_rate': 0.008927997325543143, 'gru_batch_size': 64, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.015411672585166408, 'xgb_max_depth': 8}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 28.70 seconds\n","XGBoost Training Time: 0.39 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:43:07,435] Trial 19 finished with value: 0.001984121389160417 and parameters: {'gru_units': 112, 'gru_layers': 5, 'gru_learning_rate': 0.0003677118488553378, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.07971252951792585, 'xgb_max_depth': 10}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 82.42 seconds\n","XGBoost Training Time: 0.21 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:44:55,795] Trial 20 finished with value: 0.001984083589702334 and parameters: {'gru_units': 112, 'gru_layers': 5, 'gru_learning_rate': 0.0002885438006487168, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.07592812713274708, 'xgb_max_depth': 10}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 106.30 seconds\n","XGBoost Training Time: 0.24 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:46:56,303] Trial 21 finished with value: 0.001991571291431691 and parameters: {'gru_units': 112, 'gru_layers': 5, 'gru_learning_rate': 0.00030250742250263025, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.0876144130657222, 'xgb_max_depth': 10}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 117.60 seconds\n","XGBoost Training Time: 0.18 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:48:37,699] Trial 22 finished with value: 0.002006526879769239 and parameters: {'gru_units': 112, 'gru_layers': 5, 'gru_learning_rate': 0.00017826719196236941, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.09422501056139578, 'xgb_max_depth': 10}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 99.29 seconds\n","XGBoost Training Time: 0.17 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:49:43,370] Trial 23 finished with value: 0.0020048070085827412 and parameters: {'gru_units': 128, 'gru_layers': 5, 'gru_learning_rate': 0.0003299580058786206, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.06778641590934074, 'xgb_max_depth': 9}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 63.62 seconds\n","XGBoost Training Time: 0.24 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:51:00,078] Trial 24 finished with value: 0.00203204724531367 and parameters: {'gru_units': 112, 'gru_layers': 5, 'gru_learning_rate': 0.00022600714167257766, 'gru_batch_size': 16, 'gru_epochs': 20, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.1061559454751622, 'xgb_max_depth': 4}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 74.82 seconds\n","XGBoost Training Time: 0.07 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:52:48,277] Trial 25 finished with value: 0.0038232005549156717 and parameters: {'gru_units': 96, 'gru_layers': 5, 'gru_learning_rate': 0.0004512527234152642, 'gru_batch_size': 16, 'gru_epochs': 10, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.03171257766039161, 'xgb_max_depth': 10}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 105.25 seconds\n","XGBoost Training Time: 0.26 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:54:53,760] Trial 26 finished with value: 0.002005814477083422 and parameters: {'gru_units': 112, 'gru_layers': 5, 'gru_learning_rate': 0.00014501603351152858, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.07033123312254687, 'xgb_max_depth': 9}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 123.57 seconds\n","XGBoost Training Time: 0.18 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:55:56,856] Trial 27 finished with value: 0.006339060563552471 and parameters: {'gru_units': 128, 'gru_layers': 5, 'gru_learning_rate': 0.001231196813983604, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.05057096263162523, 'xgb_max_depth': 6}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 61.21 seconds\n","XGBoost Training Time: 0.10 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:56:38,119] Trial 28 finished with value: 0.0038438964948983717 and parameters: {'gru_units': 96, 'gru_layers': 2, 'gru_learning_rate': 0.0003378030180340349, 'gru_batch_size': 16, 'gru_epochs': 50, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.021217129705337286, 'xgb_max_depth': 4}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 39.63 seconds\n","XGBoost Training Time: 0.23 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 14:58:44,202] Trial 29 finished with value: 0.001972252043674409 and parameters: {'gru_units': 64, 'gru_layers': 5, 'gru_learning_rate': 0.0005786273540454738, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.11142690428849888, 'xgb_max_depth': 7}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 123.22 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:00:26,223] Trial 30 finished with value: 0.0019785599785511992 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.0006035759355673013, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.11256964886755177, 'xgb_max_depth': 7}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 100.42 seconds\n","XGBoost Training Time: 0.19 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:01:10,388] Trial 31 finished with value: 0.002069844636209431 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.0005900577361630021, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.12234696805678383, 'xgb_max_depth': 7}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 43.11 seconds\n","XGBoost Training Time: 0.18 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:02:50,787] Trial 32 finished with value: 0.0019859268966424064 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.0005914614125238634, 'gru_batch_size': 16, 'gru_epochs': 20, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.1953731065044643, 'xgb_max_depth': 6}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 98.86 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:03:24,655] Trial 33 finished with value: 0.0020353149876119383 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.00025215909663220675, 'gru_batch_size': 32, 'gru_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.10844954852519309, 'xgb_max_depth': 6}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 32.23 seconds\n","XGBoost Training Time: 0.22 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:03:57,727] Trial 34 finished with value: 0.001998772486825518 and parameters: {'gru_units': 64, 'gru_layers': 2, 'gru_learning_rate': 0.000471650114543034, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.18132510579514854, 'xgb_max_depth': 7}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 32.05 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:04:28,785] Trial 35 finished with value: 0.002025208982467167 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.000976670286525375, 'gru_batch_size': 16, 'gru_epochs': 10, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.07639159714864235, 'xgb_max_depth': 5}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 30.00 seconds\n","XGBoost Training Time: 0.18 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:04:54,300] Trial 36 finished with value: 0.002014156602483859 and parameters: {'gru_units': 32, 'gru_layers': 3, 'gru_learning_rate': 0.00017694598924073232, 'gru_batch_size': 32, 'gru_epochs': 20, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.043679163524382815, 'xgb_max_depth': 6}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 23.79 seconds\n","XGBoost Training Time: 0.33 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:05:30,947] Trial 37 finished with value: 0.0020128987914360993 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.0007117108511086552, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.10169582511863981, 'xgb_max_depth': 7}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 35.62 seconds\n","XGBoost Training Time: 0.17 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:07:41,135] Trial 38 finished with value: 0.0019936814083703104 and parameters: {'gru_units': 64, 'gru_layers': 3, 'gru_learning_rate': 0.00040781095973790686, 'gru_batch_size': 16, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.1348813956354939, 'xgb_max_depth': 5}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 128.65 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:09:13,521] Trial 39 finished with value: 0.004203756148765568 and parameters: {'gru_units': 48, 'gru_layers': 5, 'gru_learning_rate': 0.0034158809070508954, 'gru_batch_size': 32, 'gru_epochs': 20, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.05989475055955817, 'xgb_max_depth': 8}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 89.55 seconds\n","XGBoost Training Time: 0.13 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:09:58,419] Trial 40 finished with value: 0.002008812389658596 and parameters: {'gru_units': 80, 'gru_layers': 2, 'gru_learning_rate': 0.0007097813667597702, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.15564856116438935, 'xgb_max_depth': 4}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 43.33 seconds\n","XGBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:12:24,510] Trial 41 finished with value: 0.0019895092664863564 and parameters: {'gru_units': 80, 'gru_layers': 5, 'gru_learning_rate': 0.0003738978647549196, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.08411718431756145, 'xgb_max_depth': 9}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 144.05 seconds\n","XGBoost Training Time: 0.20 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:14:01,474] Trial 42 finished with value: 0.00203925532832905 and parameters: {'gru_units': 112, 'gru_layers': 5, 'gru_learning_rate': 0.0002592826287366803, 'gru_batch_size': 16, 'gru_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.05158912603022173, 'xgb_max_depth': 9}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 93.97 seconds\n","XGBoost Training Time: 0.28 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:15:35,288] Trial 43 finished with value: 0.001981653385079103 and parameters: {'gru_units': 96, 'gru_layers': 5, 'gru_learning_rate': 0.0005043532155325456, 'gru_batch_size': 16, 'gru_epochs': 50, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.07798683924973551, 'xgb_max_depth': 10}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 90.80 seconds\n","XGBoost Training Time: 0.25 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:16:52,115] Trial 44 finished with value: 0.0020078509483089234 and parameters: {'gru_units': 96, 'gru_layers': 5, 'gru_learning_rate': 0.00081066378766369, 'gru_batch_size': 16, 'gru_epochs': 50, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.12988875814776008, 'xgb_max_depth': 8}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 73.92 seconds\n","XGBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:17:20,362] Trial 45 finished with value: 0.001993261606064698 and parameters: {'gru_units': 32, 'gru_layers': 3, 'gru_learning_rate': 0.0005074487603115958, 'gru_batch_size': 64, 'gru_epochs': 50, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.11317646710152994, 'xgb_max_depth': 9}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 26.58 seconds\n","XGBoost Training Time: 0.20 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:18:34,334] Trial 46 finished with value: 0.0019873446220504375 and parameters: {'gru_units': 80, 'gru_layers': 5, 'gru_learning_rate': 0.0009857459544369786, 'gru_batch_size': 32, 'gru_epochs': 50, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.1606818235816504, 'xgb_max_depth': 10}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 70.90 seconds\n","XGBoost Training Time: 0.34 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:19:21,769] Trial 47 finished with value: 0.0019912930448022664 and parameters: {'gru_units': 96, 'gru_layers': 5, 'gru_learning_rate': 0.0014844194393111342, 'gru_batch_size': 64, 'gru_epochs': 30, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.2995431113224378, 'xgb_max_depth': 7}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 45.56 seconds\n","XGBoost Training Time: 0.12 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:21:54,203] Trial 48 finished with value: 0.0027363365886437655 and parameters: {'gru_units': 48, 'gru_layers': 5, 'gru_learning_rate': 0.00014181348530766815, 'gru_batch_size': 16, 'gru_epochs': 50, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.0381142978132277, 'xgb_max_depth': 3}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 149.61 seconds\n","XGBoost Training Time: 0.12 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 15:22:25,145] Trial 49 finished with value: 0.0020203416033808554 and parameters: {'gru_units': 32, 'gru_layers': 3, 'gru_learning_rate': 0.0005972132566114166, 'gru_batch_size': 64, 'gru_epochs': 40, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.07410058243015998, 'xgb_max_depth': 8}. Best is trial 12 with value: 0.001964149685998495.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 29.09 seconds\n","XGBoost Training Time: 0.43 seconds\n","Best hyperparameters: {'gru_units': 32, 'gru_layers': 5, 'gru_learning_rate': 0.0003173836234393223, 'gru_batch_size': 64, 'gru_epochs': 40, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.04265304144507192, 'xgb_max_depth': 5}\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import GRU, Dense, Bidirectional\n","import xgboost as xgb\n","import optuna\n","from sklearn.metrics import mean_squared_error\n","import time  # For tracking training time\n","\n","# Function to train Bi-GRU model\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","def train_bi_gru(X_train, Y_train, X_val, Y_val, units, layers, learning_rate, batch_size, epochs):\n","    model = keras.Sequential()\n","\n","    for _ in range(layers - 1):  # All except last layer have return_sequences=True\n","        model.add(Bidirectional(GRU(units, return_sequences=True)))\n","\n","    model.add(Bidirectional(GRU(units)))  # Last GRU layer\n","    model.add(Dense(1))  # Output layer\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n","\n","    # Early Stopping Callback\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","\n","    start_time = time.time()  # Start time tracking\n","    history = model.fit(X_train, Y_train,\n","                        validation_data=(X_val, Y_val),\n","                        epochs=epochs,\n","                        batch_size=batch_size,\n","                        verbose=0,\n","                        callbacks=[early_stopping])  # Add early stopping\n","    gru_train_time = time.time() - start_time  # End time tracking\n","\n","    return model, history, gru_train_time\n","\n","\n","# Objective function for Optuna\n","def objective(trial):\n","    units = trial.suggest_int(\"gru_units\", 32, 128, step=16)\n","    layers = trial.suggest_categorical(\"gru_layers\", [2, 3, 5])\n","    learning_rate = trial.suggest_loguniform(\"gru_learning_rate\", 1e-4, 1e-2)\n","    batch_size = trial.suggest_categorical(\"gru_batch_size\", [16, 32, 64])\n","    epochs = trial.suggest_int(\"gru_epochs\", 10, 50, step=10)\n","\n","    # Reshape input for GRU\n","    X_train_r = np.expand_dims(X_train, axis=-1)\n","    X_val_r = np.expand_dims(X_val, axis=-1)\n","\n","    # Train Bi-GRU\n","    model, _, gru_train_time = train_bi_gru(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","    Y_val_pred_gru = model.predict(X_val_r).flatten()\n","\n","    # Prepare data for XGBoost\n","    X_val_xgb = np.column_stack([Y_val_pred_gru])\n","\n","    xgb_params = {\n","        \"objective\": \"reg:squarederror\",\n","        \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 50, 200, step=50),\n","        \"learning_rate\": trial.suggest_loguniform(\"xgb_learning_rate\", 0.01, 0.3),\n","        \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 10),\n","    }\n","\n","    if tf.config.list_physical_devices(\"GPU\"):\n","        xgb_params[\"tree_method\"] = \"gpu_hist\"\n","\n","    # Check XGBoost version\n","    xgb_version = xgb.__version__.split(\".\")\n","    is_new_xgb = int(xgb_version[0]) >= 1 and int(xgb_version[1]) >= 6  # XGBoost >= 1.6 supports early stopping\n","\n","    # Train XGBoost\n","    start_time = time.time()  # Start time tracking\n","    xgb_model = xgb.XGBRegressor(**xgb_params)\n","\n","    if is_new_xgb:\n","        xgb_model.fit(X_val_xgb, Y_val, eval_set=[(X_val_xgb, Y_val)], early_stopping_rounds=10, verbose=False)\n","    else:\n","        xgb_model.fit(X_val_xgb, Y_val)\n","\n","    xgb_train_time = time.time() - start_time  # End time tracking\n","\n","    # Predict and evaluate\n","    Y_val_pred_xgb = xgb_model.predict(X_val_xgb)\n","    rmse = np.sqrt(mean_squared_error(Y_val, Y_val_pred_xgb))\n","\n","    # Print training times\n","    print(f\"Bi-GRU Training Time: {gru_train_time:.2f} seconds\")\n","    print(f\"XGBoost Training Time: {xgb_train_time:.2f} seconds\")\n","\n","    return rmse\n","\n","# Run Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=50)\n","\n","# Best hyperparameters\n","print(\"Best hyperparameters:\", study.best_params)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jfgi09sNUAi1","outputId":"66a45b66-9d57-4a78-c6d6-5fb9b1257526"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\n","Final Model Performance:\n","Bi-GRU Training Time: 64.97 seconds\n","XGBoost Training Time: 0.25 seconds\n","\n","Bi-GRU Train Prediction Time: 2.5938 seconds\n","Bi-GRU Validation Prediction Time: 0.3556 seconds\n","Bi-GRU Test Prediction Time: 0.3527 seconds\n","\n","XGBoost Train Prediction Time: 0.0041 seconds\n","XGBoost Validation Prediction Time: 0.0028 seconds\n","XGBoost Test Prediction Time: 0.0026 seconds\n","\n","Train Set Metrics:\n","MAE: 0.0037, MSE: 0.0000, RMSE: 0.0056, R²: 0.9998, MAPE: 1.06%\n","\n","Validation Set Metrics:\n","MAE: 0.1604, MSE: 0.0316, RMSE: 0.1777, R²: -4.3716, MAPE: 9.01%\n","\n","Test Set Metrics:\n","MAE: 0.4294, MSE: 0.1904, RMSE: 0.4364, R²: -30.5501, MAPE: 21.18%\n"]}],"source":["from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import time\n","\n","# Retrieve best hyperparameters from Optuna study\n","best_params = study.best_params\n","units = best_params[\"gru_units\"]\n","layers = best_params[\"gru_layers\"]\n","learning_rate = best_params[\"gru_learning_rate\"]\n","batch_size = best_params[\"gru_batch_size\"]\n","epochs = best_params[\"gru_epochs\"]\n","n_estimators = best_params[\"xgb_n_estimators\"]\n","xgb_learning_rate = best_params[\"xgb_learning_rate\"]\n","max_depth = best_params[\"xgb_max_depth\"]\n","\n","# Reshape input for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train final Bi-GRU model\n","final_gru, _, gru_train_time = train_bi_gru(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","\n","# Predictions with timing\n","start_time = time.time()\n","Y_train_pred_gru = final_gru.predict(X_train_r).flatten()\n","gru_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_gru = final_gru.predict(X_val_r).flatten()\n","gru_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_gru = final_gru.predict(X_test_r).flatten()\n","gru_test_pred_time = time.time() - start_time\n","\n","# Prepare data for XGBoost\n","X_train_xgb = np.column_stack([Y_train_pred_gru])\n","X_val_xgb = np.column_stack([Y_val_pred_gru])\n","X_test_xgb = np.column_stack([Y_test_pred_gru])\n","\n","xgb_params = {\n","    \"objective\": \"reg:squarederror\",\n","    \"n_estimators\": n_estimators,\n","    \"learning_rate\": xgb_learning_rate,\n","    \"max_depth\": max_depth,\n","}\n","\n","if tf.config.list_physical_devices(\"GPU\"):\n","    xgb_params[\"tree_method\"] = \"gpu_hist\"\n","\n","# Train final XGBoost model\n","start_time = time.time()\n","final_xgb = xgb.XGBRegressor(**xgb_params)\n","if hasattr(final_xgb, \"fit\") and \"early_stopping_rounds\" in final_xgb.fit.__code__.co_varnames:\n","    final_xgb.fit(X_train_xgb, Y_train, eval_set=[(X_val_xgb, Y_val)], early_stopping_rounds=10, verbose=False)\n","else:\n","    final_xgb.fit(X_train_xgb, Y_train)\n","xgb_train_time = time.time() - start_time\n","\n","# XGBoost Predictions with timing\n","start_time = time.time()\n","Y_train_pred_xgb = final_xgb.predict(X_train_xgb)\n","xgb_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_xgb = final_xgb.predict(X_val_xgb)\n","xgb_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_xgb = final_xgb.predict(X_test_xgb)\n","xgb_test_pred_time = time.time() - start_time\n","\n","# Compute Metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","metrics_train = compute_metrics(Y_train, Y_train_pred_xgb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_xgb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_xgb)\n","\n","# Print results\n","print(\"\\nFinal Model Performance:\")\n","print(f\"Bi-GRU Training Time: {gru_train_time:.2f} seconds\")\n","print(f\"XGBoost Training Time: {xgb_train_time:.2f} seconds\\n\")\n","\n","print(f\"Bi-GRU Train Prediction Time: {gru_train_pred_time:.4f} seconds\")\n","print(f\"Bi-GRU Validation Prediction Time: {gru_val_pred_time:.4f} seconds\")\n","print(f\"Bi-GRU Test Prediction Time: {gru_test_pred_time:.4f} seconds\\n\")\n","\n","print(f\"XGBoost Train Prediction Time: {xgb_train_pred_time:.4f} seconds\")\n","print(f\"XGBoost Validation Prediction Time: {xgb_val_pred_time:.4f} seconds\")\n","print(f\"XGBoost Test Prediction Time: {xgb_test_pred_time:.4f} seconds\\n\")\n","\n","print(\"Train Set Metrics:\")\n","print(f\"MAE: {metrics_train[0]:.4f}, MSE: {metrics_train[1]:.4f}, RMSE: {metrics_train[2]:.4f}, R²: {metrics_train[3]:.4f}, MAPE: {metrics_train[4]:.2f}%\")\n","\n","print(\"\\nValidation Set Metrics:\")\n","print(f\"MAE: {metrics_val[0]:.4f}, MSE: {metrics_val[1]:.4f}, RMSE: {metrics_val[2]:.4f}, R²: {metrics_val[3]:.4f}, MAPE: {metrics_val[4]:.2f}%\")\n","\n","print(\"\\nTest Set Metrics:\")\n","print(f\"MAE: {metrics_test[0]:.4f}, MSE: {metrics_test[1]:.4f}, RMSE: {metrics_test[2]:.4f}, R²: {metrics_test[3]:.4f}, MAPE: {metrics_test[4]:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"W4bcO5Je0CqW"},"source":["##BOHB"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2AVjHDTnyKgo","outputId":"6eb4275b-f676-46c0-fc9d-7e7c50060875"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Collecting ConfigSpace (from hpbandster)\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.13.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, ConfigSpace, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=3db6595089afee72bcc1d0bff7b8dd80c22b471726dd6417d460347111dcdb89\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=configspace-1.2.1-py3-none-any.whl size=115950 sha256=78f7f715bdc63d0e4ef08a0a1ac119fd869d482577350c86f38ea68098d33818\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35182 sha256=10450893d78973ec162e35d489e648d3835624f4094cdb8e80841c3bc10b430e\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster ConfigSpace netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, ConfigSpace, hpbandster\n","Successfully installed ConfigSpace-1.2.1 Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}],"source":["!pip install hpbandster"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNvMh2Op0H1V","outputId":"9151133a-a60d-45b5-ada3-bcb419abb7a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: configSpace in /usr/local/lib/python3.11/dist-packages (1.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from configSpace) (1.26.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from configSpace) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from configSpace) (1.13.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from configSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from configSpace) (10.6.0)\n"]}],"source":["!pip install configSpace"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NW7Ps89l00ky","outputId":"78a4fd8d-d518-4d40-fcaf-917555fb1b75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Bi-GRU with 2 layers...\n","Training Bi-GRU with 3 layers...\n","Training Bi-GRU with 5 layers...\n","Train Metrics: (0.003554916496398338, 2.9515425290587386e-05, 0.005432810073119378, 0.9998313606697016, 1.0384160706778363) Time: 83.32776403427124\n","Validation Metrics: (0.15624458911306413, 0.030269454873811434, 0.17398119114953614, -4.146515571199491, 8.76857053003086) Time: 0.35892224311828613\n","Test Metrics: (0.4252033802087772, 0.18683298493196804, 0.43224181303058595, -29.957879996006177, 20.975487797715267) Time: 0.05912280082702637\n"]}],"source":["import numpy as np\n","import xgboost as xgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiGRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-directional GRU doubles output size\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Bi-GRU Configurations\n","gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store GRU feature representations\n","gru_features = []\n","\n","for num_layers in gru_layers:\n","    print(f\"Training Bi-GRU with {num_layers} layers...\")\n","\n","    gru_model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        gru_model.train()\n","        optimizer.zero_grad()\n","        outputs = gru_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    # Extract Feature Representations\n","    gru_model.eval()\n","    with torch.no_grad():\n","        val_start = time.time()\n","        train_features = gru_model(X_train_torch).numpy()\n","        val_features = gru_model(X_val_torch).numpy()\n","        val_time = time.time() - val_start\n","\n","        test_start = time.time()\n","        test_features = gru_model(X_test_torch).numpy()\n","        test_time = time.time() - test_start\n","\n","    gru_features.append((train_features, val_features, test_features, train_time, val_time, test_time))\n","\n","# Concatenate Features from All Layers\n","final_train_features = np.hstack([feat[0] for feat in gru_features])\n","final_val_features = np.hstack([feat[1] for feat in gru_features])\n","final_test_features = np.hstack([feat[2] for feat in gru_features])\n","\n","# Record Time for Each Stage\n","total_train_time = sum([feat[3] for feat in gru_features])\n","total_val_time = sum([feat[4] for feat in gru_features])\n","total_test_time = sum([feat[5] for feat in gru_features])\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"subsample\", 0.5, 1.0, default_value=0.8))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"colsample_bytree\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for XGBoost\n","class XGBoostWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = xgb.XGBRegressor(\n","            n_estimators=config[\"n_estimators\"],\n","            learning_rate=config[\"learning_rate\"],\n","            max_depth=config[\"max_depth\"],\n","            subsample=config[\"subsample\"],\n","            colsample_bytree=config[\"colsample_bytree\"],\n","            random_state=42\n","        )\n","        model.fit(final_train_features, Y_train)\n","        Y_val_pred = model.predict(final_val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"bigru_xgb_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = XGBoostWorker(nameserver=\"127.0.0.2\", run_id=\"bigru_xgb_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"bigru_xgb_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best XGBoost Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_xgb_model = xgb.XGBRegressor(\n","    n_estimators=best_params[\"n_estimators\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    max_depth=best_params[\"max_depth\"],\n","    subsample=best_params[\"subsample\"],\n","    colsample_bytree=best_params[\"colsample_bytree\"],\n","    random_state=42\n",")\n","\n","best_xgb_model.fit(final_train_features, Y_train)\n","\n","# Predictions\n","Y_train_pred = best_xgb_model.predict(final_train_features)\n","Y_val_pred = best_xgb_model.predict(final_val_features)\n","Y_test_pred = best_xgb_model.predict(final_test_features)\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", total_train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", total_val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", total_test_time)"]},{"cell_type":"markdown","metadata":{"id":"9OybYRRv8B7L"},"source":["#Catboost"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PkUPzWds9EpL","outputId":"5d772134-7400-4679-8cab-76dd8ef521d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting catboost\n","  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n","Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.7\n"]}],"source":["!pip install catboost"]},{"cell_type":"markdown","metadata":{"id":"NoEIY05G8EVr"},"source":["##Initial"]},{"cell_type":"code","source":[],"metadata":{"id":"G4MZKMSku_aL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, GRU, Dense\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Enable GPU for TensorFlow\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train a Bi-GRU model on GPU\n","def train_bi_gru(X_train, Y_train, X_val, Y_val, layers):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(Bidirectional(GRU(64, return_sequences=(layers > 1)), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(Bidirectional(GRU(64, return_sequences=(_ < layers - 2))))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        start_time = time.time()\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","        train_time = time.time() - start_time\n","        return model, train_time\n","\n","# Reshaping input for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train 2, 3, and 5-layer Bi-GRU models\n","bi_gru_models = {}\n","bi_gru_predictions = {}\n","total_train_time = 0\n","\n","for layers in [2, 3, 5]:\n","    model, train_time = train_bi_gru(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    Y_train_pred = model.predict(X_train_r)\n","    Y_val_pred = model.predict(X_val_r)\n","    Y_test_pred = model.predict(X_test_r)\n","\n","    bi_gru_models[layers] = model\n","    bi_gru_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","    total_train_time += train_time\n","\n","# Prepare input for CatBoost\n","X_train_cat = np.column_stack([bi_gru_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_cat = np.column_stack([bi_gru_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_cat = np.column_stack([bi_gru_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train CatBoost model\n","cat_model = CatBoostRegressor(iterations=100, learning_rate=0.05, depth=3, loss_function='RMSE', task_type='GPU', verbose=0)\n","\n","start_time = time.time()\n","cat_model.fit(X_train_cat, Y_train, eval_set=(X_val_cat, Y_val), verbose=0)\n","total_train_time += time.time() - start_time  # Adding CatBoost training time\n","\n","# Predictions from CatBoost\n","start_time = time.time()\n","Y_train_pred_cat = cat_model.predict(X_train_cat)\n","train_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_cat = cat_model.predict(X_val_cat)\n","val_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_cat = cat_model.predict(X_test_cat)\n","test_time = time.time() - start_time\n","\n","# Compute overall validation and test times\n","total_val_time = val_time\n","total_test_time = test_time\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_cat)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_cat)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_cat)\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print overall training, validation, and testing times\n","print(\"\\nTotal Execution Times:\")\n","print(f\"Total Train Time: {total_train_time:.2f} seconds\")\n","print(f\"Total Validation Time: {total_val_time:.2f} seconds\")\n","print(f\"Total Test Time: {total_test_time:.2f} seconds\")\n"],"metadata":{"id":"D_bl2tnWV7sD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d62cc668-a295-48ad-8496-d771a1846426"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU activated for TensorFlow!\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","Train Metrics: MAE=0.0062, MSE=0.0001, RMSE=0.0091, R²=0.9995, MAPE=2.48%\n","Validation Metrics: MAE=0.1953, MSE=0.0440, RMSE=0.2098, R²=-6.4828, MAPE=11.01%\n","Test Metrics: MAE=0.4643, MSE=0.2216, RMSE=0.4708, R²=-35.7207, MAPE=22.92%\n","\n","Total Execution Times:\n","Total Train Time: 527.31 seconds\n","Total Validation Time: 0.01 seconds\n","Total Test Time: 0.01 seconds\n"]}]},{"cell_type":"markdown","metadata":{"id":"y1MAPmqL_U1N"},"source":["## Optuna\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-vR7Bj4E9LwI","outputId":"ebea5e3b-cade-4c30-9b30-1d1092a1faa8"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:30:29,446] A new study created in memory with name: no-name-48a6ad97-634d-4c05-a3fa-c14a03325f30\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:31:10,002] Trial 0 finished with value: 0.011717731733659963 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.004457748674156657, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 6, 'cat_learning_rate': 0.04070119765260588, 'cat_iterations': 50}. Best is trial 0 with value: 0.011717731733659963.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 39.63 seconds\n","CatBoost Training Time: 0.06 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:32:45,025] Trial 1 finished with value: 0.0020868712357924248 and parameters: {'gru_units': 96, 'gru_layers': 3, 'gru_learning_rate': 0.0016182330635586908, 'gru_batch_size': 16, 'gru_epochs': 10, 'cat_depth': 9, 'cat_learning_rate': 0.10216517636704252, 'cat_iterations': 200}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 93.20 seconds\n","CatBoost Training Time: 0.46 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:33:55,481] Trial 2 finished with value: 0.0025933144724801637 and parameters: {'gru_units': 128, 'gru_layers': 3, 'gru_learning_rate': 0.009797201181203875, 'gru_batch_size': 16, 'gru_epochs': 10, 'cat_depth': 8, 'cat_learning_rate': 0.18241544989385333, 'cat_iterations': 50}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 67.71 seconds\n","CatBoost Training Time: 0.09 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:36:11,821] Trial 3 finished with value: 0.032948437576506055 and parameters: {'gru_units': 64, 'gru_layers': 5, 'gru_learning_rate': 0.00014510631345750457, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 3, 'cat_learning_rate': 0.019093964933431278, 'cat_iterations': 50}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 134.48 seconds\n","CatBoost Training Time: 0.04 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:38:08,695] Trial 4 finished with value: 0.0024295682434688037 and parameters: {'gru_units': 48, 'gru_layers': 5, 'gru_learning_rate': 0.0005145594032624714, 'gru_batch_size': 64, 'gru_epochs': 50, 'cat_depth': 7, 'cat_learning_rate': 0.032101855890990266, 'cat_iterations': 200}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 113.85 seconds\n","CatBoost Training Time: 0.35 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:38:25,498] Trial 5 finished with value: 0.017387207200187358 and parameters: {'gru_units': 112, 'gru_layers': 2, 'gru_learning_rate': 0.0030956882328685006, 'gru_batch_size': 64, 'gru_epochs': 30, 'cat_depth': 7, 'cat_learning_rate': 0.010592042089498571, 'cat_iterations': 150}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 15.74 seconds\n","CatBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:39:23,151] Trial 6 finished with value: 0.007219698433521834 and parameters: {'gru_units': 48, 'gru_layers': 5, 'gru_learning_rate': 0.0004502163992244289, 'gru_batch_size': 32, 'gru_epochs': 10, 'cat_depth': 8, 'cat_learning_rate': 0.025702960337005518, 'cat_iterations': 100}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 55.67 seconds\n","CatBoost Training Time: 0.17 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:39:47,271] Trial 7 finished with value: 0.002888521790457928 and parameters: {'gru_units': 112, 'gru_layers': 2, 'gru_learning_rate': 0.0029511805126518392, 'gru_batch_size': 32, 'gru_epochs': 40, 'cat_depth': 6, 'cat_learning_rate': 0.15605692814726163, 'cat_iterations': 50}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 22.70 seconds\n","CatBoost Training Time: 0.06 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:40:17,781] Trial 8 finished with value: 0.004035333229454191 and parameters: {'gru_units': 96, 'gru_layers': 3, 'gru_learning_rate': 0.00011383962942977606, 'gru_batch_size': 32, 'gru_epochs': 30, 'cat_depth': 5, 'cat_learning_rate': 0.07204172789061031, 'cat_iterations': 50}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 29.06 seconds\n","CatBoost Training Time: 0.09 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:40:46,010] Trial 9 finished with value: 0.007638389967982783 and parameters: {'gru_units': 96, 'gru_layers': 5, 'gru_learning_rate': 0.00010857229500697553, 'gru_batch_size': 64, 'gru_epochs': 20, 'cat_depth': 8, 'cat_learning_rate': 0.025050303640238282, 'cat_iterations': 100}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 25.49 seconds\n","CatBoost Training Time: 0.30 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:41:45,494] Trial 10 finished with value: 0.0021261065508183527 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.001400774268263933, 'gru_batch_size': 16, 'gru_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.07835018930374356, 'cat_iterations': 200}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 57.63 seconds\n","CatBoost Training Time: 0.65 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:42:49,234] Trial 11 finished with value: 0.0021373088259967507 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.001160360822173454, 'gru_batch_size': 16, 'gru_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.08772927371166316, 'cat_iterations': 200}. Best is trial 1 with value: 0.0020868712357924248.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 61.68 seconds\n","CatBoost Training Time: 0.66 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:44:18,634] Trial 12 finished with value: 0.001981236574932562 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.0013454818473692131, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.2847687343141778, 'cat_iterations': 150}. Best is trial 12 with value: 0.001981236574932562.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 87.70 seconds\n","CatBoost Training Time: 0.50 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:45:12,661] Trial 13 finished with value: 0.00206734516594303 and parameters: {'gru_units': 96, 'gru_layers': 3, 'gru_learning_rate': 0.0005794634360317092, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.2138341410191344, 'cat_iterations': 150}. Best is trial 12 with value: 0.001981236574932562.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 52.33 seconds\n","CatBoost Training Time: 0.51 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:46:40,875] Trial 14 finished with value: 0.0020005964674262 and parameters: {'gru_units': 64, 'gru_layers': 3, 'gru_learning_rate': 0.0004893370472892586, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.2852612806267708, 'cat_iterations': 150}. Best is trial 12 with value: 0.001981236574932562.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 86.60 seconds\n","CatBoost Training Time: 0.48 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:48:21,930] Trial 15 finished with value: 0.0019675758180903815 and parameters: {'gru_units': 32, 'gru_layers': 3, 'gru_learning_rate': 0.0003187650482743804, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.28925517540491286, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 99.51 seconds\n","CatBoost Training Time: 0.34 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:51:20,214] Trial 16 finished with value: 0.0021568639450820553 and parameters: {'gru_units': 32, 'gru_layers': 3, 'gru_learning_rate': 0.00025356037177090367, 'gru_batch_size': 16, 'gru_epochs': 40, 'cat_depth': 9, 'cat_learning_rate': 0.12372896101344849, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 176.55 seconds\n","CatBoost Training Time: 0.34 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:53:19,436] Trial 17 finished with value: 0.002084339830687863 and parameters: {'gru_units': 32, 'gru_layers': 3, 'gru_learning_rate': 0.00026999631572830645, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.29930509609170053, 'cat_iterations': 100}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 117.61 seconds\n","CatBoost Training Time: 0.23 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:53:42,811] Trial 18 finished with value: 0.00291829130948029 and parameters: {'gru_units': 64, 'gru_layers': 3, 'gru_learning_rate': 0.0008679092175346563, 'gru_batch_size': 64, 'gru_epochs': 40, 'cat_depth': 3, 'cat_learning_rate': 0.0493614254873029, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 22.11 seconds\n","CatBoost Training Time: 0.07 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:54:49,379] Trial 19 finished with value: 0.0021891147694134004 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.0002363383150767291, 'gru_batch_size': 32, 'gru_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.21653341064341494, 'cat_iterations': 100}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 65.52 seconds\n","CatBoost Training Time: 0.23 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:55:44,836] Trial 20 finished with value: 0.002286083059784668 and parameters: {'gru_units': 128, 'gru_layers': 3, 'gru_learning_rate': 0.0007287328526646481, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 5, 'cat_learning_rate': 0.13890279949793433, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 52.61 seconds\n","CatBoost Training Time: 0.17 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:56:47,648] Trial 21 finished with value: 0.002009574654751094 and parameters: {'gru_units': 64, 'gru_layers': 3, 'gru_learning_rate': 0.00039292304580179063, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.2657769430434602, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 60.34 seconds\n","CatBoost Training Time: 1.04 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:57:49,053] Trial 22 finished with value: 0.0019906268713087917 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.0018441530955531885, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.2829719803432433, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 59.70 seconds\n","CatBoost Training Time: 0.50 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 16:59:38,946] Trial 23 finished with value: 0.0020164528586697596 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.0018566329262187725, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.20369651444769585, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 107.45 seconds\n","CatBoost Training Time: 0.67 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:00:43,433] Trial 24 finished with value: 0.002215547740471083 and parameters: {'gru_units': 112, 'gru_layers': 3, 'gru_learning_rate': 0.002395171570761455, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 8, 'cat_learning_rate': 0.15743369682060124, 'cat_iterations': 100}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 63.07 seconds\n","CatBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:01:45,030] Trial 25 finished with value: 0.0020106533405220674 and parameters: {'gru_units': 48, 'gru_layers': 3, 'gru_learning_rate': 0.004940839035398419, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.24441852848065002, 'cat_iterations': 200}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 58.09 seconds\n","CatBoost Training Time: 0.82 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:02:50,639] Trial 26 finished with value: 0.0021097176043330446 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.0009925755711586522, 'gru_batch_size': 16, 'gru_epochs': 10, 'cat_depth': 9, 'cat_learning_rate': 0.12444868844023829, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 63.87 seconds\n","CatBoost Training Time: 0.35 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:03:38,683] Trial 27 finished with value: 0.0021389427793522277 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.0019467534664424008, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.1838347289192967, 'cat_iterations': 100}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 46.48 seconds\n","CatBoost Training Time: 0.37 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:04:50,914] Trial 28 finished with value: 0.0019908147479679983 and parameters: {'gru_units': 64, 'gru_layers': 5, 'gru_learning_rate': 0.0007219056421138095, 'gru_batch_size': 32, 'gru_epochs': 20, 'cat_depth': 8, 'cat_learning_rate': 0.2980399701217105, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 70.16 seconds\n","CatBoost Training Time: 0.23 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:05:19,891] Trial 29 finished with value: 0.0023044220416047296 and parameters: {'gru_units': 48, 'gru_layers': 2, 'gru_learning_rate': 0.0045206633236982855, 'gru_batch_size': 64, 'gru_epochs': 30, 'cat_depth': 7, 'cat_learning_rate': 0.06309075646223203, 'cat_iterations': 200}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 27.89 seconds\n","CatBoost Training Time: 0.22 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:06:05,693] Trial 30 finished with value: 0.002492032061433482 and parameters: {'gru_units': 32, 'gru_layers': 2, 'gru_learning_rate': 0.0070053597413115894, 'gru_batch_size': 16, 'gru_epochs': 50, 'cat_depth': 4, 'cat_learning_rate': 0.09609630620574923, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 44.33 seconds\n","CatBoost Training Time: 0.08 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:07:12,149] Trial 31 finished with value: 0.002045782740044484 and parameters: {'gru_units': 64, 'gru_layers': 5, 'gru_learning_rate': 0.0008017597721018635, 'gru_batch_size': 32, 'gru_epochs': 20, 'cat_depth': 8, 'cat_learning_rate': 0.2990111022030915, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 64.44 seconds\n","CatBoost Training Time: 0.22 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:08:29,200] Trial 32 finished with value: 0.0020127583907543502 and parameters: {'gru_units': 80, 'gru_layers': 5, 'gru_learning_rate': 0.001291191180725473, 'gru_batch_size': 32, 'gru_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.23615048303070846, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 74.64 seconds\n","CatBoost Training Time: 0.34 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:09:26,545] Trial 33 finished with value: 0.0020879859594744267 and parameters: {'gru_units': 96, 'gru_layers': 5, 'gru_learning_rate': 0.0003117260666818343, 'gru_batch_size': 32, 'gru_epochs': 10, 'cat_depth': 9, 'cat_learning_rate': 0.17278876826631073, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 55.22 seconds\n","CatBoost Training Time: 0.36 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:10:04,580] Trial 34 finished with value: 0.0020438491027408917 and parameters: {'gru_units': 64, 'gru_layers': 5, 'gru_learning_rate': 0.0001721449715341865, 'gru_batch_size': 32, 'gru_epochs': 10, 'cat_depth': 8, 'cat_learning_rate': 0.21602733784010725, 'cat_iterations': 200}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 35.05 seconds\n","CatBoost Training Time: 0.29 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:11:14,518] Trial 35 finished with value: 0.0021341669668013593 and parameters: {'gru_units': 48, 'gru_layers': 5, 'gru_learning_rate': 0.0006989059222408891, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.12303951884793071, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 66.94 seconds\n","CatBoost Training Time: 0.61 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:11:55,011] Trial 36 finished with value: 0.002078105516278145 and parameters: {'gru_units': 64, 'gru_layers': 3, 'gru_learning_rate': 0.0003668299056679956, 'gru_batch_size': 32, 'gru_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.25130689948031776, 'cat_iterations': 100}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 37.43 seconds\n","CatBoost Training Time: 0.34 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:13:36,858] Trial 37 finished with value: 0.0020833625812795697 and parameters: {'gru_units': 96, 'gru_layers': 5, 'gru_learning_rate': 0.0032559383826826905, 'gru_batch_size': 16, 'gru_epochs': 10, 'cat_depth': 7, 'cat_learning_rate': 0.1880621031958042, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 99.32 seconds\n","CatBoost Training Time: 0.31 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:13:58,604] Trial 38 finished with value: 0.005044793377986666 and parameters: {'gru_units': 48, 'gru_layers': 3, 'gru_learning_rate': 0.00019711157659614795, 'gru_batch_size': 64, 'gru_epochs': 20, 'cat_depth': 8, 'cat_learning_rate': 0.01522318415220229, 'cat_iterations': 200}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 20.25 seconds\n","CatBoost Training Time: 0.30 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:14:40,542] Trial 39 finished with value: 0.0026023368544768827 and parameters: {'gru_units': 80, 'gru_layers': 5, 'gru_learning_rate': 0.0014690480480560911, 'gru_batch_size': 32, 'gru_epochs': 20, 'cat_depth': 6, 'cat_learning_rate': 0.03937876714645197, 'cat_iterations': 150}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 39.80 seconds\n","CatBoost Training Time: 0.27 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:15:29,197] Trial 40 finished with value: 0.0022352720456704136 and parameters: {'gru_units': 112, 'gru_layers': 2, 'gru_learning_rate': 0.0005862836523901345, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.15157593756278948, 'cat_iterations': 100}. Best is trial 15 with value: 0.0019675758180903815.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 47.55 seconds\n","CatBoost Training Time: 0.24 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:16:58,736] Trial 41 finished with value: 0.001958437966442085 and parameters: {'gru_units': 64, 'gru_layers': 3, 'gru_learning_rate': 0.0004437915905335063, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.29910141717381666, 'cat_iterations': 150}. Best is trial 41 with value: 0.001958437966442085.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 87.64 seconds\n","CatBoost Training Time: 0.51 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:18:10,180] Trial 42 finished with value: 0.002035798896144834 and parameters: {'gru_units': 64, 'gru_layers': 3, 'gru_learning_rate': 0.00039646338924787016, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.2500133802166993, 'cat_iterations': 150}. Best is trial 41 with value: 0.001958437966442085.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 69.56 seconds\n","CatBoost Training Time: 0.49 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:18:57,325] Trial 43 finished with value: 0.001991287824703125 and parameters: {'gru_units': 64, 'gru_layers': 3, 'gru_learning_rate': 0.001183603799587222, 'gru_batch_size': 16, 'gru_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.2627189987473775, 'cat_iterations': 150}. Best is trial 41 with value: 0.001958437966442085.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 45.22 seconds\n","CatBoost Training Time: 0.52 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:20:28,379] Trial 44 finished with value: 0.0019657390815409266 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.002473078739152409, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.29634944809046887, 'cat_iterations': 150}. Best is trial 41 with value: 0.001958437966442085.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 89.34 seconds\n","CatBoost Training Time: 0.50 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:21:39,746] Trial 45 finished with value: 0.0020074768633510805 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.0020962076831214016, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.17658649511881774, 'cat_iterations': 200}. Best is trial 41 with value: 0.001958437966442085.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 69.31 seconds\n","CatBoost Training Time: 0.67 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:22:33,641] Trial 46 finished with value: 0.0020159538486340745 and parameters: {'gru_units': 96, 'gru_layers': 3, 'gru_learning_rate': 0.002593961083907694, 'gru_batch_size': 16, 'gru_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.2166236833343214, 'cat_iterations': 150}. Best is trial 41 with value: 0.001958437966442085.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 52.00 seconds\n","CatBoost Training Time: 0.50 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:24:10,036] Trial 47 finished with value: 0.001988333264294483 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.0029484227242648443, 'gru_batch_size': 16, 'gru_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.23224575360118246, 'cat_iterations': 150}. Best is trial 41 with value: 0.001958437966442085.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 93.95 seconds\n","CatBoost Training Time: 1.04 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:25:29,990] Trial 48 finished with value: 0.002098338944373401 and parameters: {'gru_units': 96, 'gru_layers': 3, 'gru_learning_rate': 0.004226906717480406, 'gru_batch_size': 16, 'gru_epochs': 10, 'cat_depth': 9, 'cat_learning_rate': 0.23240919037185623, 'cat_iterations': 100}. Best is trial 41 with value: 0.001958437966442085.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 78.49 seconds\n","CatBoost Training Time: 0.23 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 17:26:28,014] Trial 49 finished with value: 0.01846419580994496 and parameters: {'gru_units': 80, 'gru_layers': 3, 'gru_learning_rate': 0.00333039687055855, 'gru_batch_size': 16, 'gru_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.010042713682938033, 'cat_iterations': 150}. Best is trial 41 with value: 0.001958437966442085.\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU Training Time: 56.33 seconds\n","CatBoost Training Time: 0.51 seconds\n","Best hyperparameters: {'gru_units': 64, 'gru_layers': 3, 'gru_learning_rate': 0.0004437915905335063, 'gru_batch_size': 16, 'gru_epochs': 20, 'cat_depth': 10, 'cat_learning_rate': 0.29910141717381666, 'cat_iterations': 150}\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import GRU, Dense, Bidirectional\n","import catboost as cb\n","import optuna\n","from sklearn.metrics import mean_squared_error\n","import time  # For tracking training time\n","\n","# Function to train Bi-GRU model\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","def train_bi_gru(X_train, Y_train, X_val, Y_val, units, layers, learning_rate, batch_size, epochs):\n","    model = keras.Sequential()\n","\n","    for _ in range(layers - 1):  # All except last layer have return_sequences=True\n","        model.add(Bidirectional(GRU(units, return_sequences=True)))\n","\n","    model.add(Bidirectional(GRU(units)))  # Last GRU layer\n","    model.add(Dense(1))  # Output layer\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n","\n","    # Early Stopping Callback\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","\n","    start_time = time.time()  # Start time tracking\n","    history = model.fit(X_train, Y_train,\n","                        validation_data=(X_val, Y_val),\n","                        epochs=epochs,\n","                        batch_size=batch_size,\n","                        verbose=0,\n","                        callbacks=[early_stopping])  # Add early stopping\n","    gru_train_time = time.time() - start_time  # End time tracking\n","\n","    return model, history, gru_train_time\n","\n","\n","# Objective function for Optuna\n","def objective(trial):\n","    units = trial.suggest_int(\"gru_units\", 32, 128, step=16)\n","    layers = trial.suggest_categorical(\"gru_layers\", [2, 3, 5])\n","    learning_rate = trial.suggest_loguniform(\"gru_learning_rate\", 1e-4, 1e-2)\n","    batch_size = trial.suggest_categorical(\"gru_batch_size\", [16, 32, 64])\n","    epochs = trial.suggest_int(\"gru_epochs\", 10, 50, step=10)\n","\n","    # Reshape input for GRU\n","    X_train_r = np.expand_dims(X_train, axis=-1)\n","    X_val_r = np.expand_dims(X_val, axis=-1)\n","\n","    # Train Bi-GRU\n","    model, _, gru_train_time = train_bi_gru(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","    Y_val_pred_gru = model.predict(X_val_r).flatten()\n","\n","    # Prepare data for CatBoost\n","    X_val_cat = np.column_stack([Y_val_pred_gru])\n","\n","    cat_params = {\n","        \"depth\": trial.suggest_int(\"cat_depth\", 3, 10),\n","        \"learning_rate\": trial.suggest_loguniform(\"cat_learning_rate\", 0.01, 0.3),\n","        \"iterations\": trial.suggest_int(\"cat_iterations\", 50, 200, step=50),\n","        \"loss_function\": \"RMSE\",\n","        \"verbose\": 0\n","    }\n","\n","    # Train CatBoost\n","    start_time = time.time()  # Start time tracking\n","    cat_model = cb.CatBoostRegressor(**cat_params)\n","    cat_model.fit(X_val_cat, Y_val)\n","    cat_train_time = time.time() - start_time  # End time tracking\n","\n","    # Predict and evaluate\n","    Y_val_pred_cat = cat_model.predict(X_val_cat)\n","    rmse = np.sqrt(mean_squared_error(Y_val, Y_val_pred_cat))\n","\n","    # Print training times\n","    print(f\"Bi-GRU Training Time: {gru_train_time:.2f} seconds\")\n","    print(f\"CatBoost Training Time: {cat_train_time:.2f} seconds\")\n","\n","    return rmse\n","\n","# Run Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=50)\n","\n","# Best hyperparameters\n","print(\"Best hyperparameters:\", study.best_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vMjiWdmh_pV8","outputId":"8e97bb38-7b93-4c04-ef3d-cc934516662a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\n","Final Model Performance:\n","Bi-GRU Training Time: 129.37 seconds\n","CatBoost Training Time: 0.42 seconds\n","\n","Bi-GRU Train Prediction Time: 1.5987 seconds\n","Bi-GRU Validation Prediction Time: 0.2478 seconds\n","Bi-GRU Test Prediction Time: 0.3533 seconds\n","\n","CatBoost Train Prediction Time: 0.0032 seconds\n","CatBoost Validation Prediction Time: 0.0013 seconds\n","CatBoost Test Prediction Time: 0.0013 seconds\n","\n","Train Set Metrics:\n","MAE: 0.0038, MSE: 0.0000, RMSE: 0.0056, R²: 0.9998, MAPE: 1.11%\n","\n","Validation Set Metrics:\n","MAE: 0.1579, MSE: 0.0308, RMSE: 0.1755, R²: -4.2362, MAPE: 8.86%\n","\n","Test Set Metrics:\n","MAE: 0.4269, MSE: 0.1883, RMSE: 0.4339, R²: -30.1951, MAPE: 21.06%\n"]}],"source":["from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import time\n","from catboost import CatBoostRegressor\n","\n","# Retrieve best hyperparameters from Optuna study\n","best_params = study.best_params\n","units = best_params[\"gru_units\"]\n","layers = best_params[\"gru_layers\"]\n","learning_rate = best_params[\"gru_learning_rate\"]\n","batch_size = best_params[\"gru_batch_size\"]\n","epochs = best_params[\"gru_epochs\"]\n","# Use correct parameter names from Optuna study\n","n_estimators = best_params[\"cat_iterations\"]  # Changed from 'catboost_n_estimators'\n","catboost_learning_rate = best_params[\"cat_learning_rate\"]  # Changed from 'catboost_learning_rate'\n","depth = best_params[\"cat_depth\"]  # Changed from 'catboost_depth'\n","\n","# Reshape input for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train final Bi-GRU model\n","final_gru, _, gru_train_time = train_bi_gru(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","\n","# Predictions with timing\n","start_time = time.time()\n","Y_train_pred_gru = final_gru.predict(X_train_r).flatten()\n","gru_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_gru = final_gru.predict(X_val_r).flatten()\n","gru_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_gru = final_gru.predict(X_test_r).flatten()\n","gru_test_pred_time = time.time() - start_time\n","\n","# Prepare data for CatBoost\n","X_train_cb = np.column_stack([Y_train_pred_gru])\n","X_val_cb = np.column_stack([Y_val_pred_gru])\n","X_test_cb = np.column_stack([Y_test_pred_gru])\n","\n","# Train final CatBoost model\n","start_time = time.time()\n","final_cb = CatBoostRegressor(iterations=n_estimators, learning_rate=catboost_learning_rate, depth=depth, loss_function='RMSE', verbose=0)\n","final_cb.fit(X_train_cb, Y_train, eval_set=(X_val_cb, Y_val), early_stopping_rounds=10)\n","cb_train_time = time.time() - start_time\n","\n","# CatBoost Predictions with timing\n","start_time = time.time()\n","Y_train_pred_cb = final_cb.predict(X_train_cb)\n","cb_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_cb = final_cb.predict(X_val_cb)\n","cb_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_cb = final_cb.predict(X_test_cb)\n","cb_test_pred_time = time.time() - start_time\n","\n","# Compute Metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","metrics_train = compute_metrics(Y_train, Y_train_pred_cb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_cb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_cb)\n","\n","# Print results\n","print(\"\\nFinal Model Performance:\")\n","print(f\"Bi-GRU Training Time: {gru_train_time:.2f} seconds\")\n","print(f\"CatBoost Training Time: {cb_train_time:.2f} seconds\\n\")\n","\n","print(f\"Bi-GRU Train Prediction Time: {gru_train_pred_time:.4f} seconds\")\n","print(f\"Bi-GRU Validation Prediction Time: {gru_val_pred_time:.4f} seconds\")\n","print(f\"Bi-GRU Test Prediction Time: {gru_test_pred_time:.4f} seconds\\n\")\n","\n","print(f\"CatBoost Train Prediction Time: {cb_train_pred_time:.4f} seconds\")\n","print(f\"CatBoost Validation Prediction Time: {cb_val_pred_time:.4f} seconds\")\n","print(f\"CatBoost Test Prediction Time: {cb_test_pred_time:.4f} seconds\\n\")\n","\n","print(\"Train Set Metrics:\")\n","print(f\"MAE: {metrics_train[0]:.4f}, MSE: {metrics_train[1]:.4f}, RMSE: {metrics_train[2]:.4f}, R²: {metrics_train[3]:.4f}, MAPE: {metrics_train[4]:.2f}%\")\n","\n","print(\"\\nValidation Set Metrics:\")\n","print(f\"MAE: {metrics_val[0]:.4f}, MSE: {metrics_val[1]:.4f}, RMSE: {metrics_val[2]:.4f}, R²: {metrics_val[3]:.4f}, MAPE: {metrics_val[4]:.2f}%\")\n","\n","print(\"\\nTest Set Metrics:\")\n","print(f\"MAE: {metrics_test[0]:.4f}, MSE: {metrics_test[1]:.4f}, RMSE: {metrics_test[2]:.4f}, R²: {metrics_test[3]:.4f}, MAPE: {metrics_test[4]:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jq8rJfxWMyyQ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BI4AaPbNN7Z6"},"source":["##BOHB"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FYmpI7GDOWBj","outputId":"4722c166-6fbd-408f-b04d-58e14d04b409"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Bi-GRU with 2 layers...\n","Training Bi-GRU with 3 layers...\n","Training Bi-GRU with 5 layers...\n","Train Metrics: (0.0033615227114729552, 2.6511661817509874e-05, 0.005148947641752621, 0.9998485229723106, 0.9575937042076411) Time: 82.86871576309204\n","Validation Metrics: (0.15749649411088998, 0.0306671725905407, 0.17512045166267903, -4.21413688881562, 8.84029753373646) Time: 0.34781718254089355\n","Test Metrics: (0.42647160941752077, 0.18791310403016853, 0.4334894508868336, -30.13685373255308, 21.03850665612964) Time: 0.055907487869262695\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiGRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Bi-GRU Configurations\n","gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","gru_features = []\n","\n","for num_layers in gru_layers:\n","    print(f\"Training Bi-GRU with {num_layers} layers...\")\n","\n","    gru_model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        gru_model.train()\n","        optimizer.zero_grad()\n","        outputs = gru_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    # Extract Feature Representations\n","    gru_model.eval()\n","    with torch.no_grad():\n","        val_start = time.time()\n","        train_features = gru_model(X_train_torch).numpy()\n","        val_features = gru_model(X_val_torch).numpy()\n","        val_time = time.time() - val_start\n","\n","        test_start = time.time()\n","        test_features = gru_model(X_test_torch).numpy()\n","        test_time = time.time() - test_start\n","\n","    gru_features.append((train_features, val_features, test_features, train_time, val_time, test_time))\n","\n","# Concatenate Features from All Layers\n","final_train_features = np.hstack([feat[0] for feat in gru_features])\n","final_val_features = np.hstack([feat[1] for feat in gru_features])\n","final_test_features = np.hstack([feat[2] for feat in gru_features])\n","\n","total_train_time = sum([feat[3] for feat in gru_features])\n","total_val_time = sum([feat[4] for feat in gru_features])\n","total_test_time = sum([feat[5] for feat in gru_features])\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"subsample\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for CatBoost\n","class CatBoostWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = CatBoostRegressor(\n","            iterations=config[\"n_estimators\"],\n","            learning_rate=config[\"learning_rate\"],\n","            depth=config[\"depth\"],\n","            subsample=config[\"subsample\"],\n","            loss_function='RMSE',\n","            verbose=False,\n","            random_seed=42\n","        )\n","        model.fit(final_train_features, Y_train)\n","        Y_val_pred = model.predict(final_val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"bigru_catboost_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = CatBoostWorker(nameserver=\"127.0.0.2\", run_id=\"bigru_catboost_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"bigru_catboost_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best CatBoost Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_catboost_model = CatBoostRegressor(\n","    iterations=best_params[\"n_estimators\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    depth=best_params[\"depth\"],\n","    subsample=best_params[\"subsample\"],\n","    loss_function='RMSE',\n","    verbose=False,\n","    random_seed=42\n",")\n","\n","best_catboost_model.fit(final_train_features, Y_train)\n","\n","# Predictions\n","Y_train_pred = best_catboost_model.predict(final_train_features)\n","Y_val_pred = best_catboost_model.predict(final_val_features)\n","Y_test_pred = best_catboost_model.predict(final_test_features)\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", total_train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", total_val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", total_test_time)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPh27TjOOcqB"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"6kQK7gxnSdQD"},"source":["#LightBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jRgnHwF_SfvH","outputId":"4682e739-dc71-4856-cae5-f97de79ff402"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.13.1)\n"]}],"source":["!pip install lightgbm"]},{"cell_type":"markdown","metadata":{"id":"0mCOUJW_Suz-"},"source":["##Initial"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_YHD4hFgSiUe","outputId":"ad586f3b-0826-45ea-b5da-4704d77dbe90"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training Bi-GRU with 2 layers...\n","Epoch [10/100], Loss: 0.1158\n","Epoch [20/100], Loss: 0.0848\n","Epoch [30/100], Loss: 0.0509\n","Epoch [40/100], Loss: 0.0293\n","Epoch [50/100], Loss: 0.0206\n","Epoch [60/100], Loss: 0.0134\n","Epoch [70/100], Loss: 0.0082\n","Epoch [80/100], Loss: 0.0049\n","Epoch [90/100], Loss: 0.0027\n","Epoch [100/100], Loss: 0.0016\n","\n","Training Bi-GRU with 3 layers...\n","Epoch [10/100], Loss: 0.1152\n","Epoch [20/100], Loss: 0.0636\n","Epoch [30/100], Loss: 0.0346\n","Epoch [40/100], Loss: 0.0156\n","Epoch [50/100], Loss: 0.0052\n","Epoch [60/100], Loss: 0.0012\n","Epoch [70/100], Loss: 0.0012\n","Epoch [80/100], Loss: 0.0011\n","Epoch [90/100], Loss: 0.0008\n","Epoch [100/100], Loss: 0.0007\n","\n","Training Bi-GRU with 5 layers...\n","Epoch [10/100], Loss: 0.1101\n","Epoch [20/100], Loss: 0.0569\n","Epoch [30/100], Loss: 0.0293\n","Epoch [40/100], Loss: 0.0058\n","Epoch [50/100], Loss: 0.0038\n","Epoch [60/100], Loss: 0.0023\n","Epoch [70/100], Loss: 0.0016\n","Epoch [80/100], Loss: 0.0014\n","Epoch [90/100], Loss: 0.0012\n","Epoch [100/100], Loss: 0.0011\n","\n","Training LightGBM on Combined Bi-GRU Embeddings...\n","[50]\ttraining's rmse: 0.0325506\tvalid_1's rmse: 0.259504\n","[100]\ttraining's rmse: 0.0049441\tvalid_1's rmse: 0.180425\n","[150]\ttraining's rmse: 0.00420921\tvalid_1's rmse: 0.174574\n","[200]\ttraining's rmse: 0.00416906\tvalid_1's rmse: 0.173909\n","\n","Final Model Performance\n","\n","               Model    Dataset      MAE      MSE     RMSE         R²      MAPE   Time (s)\n","Bi-GRU(2,3,5) + LGBM      Train 0.002806 0.000017 0.004169   0.999901  0.840923 101.185992\n","Bi-GRU(2,3,5) + LGBM Validation 0.156165 0.030244 0.173909  -4.142237  8.764028   0.447702\n","Bi-GRU(2,3,5) + LGBM       Test 0.425123 0.186764 0.432163 -29.946526 20.971483   0.527967\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import lightgbm as lgb\n","import pandas as pd\n","import time\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(BiGRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bi_gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.bi_gru(x, h0)\n","        return out[:, -1, :]\n","\n","# Set Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).to(device)\n","\n","# Store embeddings for LGBM\n","train_embeddings, val_embeddings, test_embeddings = [], [], []\n","train_time, val_time, test_time = 0, 0, 0\n","\n","# Train multiple Bi-GRU models\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining Bi-GRU with {num_layers} layers...\")\n","    model = BiGRUModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Train Bi-GRU\n","    start_time = time.time()\n","    model.train()\n","    for epoch in range(num_epochs):\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","    train_time += time.time() - start_time\n","\n","    # Extract embeddings\n","    model.eval()\n","    with torch.no_grad():\n","        start_time = time.time()\n","        train_embed = model(X_train_torch).cpu().numpy()\n","        val_embed = model(X_val_torch).cpu().numpy()\n","        test_embed = model(X_test_torch).cpu().numpy()\n","        val_time += time.time() - start_time\n","        test_time += time.time() - start_time\n","\n","    train_embeddings.append(train_embed)\n","    val_embeddings.append(val_embed)\n","    test_embeddings.append(test_embed)\n","\n","# Concatenate embeddings from all Bi-GRU models\n","X_train_lgb = np.concatenate(train_embeddings, axis=1)\n","X_val_lgb = np.concatenate(val_embeddings, axis=1)\n","X_test_lgb = np.concatenate(test_embeddings, axis=1)\n","\n","# Ensure correct label shape\n","Y_train_lgb = Y_train.values.flatten()\n","Y_val_lgb = Y_val.values.flatten()\n","Y_test_lgb = Y_test.values.flatten()\n","\n","# Train LightGBM on Bi-GRU embeddings\n","print(\"\\nTraining LightGBM on Combined Bi-GRU Embeddings...\")\n","start_time = time.time()\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train_lgb)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val_lgb, reference=lgb_train)\n","\n","lgb_params = {\n","    \"objective\": \"regression\",\n","    \"metric\": \"rmse\",\n","    \"boosting_type\": \"gbdt\",\n","    \"learning_rate\": 0.05,\n","    \"num_leaves\": 31\n","}\n","\n","lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","train_time += time.time() - start_time\n","\n","# Predictions\n","start_time = time.time()\n","train_pred_lgb = lgb_model.predict(X_train_lgb)\n","val_pred_lgb = lgb_model.predict(X_val_lgb)\n","test_pred_lgb = lgb_model.predict(X_test_lgb)\n","test_time += time.time() - start_time\n","\n","# Compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100  # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# Store results\n","results_df = pd.DataFrame([\n","    [\"Bi-GRU(2,3,5) + LGBM\", \"Train\", *compute_metrics(Y_train_lgb, train_pred_lgb), train_time],\n","    [\"Bi-GRU(2,3,5) + LGBM\", \"Validation\", *compute_metrics(Y_val_lgb, val_pred_lgb), val_time],\n","    [\"Bi-GRU(2,3,5) + LGBM\", \"Test\", *compute_metrics(Y_test_lgb, test_pred_lgb), test_time]\n","], columns=[\"Model\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\", \"Time (s)\"])\n","\n","print(\"\\nFinal Model Performance\\n\")\n","print(results_df.to_string(index=False))\n"]},{"cell_type":"markdown","metadata":{"id":"f643oizmUBvm"},"source":["##Optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MWEnTCH9mnhz","outputId":"1c1da597-4512-4f34-c8a4-37a210e81c1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"]}],"source":["!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QaZ6HUKumq-n","outputId":"a8c3a59a-56cc-4186-c773-7970c5ebe1b6"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2025-03-04 19:49:44,003] A new study created in memory with name: no-name-bba8c814-8e9a-4b10-9d02-02a615d627d6\n","[I 2025-03-04 19:49:46,697] Trial 0 finished with value: 8.782098667699797 and parameters: {'num_leaves': 21, 'learning_rate': 0.06513857349560939, 'max_depth': 8, 'min_data_in_leaf': 25}. Best is trial 0 with value: 8.782098667699797.\n","[I 2025-03-04 19:49:47,836] Trial 1 finished with value: 13.758764382183927 and parameters: {'num_leaves': 32, 'learning_rate': 0.012995895208448776, 'max_depth': 7, 'min_data_in_leaf': 19}. Best is trial 0 with value: 8.782098667699797.\n","[I 2025-03-04 19:49:49,608] Trial 2 finished with value: 8.795129524412438 and parameters: {'num_leaves': 33, 'learning_rate': 0.05080583201074866, 'max_depth': 3, 'min_data_in_leaf': 22}. Best is trial 0 with value: 8.782098667699797.\n","[I 2025-03-04 19:49:49,790] Trial 3 finished with value: 9.457553064726325 and parameters: {'num_leaves': 46, 'learning_rate': 0.022677597775786953, 'max_depth': 8, 'min_data_in_leaf': 16}. Best is trial 0 with value: 8.782098667699797.\n","[I 2025-03-04 19:49:49,972] Trial 4 finished with value: 8.7816066401935 and parameters: {'num_leaves': 48, 'learning_rate': 0.060864484252031746, 'max_depth': 9, 'min_data_in_leaf': 20}. Best is trial 4 with value: 8.7816066401935.\n","[I 2025-03-04 19:49:50,066] Trial 5 finished with value: 8.783858799579516 and parameters: {'num_leaves': 21, 'learning_rate': 0.08566542035528608, 'max_depth': 4, 'min_data_in_leaf': 23}. Best is trial 4 with value: 8.7816066401935.\n","[I 2025-03-04 19:49:50,141] Trial 6 finished with value: 9.033201027349042 and parameters: {'num_leaves': 27, 'learning_rate': 0.031132051946258493, 'max_depth': 3, 'min_data_in_leaf': 7}. Best is trial 4 with value: 8.7816066401935.\n","[I 2025-03-04 19:49:50,231] Trial 7 finished with value: 8.783190179425562 and parameters: {'num_leaves': 34, 'learning_rate': 0.07354007170439952, 'max_depth': 4, 'min_data_in_leaf': 23}. Best is trial 4 with value: 8.7816066401935.\n","[I 2025-03-04 19:49:50,386] Trial 8 finished with value: 9.458762092344372 and parameters: {'num_leaves': 37, 'learning_rate': 0.022775885881898664, 'max_depth': 10, 'min_data_in_leaf': 26}. Best is trial 4 with value: 8.7816066401935.\n","[I 2025-03-04 19:49:50,507] Trial 9 finished with value: 13.574950835646979 and parameters: {'num_leaves': 48, 'learning_rate': 0.013691246845969151, 'max_depth': 4, 'min_data_in_leaf': 29}. Best is trial 4 with value: 8.7816066401935.\n","[I 2025-03-04 19:49:50,697] Trial 10 finished with value: 8.788361110533373 and parameters: {'num_leaves': 42, 'learning_rate': 0.04502668319866818, 'max_depth': 10, 'min_data_in_leaf': 13}. Best is trial 4 with value: 8.7816066401935.\n","[I 2025-03-04 19:49:50,830] Trial 11 finished with value: 9.311031052058619 and parameters: {'num_leaves': 20, 'learning_rate': 0.058212350223262634, 'max_depth': 8, 'min_data_in_leaf': 30}. Best is trial 4 with value: 8.7816066401935.\n","[I 2025-03-04 19:49:51,007] Trial 12 finished with value: 8.781371366736277 and parameters: {'num_leaves': 41, 'learning_rate': 0.09416625884287057, 'max_depth': 9, 'min_data_in_leaf': 17}. Best is trial 12 with value: 8.781371366736277.\n","[I 2025-03-04 19:49:51,203] Trial 13 finished with value: 8.781375496384026 and parameters: {'num_leaves': 42, 'learning_rate': 0.09254174975274915, 'max_depth': 9, 'min_data_in_leaf': 11}. Best is trial 12 with value: 8.781371366736277.\n","[I 2025-03-04 19:49:51,378] Trial 14 finished with value: 8.781408095388786 and parameters: {'num_leaves': 40, 'learning_rate': 0.09847802648466172, 'max_depth': 6, 'min_data_in_leaf': 11}. Best is trial 12 with value: 8.781371366736277.\n","[I 2025-03-04 19:49:51,595] Trial 15 finished with value: 8.808199009885742 and parameters: {'num_leaves': 43, 'learning_rate': 0.03837304706934734, 'max_depth': 6, 'min_data_in_leaf': 5}. Best is trial 12 with value: 8.781371366736277.\n","[I 2025-03-04 19:49:51,773] Trial 16 finished with value: 8.781377221842854 and parameters: {'num_leaves': 39, 'learning_rate': 0.08573000524527262, 'max_depth': 9, 'min_data_in_leaf': 15}. Best is trial 12 with value: 8.781371366736277.\n","[I 2025-03-04 19:49:51,962] Trial 17 finished with value: 8.955795370453377 and parameters: {'num_leaves': 44, 'learning_rate': 0.02932539094288496, 'max_depth': 9, 'min_data_in_leaf': 9}. Best is trial 12 with value: 8.781371366736277.\n","[I 2025-03-04 19:49:52,120] Trial 18 finished with value: 8.78146655548098 and parameters: {'num_leaves': 30, 'learning_rate': 0.09870598876553867, 'max_depth': 7, 'min_data_in_leaf': 13}. Best is trial 12 with value: 8.781371366736277.\n","[I 2025-03-04 19:49:52,331] Trial 19 finished with value: 8.790725674910394 and parameters: {'num_leaves': 50, 'learning_rate': 0.04345859819478478, 'max_depth': 10, 'min_data_in_leaf': 17}. Best is trial 12 with value: 8.781371366736277.\n"]},{"name":"stdout","output_type":"stream","text":["Best Bi-GRU Parameters: {'hidden_size': 96, 'num_layers': 4, 'learning_rate': 0.00011638809437457186}\n","Train Time: 0.748131275177002 Validation Time: 0.3791663646697998 Test Time: 0.2611074447631836\n","Train Metrics: (0.0035319614574284303, 2.8334848305520183e-05, 0.005323048779179107, 0.9998381060142177, 1.0166971522529649)\n","Validation Metrics: (0.15646807413973954, 0.030340420116489145, 0.17418501691158497, -4.158581322894635, 8.781371366736277)\n","Test Metrics: (0.4254304256586718, 0.18702611746711667, 0.43246516329886814, -29.989881699820057, 20.98676978441127)\n"]}],"source":["import time\n","import torch\n","import lightgbm as lgb\n","import optuna\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Ensure model is in evaluation mode\n","best_gru.eval()\n","\n","# Disable gradient tracking\n","torch.no_grad()\n","\n","# Record train time\n","train_time_start = time.time()\n","train_pred = best_gru(X_train_torch).detach().cpu().numpy()\n","train_time = time.time() - train_time_start\n","\n","# Record validation time\n","val_time_start = time.time()\n","val_pred = best_gru(X_val_torch).detach().cpu().numpy()\n","val_time = time.time() - val_time_start\n","\n","# Record test time\n","test_time_start = time.time()\n","test_pred = best_gru(X_test_torch).detach().cpu().numpy()\n","test_time = time.time() - test_time_start\n","\n","# Inverse transform predictions\n","X_train_lgb = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","X_val_lgb = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","X_test_lgb = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","# ----------- LightGBM Optimization -----------\n","def objective_lgb(trial):\n","    params = {\n","        \"objective\": \"regression\", \"metric\": \"rmse\", \"boosting_type\": \"gbdt\",\n","        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 50),\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n","        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 30)\n","    }\n","    lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","    lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200)\n","    val_pred = model.predict(X_val_lgb)\n","    _, _, _, _, mape = compute_metrics(Y_val.values.flatten(), val_pred.flatten())\n","    return mape\n","\n","# Run optimization\n","study_lgb = optuna.create_study(direction=\"minimize\")\n","study_lgb.optimize(objective_lgb, n_trials=20)\n","\n","# Get best parameters\n","best_lgb_params = study_lgb.best_params\n","\n","# Train final LightGBM model\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","final_lgb = lgb.train(best_lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200)\n","\n","# Make predictions\n","train_pred_lgb = final_lgb.predict(X_train_lgb)\n","val_pred_lgb = final_lgb.predict(X_val_lgb)\n","test_pred_lgb = final_lgb.predict(X_test_lgb)\n","\n","# Compute metrics\n","metrics_train_lgb = compute_metrics(Y_train.values.flatten(), train_pred_lgb.flatten())\n","metrics_val_lgb = compute_metrics(Y_val.values.flatten(), val_pred_lgb.flatten())\n","metrics_test_lgb = compute_metrics(Y_test.values.flatten(), test_pred_lgb.flatten())\n","\n","# Print results\n","print(\"Best Bi-GRU Parameters:\", {'hidden_size': 96, 'num_layers': 4, 'learning_rate': 0.00011638809437457186})\n","print(\"Train Time:\", train_time, \"Validation Time:\", val_time, \"Test Time:\", test_time)\n","print(\"Train Metrics:\", metrics_train_lgb)\n","print(\"Validation Metrics:\", metrics_val_lgb)\n","print(\"Test Metrics:\", metrics_test_lgb)\n"]},{"cell_type":"markdown","metadata":{"id":"9yeTxdFbvagh"},"source":["##bohb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SS5yFZmwJZI","outputId":"86eeae05-ae9e-4431-9ad5-22d06305e1ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting configSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.13.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from configSpace) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from configSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from configSpace) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, configSpace, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=7ad9b61528c628cd2244ee9f1dd002aadfa15166780868e1cc68da76ad2b12a8\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for configSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configSpace: filename=configspace-1.2.1-py3-none-any.whl size=115950 sha256=14792c5edeebe322b6d0e7957e4a7233cdb348a20c7ce319376b3eb04e561bdf\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35182 sha256=fe922eae57de0696057c652ed56be70755b8761547899bedfc03a2e14f244791\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster configSpace netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, configSpace, hpbandster\n","Successfully installed Pyro4-4.82 configSpace-1.2.1 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}],"source":["!pip install hpbandster configSpace"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2SVmeBVVwQ4A","outputId":"422d564a-cdfe-4c92-d7a8-4dd0d8f5cade"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Bi-GRU with 2 layers...\n","Bi-GRU (2 layers) training time: 17.14 seconds\n","Training Bi-GRU with 3 layers...\n","Bi-GRU (3 layers) training time: 26.12 seconds\n","Training Bi-GRU with 5 layers...\n","Bi-GRU (5 layers) training time: 46.59 seconds\n","\n","Running BOHB for Stacked Bi-GRU + LightGBM...\n","\n","BOHB optimization time: 32.67 seconds\n","\n","Best Parameters for Stacked Bi-GRU + LightGBM: {'feature_fraction': 0.7221661322183, 'learning_rate': 0.282900498938, 'max_depth': 10, 'num_leaves': 32}\n","\n","Training set metrics: (0.003233864426046888, 2.270418553469974e-05, 0.004764890925792503, 0.9998702773683303, 0.974916464028059)\n","\n","Validation set metrics: (0.15607493518680515, 0.030215614808520485, 0.1738263927271129, -4.137361500353835, 8.758853701947594)\n","\n","Test set metrics: (0.42503090440830055, 0.18668634009313603, 0.4320721468610723, -29.93358121747814, 20.966917400522703)\n"]}],"source":["import time\n","import numpy as np\n","import lightgbm as lgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiGRUModel, self).__init__()\n","        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-GRU doubles hidden size\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# GRU Configurations (2, 3, and 5 layers)\n","gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store GRU feature representations\n","gru_features = []\n","\n","for num_layers in gru_layers:\n","    print(f\"Training Bi-GRU with {num_layers} layers...\")\n","    start_time = time.time()\n","\n","    gru_model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        gru_model.train()\n","        optimizer.zero_grad()\n","        outputs = gru_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Extract Feature Representations\n","    gru_model.eval()\n","    with torch.no_grad():\n","        train_features = gru_model(X_train_torch).numpy()\n","        val_features = gru_model(X_val_torch).numpy()\n","        test_features = gru_model(X_test_torch).numpy()\n","\n","    gru_features.append((train_features, val_features, test_features))\n","    print(f\"Bi-GRU ({num_layers} layers) training time: {time.time() - start_time:.2f} seconds\")\n","\n","# Stack extracted features\n","train_features_stacked = np.hstack([feat[0] for feat in gru_features])\n","val_features_stacked = np.hstack([feat[1] for feat in gru_features])\n","test_features_stacked = np.hstack([feat[2] for feat in gru_features])\n","\n","# Define ConfigSpace for BOHB (LightGBM)\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_leaves\", 20, 300, default_value=50))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 12, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"feature_fraction\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for LightGBM\n","class LightGBMWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = lgb.LGBMRegressor(\n","            num_leaves=config[\"num_leaves\"],\n","            max_depth=config[\"max_depth\"],\n","            learning_rate=config[\"learning_rate\"],\n","            feature_fraction=config[\"feature_fraction\"],\n","            random_state=42\n","        )\n","        model.fit(self.train_features, Y_train)\n","        Y_val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB for stacked GRU features\n","print(\"\\nRunning BOHB for Stacked Bi-GRU + LightGBM...\")\n","start_time = time.time()\n","\n","NS = hpns.NameServer(run_id=\"stacked_bi_gru_lgb_bohb\", host=\"127.0.0.1\", port=None)\n","NS.start()\n","\n","worker = LightGBMWorker(\n","    train_features=train_features_stacked,\n","    val_features=val_features_stacked,\n","    nameserver=\"127.0.0.1\",\n","    run_id=\"stacked_bi_gru_lgb_bohb\"\n",")\n","worker.run(background=True)\n","\n","bohb = BOHB(\n","    configspace=get_config_space(),\n","    run_id=\"stacked_bi_gru_lgb_bohb\",\n","    nameserver=\"127.0.0.1\",\n","    min_budget=1,\n","    max_budget=3\n",")\n","\n","res = bohb.run(n_iterations=50)\n","\n","bohb.shutdown()\n","NS.shutdown()\n","\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_lgb_model = lgb.LGBMRegressor(\n","    num_leaves=best_params[\"num_leaves\"],\n","    max_depth=best_params[\"max_depth\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    feature_fraction=best_params[\"feature_fraction\"],\n","    random_state=42\n",")\n","\n","best_lgb_model.fit(train_features_stacked, Y_train)\n","\n","Y_train_pred = best_lgb_model.predict(train_features_stacked)\n","Y_val_pred = best_lgb_model.predict(val_features_stacked)\n","Y_test_pred = best_lgb_model.predict(test_features_stacked)\n","\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","print(f\"\\nBOHB optimization time: {time.time() - start_time:.2f} seconds\")\n","print(\"\\nBest Parameters for Stacked Bi-GRU + LightGBM:\", best_params)\n","print(\"\\nTraining set metrics:\", train_metrics)\n","print(\"\\nValidation set metrics:\", val_metrics)\n","print(\"\\nTest set metrics:\", test_metrics)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k78IlOAkwinL"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["b3zZmLiLkbT4","kMvxyuvvnbxW","5UR4JmmbypjK","gS33VlPQ8Bo3","-IPkYtel-lC5","GaUtio4BA-bB","IQyCKpKoKBcV","9OybYRRv8B7L","y1MAPmqL_U1N","0mCOUJW_Suz-"],"provenance":[{"file_id":"1d3ABbbG-dsXZT_segII0YeR6xut99Q2X","timestamp":1741270683535}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}