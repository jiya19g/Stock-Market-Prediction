{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18Klsr2VMvQQ6iwEyoRgC1SLN4lNjE7kT","timestamp":1741271232677}],"collapsed_sections":["6MM-OJGXCCK2","MFeTjGxUCEpk","Pk2BtMJuCPi9","M10Oo7qqLZgm","uUCh_YB9Vf7S","qUH36NvmtM2G","PnPul0h2lqPU","UA4RDkuZXHto"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Initial"],"metadata":{"id":"6MM-OJGXCCK2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7gOgVN8kW8r"},"outputs":[],"source":["# Importing necessary libraries for data analysis and manipulation\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","# For handling warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93jX7ecsBi-o","outputId":"20262b93-4f89-4bbd-e8f7-f1b5ae39fee3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["df_aapl = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stocks/AAPL.csv')"],"metadata":{"id":"SqkQs1FVBnLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import boxcox\n","\n","df_aapl['Close_log'] = np.log(df_aapl['Close'] + 1)\n","df_aapl['Close_sqrt'] = np.sqrt(df_aapl['Close'])\n","df_aapl['Close_boxcox'], _ = boxcox(df_aapl['Close'] + 1)\n"],"metadata":{"id":"ttF4WNTkBxms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","skew_original = df_aapl['Close'].skew()\n","skew_log = df_aapl['Close_log'].skew()\n","skew_sqrt = df_aapl['Close_sqrt'].skew()\n","skew_boxcox = pd.Series(df_aapl['Close_boxcox']).skew()\n","\n","print(f\"Original Skewness: {skew_original}\")\n","print(f\"Log Transformation Skewness: {skew_log}\")\n","print(f\"Square Root Transformation Skewness: {skew_sqrt}\")\n","print(f\"Box-Cox Transformation Skewness: {skew_boxcox}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T7rMhFf7B0Rr","outputId":"6060330f-4be1-4dbc-a477-97df5545bd3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Skewness: 2.5045276102319933\n","Log Transformation Skewness: 0.8535555176510303\n","Square Root Transformation Skewness: 1.6211545809555206\n","Box-Cox Transformation Skewness: 0.43527466713563334\n"]}]},{"cell_type":"code","source":["\n","df_aapl['Open_log'] = np.log(df_aapl['Open'])\n","df_aapl['High_log'] = np.log(df_aapl['High'])\n","df_aapl['Low_log'] = np.log(df_aapl['Low'])\n","df_aapl['Adj Close_log'] = np.log(df_aapl['Adj Close'])\n","df_aapl['Volume_log'] = np.log(df_aapl['Volume'])\n","\n","\n","df_aapl['Open_sqrt'] = np.sqrt(df_aapl['Open'])\n","df_aapl['High_sqrt'] = np.sqrt(df_aapl['High'])\n","df_aapl['Low_sqrt'] = np.sqrt(df_aapl['Low'])\n","df_aapl['Adj Close_sqrt'] = np.sqrt(df_aapl['Adj Close'])\n","df_aapl['Volume_sqrt'] = np.sqrt(df_aapl['Volume'])\n","\n","from scipy.stats import boxcox\n","df_aapl['Open_boxcox'], _ = boxcox(df_aapl['Open'])\n","df_aapl['High_boxcox'], _ = boxcox(df_aapl['High'])\n","df_aapl['Low_boxcox'], _ = boxcox(df_aapl['Low'])\n","df_aapl['Adj Close_boxcox'], _ = boxcox(df_aapl['Adj Close'])"],"metadata":{"id":"FEyv3BNQB3TB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","skewness_before = df_aapl[['Open', 'High', 'Low', 'Adj Close', 'Volume']].skew()\n","skewness_after = df_aapl[['Open_log', 'High_log', 'Low_log', 'Adj Close_log',\n","                          'Open_sqrt', 'High_sqrt', 'Low_sqrt', 'Adj Close_sqrt', 'Volume_sqrt',\n","                          'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox']].skew()\n","\n","print(\"Skewness Before Transformation:\\n\", skewness_before)\n","print(\"\\nSkewness After Transformation:\\n\", skewness_after)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z8CzGUPAB4vn","outputId":"4ffeade1-b708-4227-e0e8-cd8850c23a03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness Before Transformation:\n"," Open         2.504632\n","High         2.502208\n","Low          2.506714\n","Adj Close    2.550677\n","Volume       3.565699\n","dtype: float64\n","\n","Skewness After Transformation:\n"," Open_log            0.482872\n","High_log            0.481997\n","Low_log             0.484246\n","Adj Close_log       0.494009\n","Open_sqrt           1.620771\n","High_sqrt           1.621456\n","Low_sqrt            1.620661\n","Adj Close_sqrt      1.679402\n","Volume_sqrt         1.299776\n","Open_boxcox         0.181226\n","High_boxcox         0.179749\n","Low_boxcox          0.182882\n","Adj Close_boxcox    0.180085\n","dtype: float64\n"]}]},{"cell_type":"code","source":["from scipy import stats\n","\n","df_aapl['Open_boxcox'], _ = stats.boxcox(df_aapl['Open'] + 1)\n","df_aapl['High_boxcox'], _ = stats.boxcox(df_aapl['High'] + 1)\n","df_aapl['Low_boxcox'], _ = stats.boxcox(df_aapl['Low'] + 1)\n","df_aapl['Adj Close_boxcox'], _ = stats.boxcox(df_aapl['Adj Close'] + 1)\n","df_aapl['Close_boxcox'], _ = stats.boxcox(df_aapl['Close'] + 1)\n","\n","skewness_after_boxcox = df_aapl[['Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox', 'Close_boxcox']].skew()\n","\n","print(\"Skewness After Box-Cox Transformation:\")\n","print(skewness_after_boxcox)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uSm4tARXB8Ys","outputId":"8bf5e01b-5bc5-4bef-f143-fa7163457989"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness After Box-Cox Transformation:\n","Open_boxcox         0.435237\n","High_boxcox         0.433381\n","Low_boxcox          0.437331\n","Adj Close_boxcox    0.458762\n","Close_boxcox        0.435275\n","dtype: float64\n"]}]},{"cell_type":"code","source":["\n","df_aapl_cleaned = df_aapl[['Date', 'Open', 'High', 'Low', 'Adj Close', 'Close', 'Volume',\n","                           'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox',\n","                           'Close_boxcox']]\n","\n","print(df_aapl_cleaned.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V8jxBy1MB-AY","outputId":"f11fec0e-70d0-4455-8031-f4e9f551a01c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["         Date      Open      High       Low  Adj Close     Close     Volume  \\\n","0  1980-12-12  0.128348  0.128906  0.128348   0.098943  0.128348  469033600   \n","1  1980-12-15  0.122210  0.122210  0.121652   0.093781  0.121652  175884800   \n","2  1980-12-16  0.113281  0.113281  0.112723   0.086898  0.112723  105728000   \n","3  1980-12-17  0.115513  0.116071  0.115513   0.089049  0.115513   86441600   \n","4  1980-12-18  0.118862  0.119420  0.118862   0.091630  0.118862   73449600   \n","\n","   Open_boxcox  High_boxcox  Low_boxcox  Adj Close_boxcox  Close_boxcox  \n","0     0.117689     0.118173    0.117674          0.092374      0.117689  \n","1     0.112503     0.112516    0.112016          0.087857      0.112030  \n","2     0.104886     0.104897    0.104395          0.081785      0.104407  \n","3     0.106798     0.107287    0.106786          0.083688      0.106798  \n","4     0.109657     0.110145    0.109644          0.085966      0.109657  \n"]}]},{"cell_type":"markdown","source":["#Train, Validation and Testing"],"metadata":{"id":"MFeTjGxUCEpk"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X = df_aapl_cleaned[['Open_boxcox', 'High_boxcox', 'Low_boxcox']]\n","Y = df_aapl_cleaned['Close_boxcox']\n","\n","X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, shuffle=False)\n","X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, shuffle=False)\n","\n","print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCaInU2aCJZJ","outputId":"f0542a5c-0ccd-43c7-bcc9-e36c08cc0d9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set: (7736, 3), Validation set: (1658, 3), Test set: (1658, 3)\n"]}]},{"cell_type":"markdown","source":["# XG Boost"],"metadata":{"id":"Pk2BtMJuCPi9"}},{"cell_type":"markdown","source":["## Initial"],"metadata":{"id":"31Yn1i4585D5"}},{"cell_type":"code","source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Enable GPU for TensorFlow\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train a Bi-LSTM model on GPU\n","def train_bi_lstm(X_train, Y_train, X_val, Y_val, layers):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(Bidirectional(LSTM(64, return_sequences=(layers > 1)), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(Bidirectional(LSTM(64, return_sequences=(_ < layers - 2))))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        start_time = time.time()\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","        train_time = time.time() - start_time\n","        return model, train_time\n","\n","# Reshaping input for LSTM\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train 2, 3, and 5-layer Bi-LSTM models\n","bi_lstm_models = {}\n","bi_lstm_predictions = {}\n","times = {}\n","\n","for layers in [2, 3, 5]:\n","    model, train_time = train_bi_lstm(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    Y_train_pred = model.predict(X_train_r)\n","    Y_val_pred = model.predict(X_val_r)\n","    Y_test_pred = model.predict(X_test_r)\n","\n","    bi_lstm_models[layers] = model\n","    bi_lstm_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","    times[f'Bi-LSTM-{layers}'] = train_time\n","\n","# Prepare input for XGBoost\n","X_train_xgb = np.column_stack([bi_lstm_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_xgb = np.column_stack([bi_lstm_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_xgb = np.column_stack([bi_lstm_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train XGBoost model\n","xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=3, objective='reg:squarederror', tree_method='gpu_hist')\n","\n","start_time = time.time()\n","xgb_model.fit(X_train_xgb, Y_train, eval_set=[(X_val_xgb, Y_val)], verbose=False)\n","times['XGBoost'] = time.time() - start_time\n","\n","# Predictions from XGBoost\n","start_time = time.time()\n","Y_train_pred_xgb = xgb_model.predict(X_train_xgb)\n","times['XGBoost Train'] = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_xgb = xgb_model.predict(X_val_xgb)\n","times['XGBoost Validate'] = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_xgb = xgb_model.predict(X_test_xgb)\n","times['XGBoost Test'] = time.time() - start_time\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_xgb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_xgb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_xgb)\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print training times\n","print(\"Training Times:\")\n","for model, t in times.items():\n","    print(f\"{model}: {t:.2f} seconds\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-A4tWseQCSHW","outputId":"901b16fd-7c9c-40c0-ebe1-28aa155c5676"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU activated for TensorFlow!\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","Train Metrics: MAE=0.0043, MSE=0.0000, RMSE=0.0061, R²=0.9998, MAPE=1.53%\n","Validation Metrics: MAE=0.1710, MSE=0.0351, RMSE=0.1874, R²=-4.9687, MAPE=9.61%\n","Test Metrics: MAE=0.4400, MSE=0.1996, RMSE=0.4468, R²=-32.0772, MAPE=21.71%\n","Training Times:\n","Bi-LSTM-2: 107.32 seconds\n","Bi-LSTM-3: 168.89 seconds\n","Bi-LSTM-5: 193.44 seconds\n","XGBoost: 0.95 seconds\n","XGBoost Train: 0.03 seconds\n","XGBoost Validate: 0.01 seconds\n","XGBoost Test: 0.00 seconds\n"]}]},{"cell_type":"markdown","source":["## Optuna"],"metadata":{"id":"M10Oo7qqLZgm"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhqC08wbIYOF","outputId":"75398af2-a02f-453a-8461-285cb37e9105"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n","import xgboost as xgb\n","import optuna\n","from sklearn.metrics import mean_squared_error\n","import time\n","\n","# Early stopping callback\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Function to train Bi-LSTM model\n","def train_bi_lstm(X_train, Y_train, X_val, Y_val, units, layers, learning_rate, batch_size, epochs):\n","    model = keras.Sequential()\n","\n","    for _ in range(layers - 1):  # All except last layer have return_sequences=True\n","        model.add(Bidirectional(LSTM(units, return_sequences=True)))\n","\n","    model.add(Bidirectional(LSTM(units)))  # Last LSTM layer\n","    model.add(Dense(1))  # Output layer\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n","\n","    # Early Stopping Callback\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","\n","    start_time = time.time()  # Start time tracking\n","    history = model.fit(X_train, Y_train,\n","                        validation_data=(X_val, Y_val),\n","                        epochs=epochs,\n","                        batch_size=batch_size,\n","                        verbose=0,\n","                        callbacks=[early_stopping])  # Add early stopping\n","    lstm_train_time = time.time() - start_time  # End time tracking\n","\n","    return model, history, lstm_train_time\n","\n","# Objective function for Optuna\n","def objective(trial):\n","    units = trial.suggest_int(\"lstm_units\", 32, 128, step=16)\n","    layers = trial.suggest_categorical(\"lstm_layers\", [2, 3, 5])\n","    learning_rate = trial.suggest_loguniform(\"lstm_learning_rate\", 1e-4, 1e-2)\n","    batch_size = trial.suggest_categorical(\"lstm_batch_size\", [16, 32, 64])\n","    epochs = trial.suggest_int(\"lstm_epochs\", 10, 50, step=10)\n","\n","    # Reshape input for LSTM\n","    X_train_r = np.expand_dims(X_train, axis=-1)\n","    X_val_r = np.expand_dims(X_val, axis=-1)\n","\n","    # Train Bi-LSTM\n","    model, _, lstm_train_time = train_bi_lstm(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","    Y_val_pred_lstm = model.predict(X_val_r).flatten()\n","\n","    # Prepare data for XGBoost\n","    X_val_xgb = np.column_stack([Y_val_pred_lstm])\n","\n","    xgb_params = {\n","        \"objective\": \"reg:squarederror\",\n","        \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 50, 200, step=50),\n","        \"learning_rate\": trial.suggest_loguniform(\"xgb_learning_rate\", 0.01, 0.3),\n","        \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 10),\n","    }\n","\n","    if tf.config.list_physical_devices(\"GPU\"):\n","        xgb_params[\"tree_method\"] = \"gpu_hist\"\n","\n","    # Check XGBoost version\n","    xgb_version = xgb.__version__.split(\".\")\n","    is_new_xgb = int(xgb_version[0]) >= 1 and int(xgb_version[1]) >= 6  # XGBoost >= 1.6 supports early stopping\n","\n","    # Train XGBoost\n","    start_time = time.time()  # Start time tracking\n","    xgb_model = xgb.XGBRegressor(**xgb_params)\n","\n","    if is_new_xgb:\n","        xgb_model.fit(X_val_xgb, Y_val, eval_set=[(X_val_xgb, Y_val)], early_stopping_rounds=10, verbose=False)\n","    else:\n","        xgb_model.fit(X_val_xgb, Y_val)\n","\n","    xgb_train_time = time.time() - start_time  # End time tracking\n","\n","    # Predict and evaluate\n","    Y_val_pred_xgb = xgb_model.predict(X_val_xgb)\n","    rmse = np.sqrt(mean_squared_error(Y_val, Y_val_pred_xgb))\n","\n","    # Print training times\n","    print(f\"Bi-LSTM Training Time: {lstm_train_time:.2f} seconds\")\n","    print(f\"XGBoost Training Time: {xgb_train_time:.2f} seconds\")\n","\n","    return rmse\n","\n","# Run Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=50)\n","\n","# Best hyperparameters\n","print(\"Best hyperparameters:\", study.best_params)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5GxUPCeTCV_X","outputId":"62eec6a8-f309-4fff-8c96-5df66fd577f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:05:23,991] A new study created in memory with name: no-name-757e911b-3e2a-4e47-aa8a-bd73603c3443\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:05:57,303] Trial 0 finished with value: 0.0020128628815114166 and parameters: {'lstm_units': 112, 'lstm_layers': 3, 'lstm_learning_rate': 0.00015381380399130418, 'lstm_batch_size': 64, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.09253312375187596, 'xgb_max_depth': 7}. Best is trial 0 with value: 0.0020128628815114166.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 31.75 seconds\n","XGBoost Training Time: 0.18 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:06:35,337] Trial 1 finished with value: 0.0020455239218749857 and parameters: {'lstm_units': 48, 'lstm_layers': 3, 'lstm_learning_rate': 0.00028708292434157277, 'lstm_batch_size': 32, 'lstm_epochs': 10, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.20559905064321313, 'xgb_max_depth': 5}. Best is trial 0 with value: 0.0020128628815114166.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 35.26 seconds\n","XGBoost Training Time: 0.13 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:07:26,059] Trial 2 finished with value: 0.005486764878843222 and parameters: {'lstm_units': 112, 'lstm_layers': 3, 'lstm_learning_rate': 0.00024838773593274566, 'lstm_batch_size': 16, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.02726458556078912, 'xgb_max_depth': 4}. Best is trial 0 with value: 0.0020128628815114166.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 49.24 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:07:46,436] Trial 3 finished with value: 0.037269859275288315 and parameters: {'lstm_units': 96, 'lstm_layers': 2, 'lstm_learning_rate': 0.00476791501694372, 'lstm_batch_size': 64, 'lstm_epochs': 20, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.014572920176909627, 'xgb_max_depth': 7}. Best is trial 0 with value: 0.0020128628815114166.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 18.90 seconds\n","XGBoost Training Time: 0.13 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:08:39,278] Trial 4 finished with value: 0.002025196468884066 and parameters: {'lstm_units': 96, 'lstm_layers': 3, 'lstm_learning_rate': 0.0003363720628281372, 'lstm_batch_size': 16, 'lstm_epochs': 50, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.11699027969736218, 'xgb_max_depth': 9}. Best is trial 0 with value: 0.0020128628815114166.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 51.33 seconds\n","XGBoost Training Time: 0.21 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:09:12,601] Trial 5 finished with value: 0.003402747070345936 and parameters: {'lstm_units': 48, 'lstm_layers': 2, 'lstm_learning_rate': 0.00018441581057444448, 'lstm_batch_size': 32, 'lstm_epochs': 10, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.016896902272311977, 'xgb_max_depth': 4}. Best is trial 0 with value: 0.0020128628815114166.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 31.57 seconds\n","XGBoost Training Time: 0.37 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:09:44,477] Trial 6 finished with value: 0.0020456946364262396 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.0001526186264862357, 'lstm_batch_size': 64, 'lstm_epochs': 20, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.06707810065381986, 'xgb_max_depth': 3}. Best is trial 0 with value: 0.0020128628815114166.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 29.76 seconds\n","XGBoost Training Time: 0.11 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:11:21,282] Trial 7 finished with value: 0.01376299186907047 and parameters: {'lstm_units': 96, 'lstm_layers': 5, 'lstm_learning_rate': 0.0005310270437751482, 'lstm_batch_size': 16, 'lstm_epochs': 40, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.034544874244794056, 'xgb_max_depth': 6}. Best is trial 0 with value: 0.0020128628815114166.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 93.76 seconds\n","XGBoost Training Time: 0.14 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:12:39,345] Trial 8 finished with value: 0.0020203572230988397 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.004460568051382336, 'lstm_batch_size': 16, 'lstm_epochs': 20, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.04607634672376894, 'xgb_max_depth': 3}. Best is trial 0 with value: 0.0020128628815114166.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 72.58 seconds\n","XGBoost Training Time: 0.21 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:13:31,205] Trial 9 finished with value: 0.0019927949272716223 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.004395980982829109, 'lstm_batch_size': 32, 'lstm_epochs': 50, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.08954024674606594, 'xgb_max_depth': 9}. Best is trial 9 with value: 0.0019927949272716223.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 49.00 seconds\n","XGBoost Training Time: 0.19 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:14:41,973] Trial 10 finished with value: 0.001994834983907579 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.001888364570989061, 'lstm_batch_size': 32, 'lstm_epochs': 50, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.2372430735608607, 'xgb_max_depth': 10}. Best is trial 9 with value: 0.0019927949272716223.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 67.94 seconds\n","XGBoost Training Time: 0.13 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:16:13,011] Trial 11 finished with value: 0.0020038849894253147 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.0017781781863952936, 'lstm_batch_size': 32, 'lstm_epochs': 50, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.27763092197224776, 'xgb_max_depth': 10}. Best is trial 9 with value: 0.0019927949272716223.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 88.21 seconds\n","XGBoost Training Time: 0.13 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:17:17,901] Trial 12 finished with value: 0.001994561734710926 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.0016116631177341466, 'lstm_batch_size': 32, 'lstm_epochs': 50, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.13711410042232938, 'xgb_max_depth': 9}. Best is trial 9 with value: 0.0019927949272716223.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 62.68 seconds\n","XGBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:18:33,207] Trial 13 finished with value: 0.001989979762265609 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.0020024382795956436, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.13629571835715382, 'xgb_max_depth': 8}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 73.09 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:19:16,334] Trial 14 finished with value: 0.0027903842772083044 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.009269411556921958, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.07242525273839208, 'xgb_max_depth': 8}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 40.23 seconds\n","XGBoost Training Time: 0.20 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:19:41,051] Trial 15 finished with value: 0.002003551354767929 and parameters: {'lstm_units': 64, 'lstm_layers': 2, 'lstm_learning_rate': 0.00352711099062692, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.13050854580882934, 'xgb_max_depth': 8}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 23.59 seconds\n","XGBoost Training Time: 0.14 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:20:50,122] Trial 16 finished with value: 0.0019935462111757983 and parameters: {'lstm_units': 80, 'lstm_layers': 5, 'lstm_learning_rate': 0.000933138080007209, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.17537243227823293, 'xgb_max_depth': 8}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 63.68 seconds\n","XGBoost Training Time: 0.13 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:21:39,111] Trial 17 finished with value: 0.0022404308119758774 and parameters: {'lstm_units': 80, 'lstm_layers': 5, 'lstm_learning_rate': 0.007468166181990161, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.08474910339211197, 'xgb_max_depth': 9}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 46.08 seconds\n","XGBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:22:49,183] Trial 18 finished with value: 0.0020563681742030993 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.0024422993157655008, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.0503447646470542, 'xgb_max_depth': 6}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 67.15 seconds\n","XGBoost Training Time: 0.19 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:23:03,364] Trial 19 finished with value: 0.0020292983839894354 and parameters: {'lstm_units': 128, 'lstm_layers': 2, 'lstm_learning_rate': 0.0008385154818915774, 'lstm_batch_size': 64, 'lstm_epochs': 50, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.16549243728716784, 'xgb_max_depth': 10}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 13.09 seconds\n","XGBoost Training Time: 0.13 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:23:58,363] Trial 20 finished with value: 0.046244538317068797 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.0027345167771572064, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.01023135637961738, 'xgb_max_depth': 7}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 52.79 seconds\n","XGBoost Training Time: 0.11 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:25:00,413] Trial 21 finished with value: 0.002000607404676455 and parameters: {'lstm_units': 80, 'lstm_layers': 5, 'lstm_learning_rate': 0.0011750650365529374, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.18913249748399275, 'xgb_max_depth': 8}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 59.79 seconds\n","XGBoost Training Time: 0.12 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:26:34,910] Trial 22 finished with value: 0.0019953185341837058 and parameters: {'lstm_units': 80, 'lstm_layers': 5, 'lstm_learning_rate': 0.0008185536267541631, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.10284580956292136, 'xgb_max_depth': 8}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 91.42 seconds\n","XGBoost Training Time: 0.33 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:27:53,552] Trial 23 finished with value: 0.001998451181341914 and parameters: {'lstm_units': 80, 'lstm_layers': 5, 'lstm_learning_rate': 0.0005781912474149415, 'lstm_batch_size': 32, 'lstm_epochs': 50, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.1523609947103838, 'xgb_max_depth': 9}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 75.55 seconds\n","XGBoost Training Time: 0.17 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:28:33,438] Trial 24 finished with value: 0.0019941107012020677 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.005831654320222694, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.11096166459889453, 'xgb_max_depth': 8}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 37.54 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:29:48,085] Trial 25 finished with value: 0.0019978753684342637 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.001258078357039726, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.06636998864770373, 'xgb_max_depth': 9}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 71.57 seconds\n","XGBoost Training Time: 0.34 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:30:35,276] Trial 26 finished with value: 0.002003578556844313 and parameters: {'lstm_units': 80, 'lstm_layers': 5, 'lstm_learning_rate': 0.003081868067394048, 'lstm_batch_size': 32, 'lstm_epochs': 50, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.2851257285403045, 'xgb_max_depth': 7}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 44.99 seconds\n","XGBoost Training Time: 0.10 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:31:57,423] Trial 27 finished with value: 0.0020030023415593696 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.0005170249073712472, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.19889559709947166, 'xgb_max_depth': 6}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 76.39 seconds\n","XGBoost Training Time: 0.41 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:33:03,267] Trial 28 finished with value: 0.0025746166446331603 and parameters: {'lstm_units': 48, 'lstm_layers': 3, 'lstm_learning_rate': 0.0018897771121602356, 'lstm_batch_size': 16, 'lstm_epochs': 40, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.03877961451381456, 'xgb_max_depth': 8}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 62.62 seconds\n","XGBoost Training Time: 0.30 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:33:21,357] Trial 29 finished with value: 0.0019974026859943175 and parameters: {'lstm_units': 112, 'lstm_layers': 2, 'lstm_learning_rate': 0.0038438348559331685, 'lstm_batch_size': 64, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.08604421469483534, 'xgb_max_depth': 7}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 16.93 seconds\n","XGBoost Training Time: 0.20 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:33:37,041] Trial 30 finished with value: 0.0020312909278712155 and parameters: {'lstm_units': 96, 'lstm_layers': 3, 'lstm_learning_rate': 0.0009797936998540285, 'lstm_batch_size': 64, 'lstm_epochs': 50, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.09374139854655375, 'xgb_max_depth': 10}. Best is trial 13 with value: 0.001989979762265609.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 14.13 seconds\n","XGBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:34:28,111] Trial 31 finished with value: 0.0019803964253161544 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.0062422269241233916, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.1106969671702801, 'xgb_max_depth': 8}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 48.83 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:35:38,696] Trial 32 finished with value: 0.0019997332612689584 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.005951113441757334, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.16171078377982243, 'xgb_max_depth': 9}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 67.73 seconds\n","XGBoost Training Time: 0.13 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:36:41,507] Trial 33 finished with value: 0.0024467346825693594 and parameters: {'lstm_units': 80, 'lstm_layers': 5, 'lstm_learning_rate': 0.009089707235448835, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.21674718225758882, 'xgb_max_depth': 8}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 59.76 seconds\n","XGBoost Training Time: 0.30 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:37:44,381] Trial 34 finished with value: 0.001996001011964745 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.002366328851870559, 'lstm_batch_size': 32, 'lstm_epochs': 20, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.13087579584039877, 'xgb_max_depth': 7}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 59.90 seconds\n","XGBoost Training Time: 0.18 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","Bi-LSTM Training Time: 69.36 seconds\n","XGBoost Training Time: 0.14 seconds\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:38:55,298] Trial 35 finished with value: 0.0040854291696643475 and parameters: {'lstm_units': 48, 'lstm_layers': 3, 'lstm_learning_rate': 0.005696281562684153, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.06078011123392518, 'xgb_max_depth': 9}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:40:26,248] Trial 36 finished with value: 0.001986706561470136 and parameters: {'lstm_units': 96, 'lstm_layers': 5, 'lstm_learning_rate': 0.004671693410883282, 'lstm_batch_size': 16, 'lstm_epochs': 20, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.08353025009846621, 'xgb_max_depth': 5}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 88.60 seconds\n","XGBoost Training Time: 0.17 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:41:41,686] Trial 37 finished with value: 0.0020058251038776176 and parameters: {'lstm_units': 112, 'lstm_layers': 3, 'lstm_learning_rate': 0.004128325269706839, 'lstm_batch_size': 16, 'lstm_epochs': 10, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.077113044215976, 'xgb_max_depth': 5}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 73.92 seconds\n","XGBoost Training Time: 0.19 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:42:30,197] Trial 38 finished with value: 0.0020889825695162897 and parameters: {'lstm_units': 96, 'lstm_layers': 2, 'lstm_learning_rate': 0.00010019361636799192, 'lstm_batch_size': 16, 'lstm_epochs': 20, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.10768242198983498, 'xgb_max_depth': 5}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 46.99 seconds\n","XGBoost Training Time: 0.12 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:43:46,059] Trial 39 finished with value: 0.0071256718813245395 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.007552538624898782, 'lstm_batch_size': 16, 'lstm_epochs': 10, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.024329858734367577, 'xgb_max_depth': 6}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 73.51 seconds\n","XGBoost Training Time: 0.21 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:45:28,693] Trial 40 finished with value: 0.002385279049526648 and parameters: {'lstm_units': 128, 'lstm_layers': 5, 'lstm_learning_rate': 0.005020455516402969, 'lstm_batch_size': 16, 'lstm_epochs': 20, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.05505784671507616, 'xgb_max_depth': 4}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 97.22 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:47:07,466] Trial 41 finished with value: 0.00199503391808092 and parameters: {'lstm_units': 96, 'lstm_layers': 5, 'lstm_learning_rate': 0.003413296859268244, 'lstm_batch_size': 16, 'lstm_epochs': 20, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.12381097660226223, 'xgb_max_depth': 4}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 95.88 seconds\n","XGBoost Training Time: 0.15 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:47:59,032] Trial 42 finished with value: 0.0019866330439690947 and parameters: {'lstm_units': 80, 'lstm_layers': 5, 'lstm_learning_rate': 0.0014235806660618752, 'lstm_batch_size': 64, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.16879769157283087, 'xgb_max_depth': 7}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 48.66 seconds\n","XGBoost Training Time: 0.14 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:48:47,546] Trial 43 finished with value: 0.0020145922115572417 and parameters: {'lstm_units': 112, 'lstm_layers': 5, 'lstm_learning_rate': 0.00154703179416174, 'lstm_batch_size': 64, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.09213367715114901, 'xgb_max_depth': 7}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 46.26 seconds\n","XGBoost Training Time: 0.17 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:49:47,054] Trial 44 finished with value: 0.002002901587273335 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.0022569132701824894, 'lstm_batch_size': 64, 'lstm_epochs': 20, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.22799090839292374, 'xgb_max_depth': 5}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 56.73 seconds\n","XGBoost Training Time: 0.08 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:50:14,335] Trial 45 finished with value: 0.0020195953231878943 and parameters: {'lstm_units': 96, 'lstm_layers': 5, 'lstm_learning_rate': 0.00705874733180368, 'lstm_batch_size': 64, 'lstm_epochs': 10, 'xgb_n_estimators': 150, 'xgb_learning_rate': 0.14893605187510936, 'xgb_max_depth': 6}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 25.08 seconds\n","XGBoost Training Time: 0.13 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:51:06,868] Trial 46 finished with value: 0.001987351392672609 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.0046746459890693905, 'lstm_batch_size': 64, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.11434758219170794, 'xgb_max_depth': 7}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 47.13 seconds\n","XGBoost Training Time: 0.14 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:52:01,612] Trial 47 finished with value: 0.0020324019586905064 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.001284213196355815, 'lstm_batch_size': 64, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.11656602131047832, 'xgb_max_depth': 7}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 52.48 seconds\n","XGBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:52:56,364] Trial 48 finished with value: 0.0019842022961539013 and parameters: {'lstm_units': 80, 'lstm_layers': 5, 'lstm_learning_rate': 0.0029470677068641478, 'lstm_batch_size': 64, 'lstm_epochs': 30, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.14330089166061918, 'xgb_max_depth': 6}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 51.78 seconds\n","XGBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-04 17:53:18,389] Trial 49 finished with value: 0.002044169582655931 and parameters: {'lstm_units': 80, 'lstm_layers': 2, 'lstm_learning_rate': 0.0029580864265279713, 'lstm_batch_size': 64, 'lstm_epochs': 20, 'xgb_n_estimators': 50, 'xgb_learning_rate': 0.10262898901330383, 'xgb_max_depth': 6}. Best is trial 31 with value: 0.0019803964253161544.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 20.50 seconds\n","XGBoost Training Time: 0.11 seconds\n","Best hyperparameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.0062422269241233916, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.1106969671702801, 'xgb_max_depth': 8}\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n","import xgboost as xgb\n","import optuna\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import time\n","\n","# Retrieve best hyperparameters from Optuna study\n","best_params = study.best_params\n","units = best_params[\"lstm_units\"]\n","layers = best_params[\"lstm_layers\"]\n","learning_rate = best_params[\"lstm_learning_rate\"]\n","batch_size = best_params[\"lstm_batch_size\"]\n","epochs = best_params[\"lstm_epochs\"]\n","n_estimators = best_params[\"xgb_n_estimators\"]\n","xgb_learning_rate = best_params[\"xgb_learning_rate\"]\n","max_depth = best_params[\"xgb_max_depth\"]\n","\n","# Reshape input for LSTM\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Function to train Bi-LSTM model\n","def train_bi_lstm(X_train, Y_train, X_val, Y_val, units, layers, learning_rate, batch_size, epochs):\n","    model = keras.Sequential()\n","\n","    for _ in range(layers - 1):  # All except last layer have return_sequences=True\n","        model.add(Bidirectional(LSTM(units, return_sequences=True)))\n","\n","    model.add(Bidirectional(LSTM(units)))  # Last LSTM layer\n","    model.add(Dense(1))  # Output layer\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n","\n","    # Early Stopping Callback\n","    early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","\n","    start_time = time.time()  # Start time tracking\n","    model.fit(X_train, Y_train,\n","              validation_data=(X_val, Y_val),\n","              epochs=epochs,\n","              batch_size=batch_size,\n","              verbose=0,\n","              callbacks=[early_stopping])  # Add early stopping\n","    lstm_train_time = time.time() - start_time  # End time tracking\n","\n","    return model, lstm_train_time\n","\n","# Train final Bi-LSTM model\n","final_lstm, lstm_train_time = train_bi_lstm(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","\n","# Predictions with timing\n","start_time = time.time()\n","Y_train_pred_lstm = final_lstm.predict(X_train_r).flatten()\n","lstm_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_lstm = final_lstm.predict(X_val_r).flatten()\n","lstm_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_lstm = final_lstm.predict(X_test_r).flatten()\n","lstm_test_pred_time = time.time() - start_time\n","\n","# Prepare data for XGBoost\n","X_train_xgb = np.column_stack([Y_train_pred_lstm])\n","X_val_xgb = np.column_stack([Y_val_pred_lstm])\n","X_test_xgb = np.column_stack([Y_test_pred_lstm])\n","\n","xgb_params = {\n","    \"objective\": \"reg:squarederror\",\n","    \"n_estimators\": n_estimators,\n","    \"learning_rate\": xgb_learning_rate,\n","    \"max_depth\": max_depth,\n","}\n","\n","if tf.config.list_physical_devices(\"GPU\"):\n","    xgb_params[\"tree_method\"] = \"gpu_hist\"\n","\n","# Train final XGBoost model\n","start_time = time.time()\n","final_xgb = xgb.XGBRegressor(**xgb_params)\n","if hasattr(final_xgb, \"fit\") and \"early_stopping_rounds\" in final_xgb.fit.__code__.co_varnames:\n","    final_xgb.fit(X_train_xgb, Y_train, eval_set=[(X_val_xgb, Y_val)], early_stopping_rounds=10, verbose=False)\n","else:\n","    final_xgb.fit(X_train_xgb, Y_train)\n","xgb_train_time = time.time() - start_time\n","\n","# XGBoost Predictions with timing\n","start_time = time.time()\n","Y_train_pred_xgb = final_xgb.predict(X_train_xgb)\n","xgb_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_xgb = final_xgb.predict(X_val_xgb)\n","xgb_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_xgb = final_xgb.predict(X_test_xgb)\n","xgb_test_pred_time = time.time() - start_time\n","\n","# Compute Metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","metrics_train = compute_metrics(Y_train, Y_train_pred_xgb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_xgb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_xgb)\n","\n","# Print results\n","print(\"\\nFinal Model Performance:\")\n","print(f\"Bi-LSTM Training Time: {lstm_train_time:.2f} seconds\")\n","print(f\"XGBoost Training Time: {xgb_train_time:.2f} seconds\\n\")\n","\n","print(f\"Bi-LSTM Train Prediction Time: {lstm_train_pred_time:.4f} seconds\")\n","print(f\"Bi-LSTM Validation Prediction Time: {lstm_val_pred_time:.4f} seconds\")\n","print(f\"Bi-LSTM Test Prediction Time: {lstm_test_pred_time:.4f} seconds\\n\")\n","\n","print(f\"XGBoost Train Prediction Time: {xgb_train_pred_time:.4f} seconds\")\n","print(f\"XGBoost Validation Prediction Time: {xgb_val_pred_time:.4f} seconds\")\n","print(f\"XGBoost Test Prediction Time: {xgb_test_pred_time:.4f} seconds\\n\")\n","\n","print(\"Train Set Metrics:\")\n","print(f\"MAE: {metrics_train[0]:.4f}, MSE: {metrics_train[1]:.4f}, RMSE: {metrics_train[2]:.4f}, R²: {metrics_train[3]:.4f}, MAPE: {metrics_train[4]:.2f}%\")\n","\n","print(\"\\nValidation Set Metrics:\")\n","print(f\"MAE: {metrics_val[0]:.4f}, MSE: {metrics_val[1]:.4f}, RMSE: {metrics_val[2]:.4f}, R²: {metrics_val[3]:.4f}, MAPE: {metrics_val[4]:.2f}%\")\n","\n","print(\"\\nTest Set Metrics:\")\n","print(f\"MAE: {metrics_test[0]:.4f}, MSE: {metrics_test[1]:.4f}, RMSE: {metrics_test[2]:.4f}, R²: {metrics_test[3]:.4f}, MAPE: {metrics_test[4]:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yRw2zPY3IWUN","outputId":"fab69998-b286-4e03-e9a6-0a6bd3ca4632"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\n","Final Model Performance:\n","Bi-LSTM Training Time: 44.63 seconds\n","XGBoost Training Time: 0.19 seconds\n","\n","Bi-LSTM Train Prediction Time: 5.1759 seconds\n","Bi-LSTM Validation Prediction Time: 0.2825 seconds\n","Bi-LSTM Test Prediction Time: 0.3464 seconds\n","\n","XGBoost Train Prediction Time: 0.0032 seconds\n","XGBoost Validation Prediction Time: 0.0020 seconds\n","XGBoost Test Prediction Time: 0.0021 seconds\n","\n","Train Set Metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0055, R²: 0.9998, MAPE: 1.03%\n","\n","Validation Set Metrics:\n","MAE: 0.1581, MSE: 0.0309, RMSE: 0.1757, R²: -4.2458, MAPE: 8.87%\n","\n","Test Set Metrics:\n","MAE: 0.4271, MSE: 0.1884, RMSE: 0.4341, R²: -30.2204, MAPE: 21.07%\n"]}]},{"cell_type":"markdown","source":["## BOHB"],"metadata":{"id":"eZRR2DI_T-8W"}},{"cell_type":"code","source":["!pip install ConfigSpace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HXtRijg2ULai","outputId":"be67f83b-fcd3-4fc4-fd20-e921c56c2f9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.26.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.13.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (10.6.0)\n","Building wheels for collected packages: ConfigSpace\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=configspace-1.2.1-py3-none-any.whl size=115950 sha256=03f589427c08258ce09cf2fa1899744cc913d03cfca0707321f42184c79a5cbf\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","Successfully built ConfigSpace\n","Installing collected packages: ConfigSpace\n","Successfully installed ConfigSpace-1.2.1\n"]}]},{"cell_type":"code","source":["!pip install hpbandster"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ptsnU6gQUVYy","outputId":"b5e204ba-e56d-4a87-9bd5-84e167927726"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: ConfigSpace in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.13.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=011af9744a2bca9248eaeb98c37826c91f6bcbab6ea5dd6cbcceeb7e2b8b1689\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35185 sha256=e2485cdabddb91edc0249e96166c7568f54496bedfa0006749693697760b399f\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, hpbandster\n","Successfully installed Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import xgboost as xgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-directional LSTM doubles output size\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Bi-LSTM Configurations\n","lstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store LSTM feature representations\n","lstm_features = []\n","\n","for num_layers in lstm_layers:\n","    print(f\"Training Bi-LSTM with {num_layers} layers...\")\n","\n","    lstm_model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        lstm_model.train()\n","        optimizer.zero_grad()\n","        outputs = lstm_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    # Extract Feature Representations\n","    lstm_model.eval()\n","    with torch.no_grad():\n","        val_start = time.time()\n","        train_features = lstm_model(X_train_torch).numpy()\n","        val_features = lstm_model(X_val_torch).numpy()\n","        val_time = time.time() - val_start\n","\n","        test_start = time.time()\n","        test_features = lstm_model(X_test_torch).numpy()\n","        test_time = time.time() - test_start\n","\n","    lstm_features.append((train_features, val_features, test_features, train_time, val_time, test_time))\n","\n","# Concatenate Features from All Layers\n","final_train_features = np.hstack([feat[0] for feat in lstm_features])\n","final_val_features = np.hstack([feat[1] for feat in lstm_features])\n","final_test_features = np.hstack([feat[2] for feat in lstm_features])\n","\n","# Record Time for Each Stage\n","total_train_time = sum([feat[3] for feat in lstm_features])\n","total_val_time = sum([feat[4] for feat in lstm_features])\n","total_test_time = sum([feat[5] for feat in lstm_features])\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"subsample\", 0.5, 1.0, default_value=0.8))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"colsample_bytree\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for XGBoost\n","class XGBoostWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = xgb.XGBRegressor(\n","            n_estimators=config[\"n_estimators\"],\n","            learning_rate=config[\"learning_rate\"],\n","            max_depth=config[\"max_depth\"],\n","            subsample=config[\"subsample\"],\n","            colsample_bytree=config[\"colsample_bytree\"],\n","            random_state=42\n","        )\n","        model.fit(final_train_features, Y_train)\n","        Y_val_pred = model.predict(final_val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"bilstm_xgb_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = XGBoostWorker(nameserver=\"127.0.0.2\", run_id=\"bilstm_xgb_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"bilstm_xgb_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best XGBoost Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_xgb_model = xgb.XGBRegressor(\n","    n_estimators=best_params[\"n_estimators\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    max_depth=best_params[\"max_depth\"],\n","    subsample=best_params[\"subsample\"],\n","    colsample_bytree=best_params[\"colsample_bytree\"],\n","    random_state=42\n",")\n","\n","best_xgb_model.fit(final_train_features, Y_train)\n","\n","# Predictions\n","Y_train_pred = best_xgb_model.predict(final_train_features)\n","Y_val_pred = best_xgb_model.predict(final_val_features)\n","Y_test_pred = best_xgb_model.predict(final_test_features)\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", total_train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", total_val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", total_test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQqF-QdwUBvx","outputId":"564c1833-abd5-4867-edcc-ac2d5e58150f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Bi-LSTM with 2 layers...\n","Training Bi-LSTM with 3 layers...\n","Training Bi-LSTM with 5 layers...\n","Train Metrics: (0.003575375090794875, 2.9784843822823764e-05, 0.005457549250609083, 0.999829821310102, 1.0417402490448406) Time: 141.69099259376526\n","Validation Metrics: (0.1563152800618162, 0.030291889813740626, 0.17404565439487601, -4.150331022749234, 8.772619942988232) Time: 0.45818471908569336\n","Test Metrics: (0.42527516623005457, 0.1868940357839429, 0.43231242844029233, -29.968004317172483, 20.979055916122107) Time: 0.050843000411987305\n"]}]},{"cell_type":"markdown","source":["# Catboost"],"metadata":{"id":"uUCh_YB9Vf7S"}},{"cell_type":"markdown","source":["## Intial"],"metadata":{"id":"qUH36NvmtM2G"}},{"cell_type":"code","source":["!pip install catboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uREdORkVkJz","outputId":"da72c9fa-d767-4bf9-a224-55dc250c6875"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n","Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.7\n"]}]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Ensure TensorFlow runs on CPU\n","tf.config.set_visible_devices([], 'GPU')\n","print(\"Running on CPU\")\n","\n","# Function to define and train a Bi-LSTM model on CPU\n","def train_bi_lstm(X_train, Y_train, X_val, Y_val, layers):\n","    model = Sequential()\n","    model.add(Bidirectional(LSTM(64, return_sequences=(layers > 1)), input_shape=(X_train.shape[1], 1)))\n","    for _ in range(layers - 1):\n","        model.add(Bidirectional(LSTM(64, return_sequences=(_ < layers - 2))))\n","    model.add(Dense(1))\n","\n","    model.compile(optimizer='adam', loss='mse')\n","    start_time = time.time()\n","    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","    train_time = time.time() - start_time\n","    return model, train_time\n","\n","# Reshaping input for LSTM\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train 2, 3, and 5-layer Bi-LSTM models\n","bi_lstm_models = {}\n","bi_lstm_predictions = {}\n","times = {}\n","\n","for layers in [2, 3, 5]:\n","    model, train_time = train_bi_lstm(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    Y_train_pred = model.predict(X_train_r)\n","    Y_val_pred = model.predict(X_val_r)\n","    Y_test_pred = model.predict(X_test_r)\n","\n","    bi_lstm_models[layers] = model\n","    bi_lstm_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","    times[f'Bi-LSTM-{layers}'] = train_time\n","\n","# Prepare input for CatBoost\n","X_train_cat = np.column_stack([bi_lstm_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_cat = np.column_stack([bi_lstm_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_cat = np.column_stack([bi_lstm_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train CatBoost model\n","cat_model = CatBoostRegressor(iterations=100, learning_rate=0.05, depth=3, loss_function='RMSE', task_type='CPU', verbose=0)\n","\n","start_time = time.time()\n","cat_model.fit(X_train_cat, Y_train, eval_set=(X_val_cat, Y_val), verbose=0)\n","times['CatBoost'] = time.time() - start_time\n","\n","# Predictions from CatBoost\n","start_time = time.time()\n","Y_train_pred_cat = cat_model.predict(X_train_cat)\n","times['CatBoost Train'] = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_cat = cat_model.predict(X_val_cat)\n","times['CatBoost Validate'] = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_cat = cat_model.predict(X_test_cat)\n","times['CatBoost Test'] = time.time() - start_time\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_cat)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_cat)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_cat)\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print training times\n","print(\"Training Times:\")\n","for model, t in times.items():\n","    print(f\"{model}: {t:.2f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hcZDoVAPV2Gh","outputId":"4f1fa1f0-d1c1-4e3d-e8f4-d94f9bbcbc3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on CPU\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","Train Metrics: MAE=0.0076, MSE=0.0001, RMSE=0.0109, R²=0.9993, MAPE=2.83%\n","Validation Metrics: MAE=0.1973, MSE=0.0448, RMSE=0.2117, R²=-6.6213, MAPE=11.13%\n","Test Metrics: MAE=0.4664, MSE=0.2235, RMSE=0.4728, R²=-36.0407, MAPE=23.02%\n","Training Times:\n","Bi-LSTM-2: 150.25 seconds\n","Bi-LSTM-3: 200.09 seconds\n","Bi-LSTM-5: 349.53 seconds\n","CatBoost: 0.19 seconds\n","CatBoost Train: 0.00 seconds\n","CatBoost Validate: 0.00 seconds\n","CatBoost Test: 0.00 seconds\n"]}]},{"cell_type":"markdown","source":["## Optuna"],"metadata":{"id":"PnPul0h2lqPU"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iaFt4LZ3mLdj","outputId":"0101bb57-4959-4d71-902b-48fa65638612"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n","import catboost as cb\n","import optuna\n","from sklearn.metrics import mean_squared_error\n","import time  # For tracking training time\n","\n","# Function to train Bi-LSTM model\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","def train_bi_lstm(X_train, Y_train, X_val, Y_val, units, layers, learning_rate, batch_size, epochs):\n","    model = keras.Sequential()\n","\n","    for _ in range(layers - 1):  # All except last layer have return_sequences=True\n","        model.add(Bidirectional(LSTM(units, return_sequences=True)))\n","\n","    model.add(Bidirectional(LSTM(units)))  # Last LSTM layer\n","    model.add(Dense(1))  # Output layer\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n","\n","    # Early Stopping Callback\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","\n","    start_time = time.time()  # Start time tracking\n","    history = model.fit(X_train, Y_train,\n","                        validation_data=(X_val, Y_val),\n","                        epochs=epochs,\n","                        batch_size=batch_size,\n","                        verbose=0,\n","                        callbacks=[early_stopping])  # Add early stopping\n","    lstm_train_time = time.time() - start_time  # End time tracking\n","\n","    return model, history, lstm_train_time\n","\n","\n","# Objective function for Optuna\n","def objective(trial):\n","    units = trial.suggest_int(\"lstm_units\", 32, 128, step=16)\n","    layers = trial.suggest_categorical(\"lstm_layers\", [2, 3, 5])\n","    learning_rate = trial.suggest_loguniform(\"lstm_learning_rate\", 1e-4, 1e-2)\n","    batch_size = trial.suggest_categorical(\"lstm_batch_size\", [16, 32, 64])\n","    epochs = trial.suggest_int(\"lstm_epochs\", 10, 50, step=10)\n","\n","    # Reshape input for LSTM\n","    X_train_r = np.expand_dims(X_train, axis=-1)\n","    X_val_r = np.expand_dims(X_val, axis=-1)\n","\n","    # Train Bi-LSTM\n","    model, _, lstm_train_time = train_bi_lstm(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","    Y_val_pred_lstm = model.predict(X_val_r).flatten()\n","\n","    # Prepare data for CatBoost\n","    X_val_cat = np.column_stack([Y_val_pred_lstm])\n","\n","    cat_params = {\n","        \"depth\": trial.suggest_int(\"cat_depth\", 3, 10),\n","        \"learning_rate\": trial.suggest_loguniform(\"cat_learning_rate\", 0.01, 0.3),\n","        \"iterations\": trial.suggest_int(\"cat_iterations\", 50, 200, step=50),\n","        \"loss_function\": \"RMSE\",\n","        \"verbose\": 0\n","    }\n","\n","    # Train CatBoost\n","    start_time = time.time()  # Start time tracking\n","    cat_model = cb.CatBoostRegressor(**cat_params)\n","    cat_model.fit(X_val_cat, Y_val)\n","    cat_train_time = time.time() - start_time  # End time tracking\n","\n","    # Predict and evaluate\n","    Y_val_pred_cat = cat_model.predict(X_val_cat)\n","    rmse = np.sqrt(mean_squared_error(Y_val, Y_val_pred_cat))\n","\n","    # Print training times\n","    print(f\"Bi-LSTM Training Time: {lstm_train_time:.2f} seconds\")\n","    print(f\"CatBoost Training Time: {cat_train_time:.2f} seconds\")\n","\n","    return rmse\n","\n","# Run Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=50)\n","\n","# Best hyperparameters\n","print(\"Best hyperparameters:\", study.best_params)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hJfg66lpWB8w","outputId":"7db089f8-7ba6-45c9-b3fa-917ae65d08d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:49:46,886] A new study created in memory with name: no-name-dbc686c1-e9d4-42a4-8c9b-b5c168fdb147\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:50:49,436] Trial 0 finished with value: 0.0387822240214557 and parameters: {'lstm_units': 48, 'lstm_layers': 2, 'lstm_learning_rate': 0.0012914287757185147, 'lstm_batch_size': 16, 'lstm_epochs': 40, 'cat_depth': 4, 'cat_learning_rate': 0.014889830996739484, 'cat_iterations': 50}. Best is trial 0 with value: 0.0387822240214557.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 60.34 seconds\n","CatBoost Training Time: 0.10 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:52:17,554] Trial 1 finished with value: 0.03698650389738777 and parameters: {'lstm_units': 128, 'lstm_layers': 3, 'lstm_learning_rate': 0.006072853717520684, 'lstm_batch_size': 16, 'lstm_epochs': 10, 'cat_depth': 3, 'cat_learning_rate': 0.01644078221366311, 'cat_iterations': 50}. Best is trial 1 with value: 0.03698650389738777.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 86.72 seconds\n","CatBoost Training Time: 0.04 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:52:42,030] Trial 2 finished with value: 0.030064733114952426 and parameters: {'lstm_units': 112, 'lstm_layers': 3, 'lstm_learning_rate': 0.0017733970359111357, 'lstm_batch_size': 64, 'lstm_epochs': 20, 'cat_depth': 5, 'cat_learning_rate': 0.02022487253447511, 'cat_iterations': 50}. Best is trial 2 with value: 0.030064733114952426.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 23.07 seconds\n","CatBoost Training Time: 0.05 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:53:16,799] Trial 3 finished with value: 0.00532475060317284 and parameters: {'lstm_units': 96, 'lstm_layers': 5, 'lstm_learning_rate': 0.00016686692963282517, 'lstm_batch_size': 64, 'lstm_epochs': 30, 'cat_depth': 5, 'cat_learning_rate': 0.06083382558376327, 'cat_iterations': 50}. Best is trial 3 with value: 0.00532475060317284.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 31.87 seconds\n","CatBoost Training Time: 0.05 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:53:45,351] Trial 4 finished with value: 0.00229629453046146 and parameters: {'lstm_units': 112, 'lstm_layers': 3, 'lstm_learning_rate': 0.0001639194131471027, 'lstm_batch_size': 64, 'lstm_epochs': 50, 'cat_depth': 8, 'cat_learning_rate': 0.04260306756665395, 'cat_iterations': 200}. Best is trial 4 with value: 0.00229629453046146.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 26.98 seconds\n","CatBoost Training Time: 0.28 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:54:05,399] Trial 5 finished with value: 0.0020492131430758934 and parameters: {'lstm_units': 80, 'lstm_layers': 2, 'lstm_learning_rate': 0.0003212078950516867, 'lstm_batch_size': 64, 'lstm_epochs': 20, 'cat_depth': 7, 'cat_learning_rate': 0.2757422058105729, 'cat_iterations': 200}. Best is trial 5 with value: 0.0020492131430758934.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 18.41 seconds\n","CatBoost Training Time: 0.29 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:54:27,864] Trial 6 finished with value: 0.011630641621585034 and parameters: {'lstm_units': 128, 'lstm_layers': 2, 'lstm_learning_rate': 0.0003765192208474991, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'cat_depth': 9, 'cat_learning_rate': 0.01341466292953254, 'cat_iterations': 150}. Best is trial 5 with value: 0.0020492131430758934.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 20.78 seconds\n","CatBoost Training Time: 0.33 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:55:16,960] Trial 7 finished with value: 0.002689692553689093 and parameters: {'lstm_units': 128, 'lstm_layers': 5, 'lstm_learning_rate': 0.00591260367551317, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 3, 'cat_learning_rate': 0.042243916188627655, 'cat_iterations': 200}. Best is trial 5 with value: 0.0020492131430758934.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 46.35 seconds\n","CatBoost Training Time: 0.08 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:55:49,815] Trial 8 finished with value: 0.020428298027446397 and parameters: {'lstm_units': 32, 'lstm_layers': 3, 'lstm_learning_rate': 0.00037264125182694076, 'lstm_batch_size': 32, 'lstm_epochs': 50, 'cat_depth': 6, 'cat_learning_rate': 0.01424979914278127, 'cat_iterations': 100}. Best is trial 5 with value: 0.0020492131430758934.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 30.09 seconds\n","CatBoost Training Time: 0.13 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:56:42,394] Trial 9 finished with value: 0.0024841896772419777 and parameters: {'lstm_units': 64, 'lstm_layers': 2, 'lstm_learning_rate': 0.004126963371072696, 'lstm_batch_size': 16, 'lstm_epochs': 20, 'cat_depth': 7, 'cat_learning_rate': 0.2656924162061817, 'cat_iterations': 50}. Best is trial 5 with value: 0.0020492131430758934.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 51.27 seconds\n","CatBoost Training Time: 0.07 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:56:59,587] Trial 10 finished with value: 0.0020381622827240456 and parameters: {'lstm_units': 80, 'lstm_layers': 2, 'lstm_learning_rate': 0.00046166350381756835, 'lstm_batch_size': 64, 'lstm_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.2844547872517503, 'cat_iterations': 150}. Best is trial 10 with value: 0.0020381622827240456.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 15.75 seconds\n","CatBoost Training Time: 0.49 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:57:24,356] Trial 11 finished with value: 0.0020402473457029665 and parameters: {'lstm_units': 80, 'lstm_layers': 2, 'lstm_learning_rate': 0.0005123652353068117, 'lstm_batch_size': 64, 'lstm_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.2876931042686788, 'cat_iterations': 150}. Best is trial 10 with value: 0.0020381622827240456.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 21.32 seconds\n","CatBoost Training Time: 0.78 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:57:41,723] Trial 12 finished with value: 0.002115695511792557 and parameters: {'lstm_units': 80, 'lstm_layers': 2, 'lstm_learning_rate': 0.0005923439969178698, 'lstm_batch_size': 64, 'lstm_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.1410369300911369, 'cat_iterations': 150}. Best is trial 10 with value: 0.0020381622827240456.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 15.89 seconds\n","CatBoost Training Time: 0.50 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:57:57,876] Trial 13 finished with value: 0.0021471397140626334 and parameters: {'lstm_units': 64, 'lstm_layers': 2, 'lstm_learning_rate': 0.0007955353867680536, 'lstm_batch_size': 64, 'lstm_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.12504797568507459, 'cat_iterations': 150}. Best is trial 10 with value: 0.0020381622827240456.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 13.86 seconds\n","CatBoost Training Time: 0.91 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:58:12,691] Trial 14 finished with value: 0.0022264818015340854 and parameters: {'lstm_units': 96, 'lstm_layers': 2, 'lstm_learning_rate': 0.001967774866370572, 'lstm_batch_size': 64, 'lstm_epochs': 10, 'cat_depth': 9, 'cat_learning_rate': 0.15576948150669467, 'cat_iterations': 100}. Best is trial 10 with value: 0.0020381622827240456.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 13.61 seconds\n","CatBoost Training Time: 0.23 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:58:29,943] Trial 15 finished with value: 0.0022282150104202887 and parameters: {'lstm_units': 64, 'lstm_layers': 2, 'lstm_learning_rate': 0.0005486831912984126, 'lstm_batch_size': 64, 'lstm_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.0869309232507556, 'cat_iterations': 150}. Best is trial 10 with value: 0.0020381622827240456.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 15.55 seconds\n","CatBoost Training Time: 0.33 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:59:11,933] Trial 16 finished with value: 0.002107294674149792 and parameters: {'lstm_units': 96, 'lstm_layers': 5, 'lstm_learning_rate': 0.00010438555449291487, 'lstm_batch_size': 64, 'lstm_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.25865142126455565, 'cat_iterations': 100}. Best is trial 10 with value: 0.0020381622827240456.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 38.98 seconds\n","CatBoost Training Time: 0.32 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 07:59:35,722] Trial 17 finished with value: 0.002126803908047471 and parameters: {'lstm_units': 48, 'lstm_layers': 2, 'lstm_learning_rate': 0.00021600905349961085, 'lstm_batch_size': 64, 'lstm_epochs': 20, 'cat_depth': 8, 'cat_learning_rate': 0.168753614909242, 'cat_iterations': 150}. Best is trial 10 with value: 0.0020381622827240456.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 22.20 seconds\n","CatBoost Training Time: 0.22 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:00:55,619] Trial 18 finished with value: 0.002336491870222927 and parameters: {'lstm_units': 80, 'lstm_layers': 2, 'lstm_learning_rate': 0.0010332786421495524, 'lstm_batch_size': 16, 'lstm_epochs': 30, 'cat_depth': 8, 'cat_learning_rate': 0.08968365684699178, 'cat_iterations': 100}. Best is trial 10 with value: 0.0020381622827240456.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 78.76 seconds\n","CatBoost Training Time: 0.16 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:01:54,078] Trial 19 finished with value: 0.002012429770404404 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.00220267461374858, 'lstm_batch_size': 32, 'lstm_epochs': 10, 'cat_depth': 10, 'cat_learning_rate': 0.20699441085619852, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 55.07 seconds\n","CatBoost Training Time: 0.66 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:03:25,325] Trial 20 finished with value: 0.0020230197406690123 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.003119883487610931, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'cat_depth': 9, 'cat_learning_rate': 0.20425485019588394, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 88.74 seconds\n","CatBoost Training Time: 0.43 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:04:27,374] Trial 21 finished with value: 0.00205586490011984 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.0029588171712435704, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'cat_depth': 9, 'cat_learning_rate': 0.1901405274028759, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 58.52 seconds\n","CatBoost Training Time: 0.83 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:05:48,520] Trial 22 finished with value: 0.0020152375484065043 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.0028647418987266552, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'cat_depth': 10, 'cat_learning_rate': 0.2000913288612169, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 75.17 seconds\n","CatBoost Training Time: 0.66 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:06:44,695] Trial 23 finished with value: 0.002126300067479158 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.0032062602503635375, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'cat_depth': 9, 'cat_learning_rate': 0.11001753809774366, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 53.06 seconds\n","CatBoost Training Time: 0.43 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:07:57,541] Trial 24 finished with value: 0.0020779556367658708 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.0022143063607138695, 'lstm_batch_size': 32, 'lstm_epochs': 50, 'cat_depth': 8, 'cat_learning_rate': 0.20244217395480696, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 69.86 seconds\n","CatBoost Training Time: 0.29 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:09:18,429] Trial 25 finished with value: 0.0021490446596183475 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.0082253040496729, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.07362770745436908, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 78.24 seconds\n","CatBoost Training Time: 0.63 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:10:08,848] Trial 26 finished with value: 0.0020869489387046127 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.0037850727441389037, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'cat_depth': 9, 'cat_learning_rate': 0.10964354574850294, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 47.96 seconds\n","CatBoost Training Time: 0.43 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:10:59,024] Trial 27 finished with value: 0.0020557794886237233 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.0014777700488829248, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'cat_depth': 7, 'cat_learning_rate': 0.21847969756382876, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 47.89 seconds\n","CatBoost Training Time: 0.21 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:11:56,273] Trial 28 finished with value: 0.0022725198031317705 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.009984720893181753, 'lstm_batch_size': 32, 'lstm_epochs': 50, 'cat_depth': 10, 'cat_learning_rate': 0.03340775800608499, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 54.22 seconds\n","CatBoost Training Time: 0.95 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:13:11,017] Trial 29 finished with value: 0.002052466334178062 and parameters: {'lstm_units': 64, 'lstm_layers': 5, 'lstm_learning_rate': 0.0011267113526402675, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 8, 'cat_learning_rate': 0.20147399710936964, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 72.37 seconds\n","CatBoost Training Time: 0.28 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:14:23,648] Trial 30 finished with value: 0.002203939973870269 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.002366678440941675, 'lstm_batch_size': 32, 'lstm_epochs': 40, 'cat_depth': 6, 'cat_learning_rate': 0.14062221527275617, 'cat_iterations': 150}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 70.40 seconds\n","CatBoost Training Time: 0.12 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:15:10,619] Trial 31 finished with value: 0.002013125502122775 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.004925296736821942, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.2279611330977161, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 42.01 seconds\n","CatBoost Training Time: 0.64 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:16:20,576] Trial 32 finished with value: 0.002014519243416602 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.005699714496662711, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.20599312804446468, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 67.54 seconds\n","CatBoost Training Time: 0.44 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:17:21,955] Trial 33 finished with value: 0.002535256698722826 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.005296659274981486, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.0235713525947769, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 57.77 seconds\n","CatBoost Training Time: 0.91 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:18:31,232] Trial 34 finished with value: 0.002033272424758669 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.004265493737418265, 'lstm_batch_size': 16, 'lstm_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.1669914332003508, 'cat_iterations': 200}. Best is trial 19 with value: 0.002012429770404404.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 65.95 seconds\n","CatBoost Training Time: 0.63 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:19:06,956] Trial 35 finished with value: 0.00199749070500472 and parameters: {'lstm_units': 32, 'lstm_layers': 3, 'lstm_learning_rate': 0.007292841018612873, 'lstm_batch_size': 32, 'lstm_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.22270364294722322, 'cat_iterations': 200}. Best is trial 35 with value: 0.00199749070500472.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 34.02 seconds\n","CatBoost Training Time: 0.43 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:19:57,267] Trial 36 finished with value: 0.011602155801782888 and parameters: {'lstm_units': 32, 'lstm_layers': 3, 'lstm_learning_rate': 0.0077380854781603425, 'lstm_batch_size': 32, 'lstm_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.010098772499118709, 'cat_iterations': 200}. Best is trial 35 with value: 0.00199749070500472.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 48.47 seconds\n","CatBoost Training Time: 0.44 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:20:35,291] Trial 37 finished with value: 0.0021495494800876334 and parameters: {'lstm_units': 32, 'lstm_layers': 3, 'lstm_learning_rate': 0.006743936905215637, 'lstm_batch_size': 32, 'lstm_epochs': 20, 'cat_depth': 5, 'cat_learning_rate': 0.23116862113867132, 'cat_iterations': 150}. Best is trial 35 with value: 0.00199749070500472.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 36.59 seconds\n","CatBoost Training Time: 0.09 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:21:48,191] Trial 38 finished with value: 0.0020818648703442287 and parameters: {'lstm_units': 32, 'lstm_layers': 3, 'lstm_learning_rate': 0.005106761986738307, 'lstm_batch_size': 16, 'lstm_epochs': 20, 'cat_depth': 8, 'cat_learning_rate': 0.11261444011998852, 'cat_iterations': 200}. Best is trial 35 with value: 0.00199749070500472.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 70.86 seconds\n","CatBoost Training Time: 0.65 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:22:29,626] Trial 39 finished with value: 0.0023879376053434855 and parameters: {'lstm_units': 32, 'lstm_layers': 3, 'lstm_learning_rate': 0.009335386298888212, 'lstm_batch_size': 32, 'lstm_epochs': 20, 'cat_depth': 4, 'cat_learning_rate': 0.0894032682833989, 'cat_iterations': 200}. Best is trial 35 with value: 0.00199749070500472.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 39.94 seconds\n","CatBoost Training Time: 0.10 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:23:24,354] Trial 40 finished with value: 0.00202182666330377 and parameters: {'lstm_units': 48, 'lstm_layers': 3, 'lstm_learning_rate': 0.0015887511820337839, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.2456427611598378, 'cat_iterations': 200}. Best is trial 35 with value: 0.00199749070500472.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 52.92 seconds\n","CatBoost Training Time: 0.43 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:24:25,618] Trial 41 finished with value: 0.002042481591197424 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.006718245217162945, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.1744722491902194, 'cat_iterations': 200}. Best is trial 35 with value: 0.00199749070500472.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 55.34 seconds\n","CatBoost Training Time: 0.65 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:25:22,007] Trial 42 finished with value: 0.002024538854528376 and parameters: {'lstm_units': 32, 'lstm_layers': 5, 'lstm_learning_rate': 0.0047839926713130285, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 10, 'cat_learning_rate': 0.2990334523093253, 'cat_iterations': 200}. Best is trial 35 with value: 0.00199749070500472.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 53.70 seconds\n","CatBoost Training Time: 0.65 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:26:19,337] Trial 43 finished with value: 0.0021029420636588473 and parameters: {'lstm_units': 48, 'lstm_layers': 5, 'lstm_learning_rate': 0.003595498048909373, 'lstm_batch_size': 32, 'lstm_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.13388538968734323, 'cat_iterations': 200}. Best is trial 35 with value: 0.00199749070500472.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 51.52 seconds\n","CatBoost Training Time: 0.43 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:27:24,682] Trial 44 finished with value: 0.0019825179050648235 and parameters: {'lstm_units': 48, 'lstm_layers': 3, 'lstm_learning_rate': 0.0023368035269207325, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.22194511441333833, 'cat_iterations': 200}. Best is trial 44 with value: 0.0019825179050648235.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 62.22 seconds\n","CatBoost Training Time: 0.43 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:28:02,159] Trial 45 finished with value: 0.0025654659909453903 and parameters: {'lstm_units': 32, 'lstm_layers': 3, 'lstm_learning_rate': 0.005979402244277442, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 7, 'cat_learning_rate': 0.23590043697206403, 'cat_iterations': 50}. Best is trial 44 with value: 0.0019825179050648235.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 36.12 seconds\n","CatBoost Training Time: 0.07 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:29:15,622] Trial 46 finished with value: 0.0022823469913291062 and parameters: {'lstm_units': 32, 'lstm_layers': 3, 'lstm_learning_rate': 0.0025246039337913995, 'lstm_batch_size': 32, 'lstm_epochs': 20, 'cat_depth': 8, 'cat_learning_rate': 0.05678113867207872, 'cat_iterations': 150}. Best is trial 44 with value: 0.0019825179050648235.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 71.79 seconds\n","CatBoost Training Time: 0.22 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:31:04,683] Trial 47 finished with value: 0.002050714850964961 and parameters: {'lstm_units': 64, 'lstm_layers': 3, 'lstm_learning_rate': 0.001831103069324521, 'lstm_batch_size': 16, 'lstm_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.15465716782112643, 'cat_iterations': 200}. Best is trial 44 with value: 0.0019825179050648235.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 107.24 seconds\n","CatBoost Training Time: 0.43 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:31:51,029] Trial 48 finished with value: 0.0020574681892367775 and parameters: {'lstm_units': 112, 'lstm_layers': 3, 'lstm_learning_rate': 0.0008463635332788621, 'lstm_batch_size': 32, 'lstm_epochs': 10, 'cat_depth': 8, 'cat_learning_rate': 0.2526613517985737, 'cat_iterations': 150}. Best is trial 44 with value: 0.0019825179050648235.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 44.74 seconds\n","CatBoost Training Time: 0.22 seconds\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 08:32:52,774] Trial 49 finished with value: 0.002254469594124472 and parameters: {'lstm_units': 48, 'lstm_layers': 3, 'lstm_learning_rate': 0.004402669981015406, 'lstm_batch_size': 32, 'lstm_epochs': 20, 'cat_depth': 9, 'cat_learning_rate': 0.041122826208360254, 'cat_iterations': 200}. Best is trial 44 with value: 0.0019825179050648235.\n"]},{"output_type":"stream","name":"stdout","text":["Bi-LSTM Training Time: 58.64 seconds\n","CatBoost Training Time: 0.41 seconds\n","Best hyperparameters: {'lstm_units': 48, 'lstm_layers': 3, 'lstm_learning_rate': 0.0023368035269207325, 'lstm_batch_size': 32, 'lstm_epochs': 30, 'cat_depth': 9, 'cat_learning_rate': 0.22194511441333833, 'cat_iterations': 200}\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import time\n","from catboost import CatBoostRegressor\n","\n","# Function to train Bi-LSTM model\n","def train_bi_lstm(X_train, Y_train, X_val, Y_val, units, layers, learning_rate, batch_size, epochs):\n","    model = keras.Sequential()\n","\n","    for _ in range(layers - 1):  # All except last layer have return_sequences=True\n","        model.add(Bidirectional(LSTM(units, return_sequences=True)))\n","\n","    model.add(Bidirectional(LSTM(units)))  # Last LSTM layer\n","    model.add(Dense(1))  # Output layer\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n","\n","    early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n","\n","    start_time = time.time()\n","    history = model.fit(X_train, Y_train,\n","                        validation_data=(X_val, Y_val),\n","                        epochs=epochs,\n","                        batch_size=batch_size,\n","                        verbose=0,\n","                        callbacks=[early_stopping])\n","    lstm_train_time = time.time() - start_time\n","\n","    return model, history, lstm_train_time\n","\n","# Retrieve best hyperparameters from Optuna study (Make sure these are from Bi-LSTM tuning!)\n","best_params = study.best_params\n","units = best_params[\"lstm_units\"]\n","layers = best_params[\"lstm_layers\"]\n","learning_rate = best_params[\"lstm_learning_rate\"]\n","batch_size = best_params[\"lstm_batch_size\"]\n","epochs = best_params[\"lstm_epochs\"]\n","\n","n_estimators = best_params[\"cat_iterations\"]\n","catboost_learning_rate = best_params[\"cat_learning_rate\"]\n","depth = best_params[\"cat_depth\"]\n","\n","# Reshape input for LSTM (Bi-LSTM expects 3D input: (samples, time steps, features))\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train final Bi-LSTM model\n","final_lstm, _, lstm_train_time = train_bi_lstm(X_train_r, Y_train, X_val_r, Y_val, units, layers, learning_rate, batch_size, epochs)\n","\n","# Predictions with timing\n","start_time = time.time()\n","Y_train_pred_lstm = final_lstm.predict(X_train_r).flatten()\n","lstm_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_lstm = final_lstm.predict(X_val_r).flatten()\n","lstm_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_lstm = final_lstm.predict(X_test_r).flatten()\n","lstm_test_pred_time = time.time() - start_time\n","\n","# Prepare data for CatBoost (LSTM outputs become inputs for CatBoost)\n","X_train_cb = np.column_stack([Y_train_pred_lstm])\n","X_val_cb = np.column_stack([Y_val_pred_lstm])\n","X_test_cb = np.column_stack([Y_test_pred_lstm])\n","\n","# Train final CatBoost model\n","start_time = time.time()\n","final_cb = CatBoostRegressor(iterations=n_estimators, learning_rate=catboost_learning_rate, depth=depth, loss_function='RMSE', verbose=0)\n","final_cb.fit(X_train_cb, Y_train, eval_set=(X_val_cb, Y_val), early_stopping_rounds=10)\n","cb_train_time = time.time() - start_time\n","\n","# CatBoost Predictions with timing\n","start_time = time.time()\n","Y_train_pred_cb = final_cb.predict(X_train_cb)\n","cb_train_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_cb = final_cb.predict(X_val_cb)\n","cb_val_pred_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_test_pred_cb = final_cb.predict(X_test_cb)\n","cb_test_pred_time = time.time() - start_time\n","\n","# Compute Metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","metrics_train = compute_metrics(Y_train, Y_train_pred_cb)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_cb)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_cb)\n","\n","# Print results\n","print(\"\\nFinal Model Performance:\")\n","print(f\"Bi-LSTM Training Time: {lstm_train_time:.2f} seconds\")\n","print(f\"CatBoost Training Time: {cb_train_time:.2f} seconds\\n\")\n","\n","print(f\"Bi-LSTM Train Prediction Time: {lstm_train_pred_time:.4f} seconds\")\n","print(f\"Bi-LSTM Validation Prediction Time: {lstm_val_pred_time:.4f} seconds\")\n","print(f\"Bi-LSTM Test Prediction Time: {lstm_test_pred_time:.4f} seconds\\n\")\n","\n","print(f\"CatBoost Train Prediction Time: {cb_train_pred_time:.4f} seconds\")\n","print(f\"CatBoost Validation Prediction Time: {cb_val_pred_time:.4f} seconds\")\n","print(f\"CatBoost Test Prediction Time: {cb_test_pred_time:.4f} seconds\\n\")\n","\n","print(\"Train Set Metrics:\")\n","print(f\"MAE: {metrics_train[0]:.4f}, MSE: {metrics_train[1]:.4f}, RMSE: {metrics_train[2]:.4f}, R²: {metrics_train[3]:.4f}, MAPE: {metrics_train[4]:.2f}%\")\n","\n","print(\"\\nValidation Set Metrics:\")\n","print(f\"MAE: {metrics_val[0]:.4f}, MSE: {metrics_val[1]:.4f}, RMSE: {metrics_val[2]:.4f}, R²: {metrics_val[3]:.4f}, MAPE: {metrics_val[4]:.2f}%\")\n","\n","print(\"\\nTest Set Metrics:\")\n","print(f\"MAE: {metrics_test[0]:.4f}, MSE: {metrics_test[1]:.4f}, RMSE: {metrics_test[2]:.4f}, R²: {metrics_test[3]:.4f}, MAPE: {metrics_test[4]:.2f}%\")\n"],"metadata":{"id":"ZjK9aFoRaBHM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a32107f-1b02-4a36-b149-a069610661d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\n","Final Model Performance:\n","Bi-LSTM Training Time: 73.52 seconds\n","CatBoost Training Time: 0.59 seconds\n","\n","Bi-LSTM Train Prediction Time: 1.7040 seconds\n","Bi-LSTM Validation Prediction Time: 0.3478 seconds\n","Bi-LSTM Test Prediction Time: 0.2351 seconds\n","\n","CatBoost Train Prediction Time: 0.0049 seconds\n","CatBoost Validation Prediction Time: 0.0017 seconds\n","CatBoost Test Prediction Time: 0.0017 seconds\n","\n","Train Set Metrics:\n","MAE: 0.0036, MSE: 0.0000, RMSE: 0.0054, R²: 0.9998, MAPE: 1.02%\n","\n","Validation Set Metrics:\n","MAE: 0.1577, MSE: 0.0307, RMSE: 0.1753, R²: -4.2230, MAPE: 8.85%\n","\n","Test Set Metrics:\n","MAE: 0.4266, MSE: 0.1881, RMSE: 0.4337, R²: -30.1602, MAPE: 21.05%\n"]}]},{"cell_type":"markdown","source":["## BOHB"],"metadata":{"id":"UA4RDkuZXHto"}},{"cell_type":"code","source":["!pip install ConfigSpace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XHZKQWHJeO1O","outputId":"763e4f88-5414-46e0-eef8-8d0d41c6dff5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/131.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.26.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.13.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (10.6.0)\n","Building wheels for collected packages: ConfigSpace\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=configspace-1.2.1-py3-none-any.whl size=115950 sha256=e9b7777ad06657ff23d67b985cfcf0c0dcab0bed51d0e6d8e4329044acb3e6da\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","Successfully built ConfigSpace\n","Installing collected packages: ConfigSpace\n","Successfully installed ConfigSpace-1.2.1\n"]}]},{"cell_type":"code","source":["!pip install hpbandster"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2UHtYiTefE7","outputId":"23a7f897-43b5-4600-a2b5-746abe7d5834"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: ConfigSpace in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.13.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=7d801c54652812fe791c6295745f9ccbf67f6bc15dcc4429a0496853b765d0b9\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35185 sha256=e12560978d91ceaa7ac8ff62b0b0e541b776b7fdce532e75229fa3ac0b14032f\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, hpbandster\n","Successfully installed Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from catboost import CatBoostRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-directional LSTM outputs 2x hidden_dim\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])  # Take only the last time step output\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors (reshaping for LSTM)\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# Bi-LSTM Configurations\n","lstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","lstm_features = []\n","\n","for num_layers in lstm_layers:\n","    print(f\"Training Bi-LSTM with {num_layers} layers...\")\n","\n","    lstm_model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        lstm_model.train()\n","        optimizer.zero_grad()\n","        outputs = lstm_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    # Extract Feature Representations\n","    lstm_model.eval()\n","    with torch.no_grad():\n","        val_start = time.time()\n","        train_features = lstm_model(X_train_torch).numpy()\n","        val_features = lstm_model(X_val_torch).numpy()\n","        val_time = time.time() - val_start\n","\n","        test_start = time.time()\n","        test_features = lstm_model(X_test_torch).numpy()\n","        test_time = time.time() - test_start\n","\n","    lstm_features.append((train_features, val_features, test_features, train_time, val_time, test_time))\n","\n","# Concatenate Features from All Layers\n","final_train_features = np.hstack([feat[0] for feat in lstm_features])\n","final_val_features = np.hstack([feat[1] for feat in lstm_features])\n","final_test_features = np.hstack([feat[2] for feat in lstm_features])\n","\n","total_train_time = sum([feat[3] for feat in lstm_features])\n","total_val_time = sum([feat[4] for feat in lstm_features])\n","total_test_time = sum([feat[5] for feat in lstm_features])\n","\n","# Define ConfigSpace for BOHB\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"n_estimators\", 50, 500, default_value=100))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"depth\", 3, 10, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"subsample\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for CatBoost\n","class CatBoostWorker(Worker):\n","    def compute(self, config, budget, **kwargs):\n","        model = CatBoostRegressor(\n","            iterations=config[\"n_estimators\"],\n","            learning_rate=config[\"learning_rate\"],\n","            depth=config[\"depth\"],\n","            subsample=config[\"subsample\"],\n","            loss_function='RMSE',\n","            verbose=False,\n","            random_seed=42\n","        )\n","        model.fit(final_train_features, Y_train)\n","        Y_val_pred = model.predict(final_val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB\n","NS = hpns.NameServer(run_id=\"bilstm_catboost_bohb\", host=\"127.0.0.2\", port=None)\n","NS.start()\n","\n","worker = CatBoostWorker(nameserver=\"127.0.0.2\", run_id=\"bilstm_catboost_bohb\")\n","worker.run(background=True)\n","\n","bohb = BOHB(configspace=get_config_space(), run_id=\"bilstm_catboost_bohb\", nameserver=\"127.0.0.2\", min_budget=1, max_budget=3)\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","\n","# Train Best CatBoost Model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_catboost_model = CatBoostRegressor(\n","    iterations=best_params[\"n_estimators\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    depth=best_params[\"depth\"],\n","    subsample=best_params[\"subsample\"],\n","    loss_function='RMSE',\n","    verbose=False,\n","    random_seed=42\n",")\n","\n","best_catboost_model.fit(final_train_features, Y_train)\n","\n","# Predictions\n","Y_train_pred = best_catboost_model.predict(final_train_features)\n","Y_val_pred = best_catboost_model.predict(final_val_features)\n","Y_test_pred = best_catboost_model.predict(final_test_features)\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"Train Metrics:\", train_metrics, \"Time:\", total_train_time)\n","print(\"Validation Metrics:\", val_metrics, \"Time:\", total_val_time)\n","print(\"Test Metrics:\", test_metrics, \"Time:\", total_test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDjRPSqdPhUg","outputId":"5fa383c6-3821-4a2e-87e2-5c61c2ac0464"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Bi-LSTM with 2 layers...\n","Training Bi-LSTM with 3 layers...\n","Training Bi-LSTM with 5 layers...\n","Train Metrics: (0.0035024383508036492, 2.8674922941822208e-05, 0.005354897099088106, 0.9998361629638177, 1.000376864403763) Time: 250.22765851020813\n","Validation Metrics: (0.15750416040680226, 0.030669610051107212, 0.17512741090733686, -4.214551314143464, 8.84073690871622) Time: 1.3575997352600098\n","Test Metrics: (0.426479350415464, 0.18791970672179414, 0.43349706656653875, -30.13794778634166, 21.0388913096686) Time: 0.18187355995178223\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"edscyf4g0rS4"}},{"cell_type":"markdown","source":["# LightBoost"],"metadata":{"id":"f6q5ctHm0w6I"}},{"cell_type":"markdown","source":["## Initial"],"metadata":{"id":"alL2Zfju81mm"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import lightgbm as lgb\n","import pandas as pd\n","import time\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(BiLSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bi_lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.bi_lstm(x, (h0, c0))\n","        return out[:, -1, :]\n","\n","# Set Parameters\n","input_size = 3\n","hidden_size = 64\n","num_layers_list = [2, 3, 5]\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# MinMax Scaling\n","scaler = MinMaxScaler()\n","Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1, 1))\n","Y_val_scaled = scaler.transform(Y_val.values.reshape(-1, 1))\n","Y_test_scaled = scaler.transform(Y_test.values.reshape(-1, 1))\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","Y_train_torch = torch.tensor(Y_train_scaled, dtype=torch.float32).to(device)\n","Y_val_torch = torch.tensor(Y_val_scaled, dtype=torch.float32).to(device)\n","Y_test_torch = torch.tensor(Y_test_scaled, dtype=torch.float32).to(device)\n","\n","# Store embeddings for LGBM\n","train_embeddings, val_embeddings, test_embeddings = [], [], []\n","train_time, val_time, test_time = 0, 0, 0\n","\n","# Train multiple Bi-LSTM models\n","for num_layers in num_layers_list:\n","    print(f\"\\nTraining Bi-LSTM with {num_layers} layers...\")\n","    model = BiLSTMModel(input_size, hidden_size, num_layers).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Train Bi-LSTM\n","    start_time = time.time()\n","    model.train()\n","    for epoch in range(num_epochs):\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","    train_time += time.time() - start_time\n","\n","    # Extract embeddings\n","    model.eval()\n","    with torch.no_grad():\n","        start_time = time.time()\n","        train_embed = model(X_train_torch).cpu().numpy()\n","        val_embed = model(X_val_torch).cpu().numpy()\n","        test_embed = model(X_test_torch).cpu().numpy()\n","        val_time += time.time() - start_time\n","        test_time += time.time() - start_time\n","\n","    train_embeddings.append(train_embed)\n","    val_embeddings.append(val_embed)\n","    test_embeddings.append(test_embed)\n","\n","# Concatenate embeddings from all Bi-LSTM models\n","X_train_lgb = np.concatenate(train_embeddings, axis=1)\n","X_val_lgb = np.concatenate(val_embeddings, axis=1)\n","X_test_lgb = np.concatenate(test_embeddings, axis=1)\n","\n","# Ensure correct label shape\n","Y_train_lgb = Y_train.values.flatten()\n","Y_val_lgb = Y_val.values.flatten()\n","Y_test_lgb = Y_test.values.flatten()\n","\n","# Train LightGBM on Bi-LSTM embeddings\n","print(\"\\nTraining LightGBM on Combined Bi-LSTM Embeddings...\")\n","start_time = time.time()\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train_lgb)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val_lgb, reference=lgb_train)\n","\n","lgb_params = {\n","    \"objective\": \"regression\",\n","    \"metric\": \"rmse\",\n","    \"boosting_type\": \"gbdt\",\n","    \"learning_rate\": 0.05,\n","    \"num_leaves\": 31\n","}\n","\n","lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200, callbacks=[lgb.log_evaluation(50)])\n","train_time += time.time() - start_time\n","\n","# Predictions\n","start_time = time.time()\n","train_pred_lgb = lgb_model.predict(X_train_lgb)\n","val_pred_lgb = lgb_model.predict(X_val_lgb)\n","test_pred_lgb = lgb_model.predict(X_test_lgb)\n","test_time += time.time() - start_time\n","\n","# Compute evaluation metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100  # Avoid divide-by-zero\n","    return mae, mse, rmse, r2, mape\n","\n","# Store results\n","results_df = pd.DataFrame([\n","    [\"Bi-LSTM(2,3,5) + LGBM\", \"Train\", *compute_metrics(Y_train_lgb, train_pred_lgb), train_time],\n","    [\"Bi-LSTM(2,3,5) + LGBM\", \"Validation\", *compute_metrics(Y_val_lgb, val_pred_lgb), val_time],\n","    [\"Bi-LSTM(2,3,5) + LGBM\", \"Test\", *compute_metrics(Y_test_lgb, test_pred_lgb), test_time]\n","], columns=[\"Model\", \"Dataset\", \"MAE\", \"MSE\", \"RMSE\", \"R²\", \"MAPE\", \"Time (s)\"])\n","\n","print(\"\\nFinal Model Performance\\n\")\n","print(results_df.to_string(index=False))\n"],"metadata":{"id":"WsYzf7aK1ntS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a0b79d64-b954-4cae-de12-0986853512c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training Bi-LSTM with 2 layers...\n","Epoch [10/100], Loss: 0.1307\n","Epoch [20/100], Loss: 0.1173\n","Epoch [30/100], Loss: 0.0978\n","Epoch [40/100], Loss: 0.0721\n","Epoch [50/100], Loss: 0.0478\n","Epoch [60/100], Loss: 0.0336\n","Epoch [70/100], Loss: 0.0264\n","Epoch [80/100], Loss: 0.0202\n","Epoch [90/100], Loss: 0.0154\n","Epoch [100/100], Loss: 0.0119\n","\n","Training Bi-LSTM with 3 layers...\n","Epoch [10/100], Loss: 0.1313\n","Epoch [20/100], Loss: 0.1151\n","Epoch [30/100], Loss: 0.0804\n","Epoch [40/100], Loss: 0.0481\n","Epoch [50/100], Loss: 0.0374\n","Epoch [60/100], Loss: 0.0225\n","Epoch [70/100], Loss: 0.0128\n","Epoch [80/100], Loss: 0.0069\n","Epoch [90/100], Loss: 0.0044\n","Epoch [100/100], Loss: 0.0035\n","\n","Training Bi-LSTM with 5 layers...\n","Epoch [10/100], Loss: 0.1297\n","Epoch [20/100], Loss: 0.1111\n","Epoch [30/100], Loss: 0.0648\n","Epoch [40/100], Loss: 0.0498\n","Epoch [50/100], Loss: 0.0276\n","Epoch [60/100], Loss: 0.0086\n","Epoch [70/100], Loss: 0.0047\n","Epoch [80/100], Loss: 0.0046\n","Epoch [90/100], Loss: 0.0044\n","Epoch [100/100], Loss: 0.0040\n","\n","Training LightGBM on Combined Bi-LSTM Embeddings...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047350 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 97920\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 384\n","[LightGBM] [Info] Start training from score 0.454038\n","[50]\ttraining's rmse: 0.0325847\tvalid_1's rmse: 0.259058\n","[100]\ttraining's rmse: 0.00518623\tvalid_1's rmse: 0.180279\n","[150]\ttraining's rmse: 0.00452125\tvalid_1's rmse: 0.174445\n","[200]\ttraining's rmse: 0.00449571\tvalid_1's rmse: 0.173858\n","\n","Final Model Performance\n","\n","                Model    Dataset      MAE      MSE     RMSE         R²      MAPE   Time (s)\n","Bi-LSTM(2,3,5) + LGBM      Train 0.002995 0.000020 0.004496   0.999885  0.899783 261.713328\n","Bi-LSTM(2,3,5) + LGBM Validation 0.156109 0.030227 0.173858  -4.139226  8.760832   1.355527\n","Bi-LSTM(2,3,5) + LGBM       Test 0.425066 0.186716 0.432107 -29.938531 20.968663   1.517414\n"]}]},{"cell_type":"markdown","source":["## Optuna"],"metadata":{"id":"RaaZwErRlD3M"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEuxKGyDj3VW","outputId":"e5a3b8ae-6e00-4152-e8df-5d44516b0609"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["import time\n","import torch\n","import lightgbm as lgb\n","import optuna\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Ensure model is in evaluation mode\n","best_lstm.eval()\n","\n","# Disable gradient tracking\n","torch.no_grad()\n","\n","# Record train time\n","train_time_start = time.time()\n","train_pred = best_lstm(X_train_torch).detach().cpu().numpy()\n","train_time = time.time() - train_time_start\n","\n","# Record validation time\n","val_time_start = time.time()\n","val_pred = best_lstm(X_val_torch).detach().cpu().numpy()\n","val_time = time.time() - val_time_start\n","\n","# Record test time\n","test_time_start = time.time()\n","test_pred = best_lstm(X_test_torch).detach().cpu().numpy()\n","test_time = time.time() - test_time_start\n","\n","# Inverse transform predictions\n","X_train_lgb = scaler.inverse_transform(train_pred.reshape(-1, 1))\n","X_val_lgb = scaler.inverse_transform(val_pred.reshape(-1, 1))\n","X_test_lgb = scaler.inverse_transform(test_pred.reshape(-1, 1))\n","\n","# ----------- LightGBM Optimization -----------\n","def objective_lgb(trial):\n","    params = {\n","        \"objective\": \"regression\", \"metric\": \"rmse\", \"boosting_type\": \"gbdt\",\n","        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 50),\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n","        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 30)\n","    }\n","    lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","    lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200)\n","    val_pred = model.predict(X_val_lgb)\n","    _, _, _, _, mape = compute_metrics(Y_val.values.flatten(), val_pred.flatten())\n","    return mape\n","\n","# Run optimization\n","study_lgb = optuna.create_study(direction=\"minimize\")\n","study_lgb.optimize(objective_lgb, n_trials=20)\n","\n","# Get best parameters\n","best_lgb_params = study_lgb.best_params\n","\n","# Train final LightGBM model\n","lgb_train = lgb.Dataset(X_train_lgb, label=Y_train)\n","lgb_val = lgb.Dataset(X_val_lgb, label=Y_val, reference=lgb_train)\n","final_lgb = lgb.train(best_lgb_params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=200)\n","\n","# Make predictions\n","train_pred_lgb = final_lgb.predict(X_train_lgb)\n","val_pred_lgb = final_lgb.predict(X_val_lgb)\n","test_pred_lgb = final_lgb.predict(X_test_lgb)\n","\n","# Compute metrics\n","metrics_train_lgb = compute_metrics(Y_train.values.flatten(), train_pred_lgb.flatten())\n","metrics_val_lgb = compute_metrics(Y_val.values.flatten(), val_pred_lgb.flatten())\n","metrics_test_lgb = compute_metrics(Y_test.values.flatten(), test_pred_lgb.flatten())\n","\n","# Print results\n","print(\"Best Bi-LSTM Parameters:\", {'hidden_size': 96, 'num_layers': 4, 'learning_rate': 0.00011638809437457186})\n","print(\"Train Time:\", train_time, \"Validation Time:\", val_time, \"Test Time:\", test_time)\n","print(\"Train Metrics:\", metrics_train_lgb)\n","print(\"Validation Metrics:\", metrics_val_lgb)\n","print(\"Test Metrics:\", metrics_test_lgb)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBystXYXLclw","outputId":"34f1ac27-6c91-4c59-85da-8d7ec44ac035"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:40,572] A new study created in memory with name: no-name-c489e9d5-0b69-4c93-8250-689d365d559f\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000526 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:40,883] Trial 0 finished with value: 8.776285397598658 and parameters: {'num_leaves': 37, 'learning_rate': 0.05170968472279115, 'max_depth': 5, 'min_data_in_leaf': 28}. Best is trial 0 with value: 8.776285397598658.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:41,190] Trial 1 finished with value: 8.77577322076169 and parameters: {'num_leaves': 40, 'learning_rate': 0.053220851816039395, 'max_depth': 8, 'min_data_in_leaf': 13}. Best is trial 1 with value: 8.77577322076169.\n","[I 2025-03-05 16:36:41,359] Trial 2 finished with value: 8.776374749764898 and parameters: {'num_leaves': 46, 'learning_rate': 0.06781899354233757, 'max_depth': 4, 'min_data_in_leaf': 26}. Best is trial 1 with value: 8.77577322076169.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000224 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:41,646] Trial 3 finished with value: 8.774558492287419 and parameters: {'num_leaves': 34, 'learning_rate': 0.08517921874241612, 'max_depth': 10, 'min_data_in_leaf': 26}. Best is trial 3 with value: 8.774558492287419.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:41,840] Trial 4 finished with value: 14.358756337699887 and parameters: {'num_leaves': 40, 'learning_rate': 0.01289922416475169, 'max_depth': 4, 'min_data_in_leaf': 10}. Best is trial 3 with value: 8.774558492287419.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000198 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:42,049] Trial 5 finished with value: 10.697776406339713 and parameters: {'num_leaves': 20, 'learning_rate': 0.01801954298595392, 'max_depth': 6, 'min_data_in_leaf': 8}. Best is trial 3 with value: 8.774558492287419.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:42,363] Trial 6 finished with value: 17.520109263918666 and parameters: {'num_leaves': 46, 'learning_rate': 0.010053525588836813, 'max_depth': 10, 'min_data_in_leaf': 14}. Best is trial 3 with value: 8.774558492287419.\n","[I 2025-03-05 16:36:42,564] Trial 7 finished with value: 9.312509224375663 and parameters: {'num_leaves': 25, 'learning_rate': 0.02415300099613293, 'max_depth': 5, 'min_data_in_leaf': 9}. Best is trial 3 with value: 8.774558492287419.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000185 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:42,894] Trial 8 finished with value: 12.196408730408304 and parameters: {'num_leaves': 38, 'learning_rate': 0.01478097434056082, 'max_depth': 8, 'min_data_in_leaf': 11}. Best is trial 3 with value: 8.774558492287419.\n","[I 2025-03-05 16:36:43,083] Trial 9 finished with value: 11.753086848034572 and parameters: {'num_leaves': 23, 'learning_rate': 0.01615558947300231, 'max_depth': 4, 'min_data_in_leaf': 22}. Best is trial 3 with value: 8.774558492287419.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000224 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:43,315] Trial 10 finished with value: 8.774607314701676 and parameters: {'num_leaves': 30, 'learning_rate': 0.09233406129094823, 'max_depth': 10, 'min_data_in_leaf': 20}. Best is trial 3 with value: 8.774558492287419.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000179 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:43,586] Trial 11 finished with value: 8.774671212829462 and parameters: {'num_leaves': 29, 'learning_rate': 0.09967568132940113, 'max_depth': 10, 'min_data_in_leaf': 19}. Best is trial 3 with value: 8.774558492287419.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000138 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:43,868] Trial 12 finished with value: 8.774602114873032 and parameters: {'num_leaves': 31, 'learning_rate': 0.09294935069293747, 'max_depth': 8, 'min_data_in_leaf': 24}. Best is trial 3 with value: 8.774558492287419.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000126 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:44,056] Trial 13 finished with value: 8.84893473133869 and parameters: {'num_leaves': 32, 'learning_rate': 0.03367496752027245, 'max_depth': 8, 'min_data_in_leaf': 24}. Best is trial 3 with value: 8.774558492287419.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000127 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:44,266] Trial 14 finished with value: 9.309758166473657 and parameters: {'num_leaves': 34, 'learning_rate': 0.06757299704720167, 'max_depth': 9, 'min_data_in_leaf': 30}. Best is trial 3 with value: 8.774558492287419.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000127 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:44,441] Trial 15 finished with value: 8.807864656159706 and parameters: {'num_leaves': 27, 'learning_rate': 0.03779255140180411, 'max_depth': 7, 'min_data_in_leaf': 24}. Best is trial 3 with value: 8.774558492287419.\n","[I 2025-03-05 16:36:44,635] Trial 16 finished with value: 8.774542084600505 and parameters: {'num_leaves': 34, 'learning_rate': 0.07449364958315129, 'max_depth': 9, 'min_data_in_leaf': 16}. Best is trial 16 with value: 8.774542084600505.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000129 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000125 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:44,835] Trial 17 finished with value: 8.780225283847576 and parameters: {'num_leaves': 43, 'learning_rate': 0.04591532618032155, 'max_depth': 9, 'min_data_in_leaf': 5}. Best is trial 16 with value: 8.774542084600505.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000198 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:45,230] Trial 18 finished with value: 8.774519812665046 and parameters: {'num_leaves': 50, 'learning_rate': 0.06981985662323106, 'max_depth': 9, 'min_data_in_leaf': 16}. Best is trial 18 with value: 8.774519812665046.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000275 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-05 16:36:45,578] Trial 19 finished with value: 9.18951693323933 and parameters: {'num_leaves': 50, 'learning_rate': 0.025037260932418485, 'max_depth': 7, 'min_data_in_leaf': 15}. Best is trial 18 with value: 8.774519812665046.\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 255\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 1\n","[LightGBM] [Info] Start training from score 0.454038\n","Best Bi-LSTM Parameters: {'hidden_size': 96, 'num_layers': 4, 'learning_rate': 0.00011638809437457186}\n","Train Time: 0.1817765235900879 Validation Time: 0.11294436454772949 Test Time: 0.04371023178100586\n","Train Metrics: (0.003596152206803251, 2.9229971458407824e-05, 0.005406474956790961, 0.9998329916386819, 1.0379758334461942)\n","Validation Metrics: (0.15634845541640022, 0.03030242366683534, 0.1740759135171645, -4.152121037415138, 8.774519812665046)\n","Test Metrics: (0.42530890124488707, 0.18692273186913116, 0.43234561622518064, -29.972750897478512, 20.98073118368091)\n"]}]},{"cell_type":"markdown","source":["## BOHB"],"metadata":{"id":"zrFAJLXx3Kcm"}},{"cell_type":"code","source":["!pip install hpbandster configSpace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAxFPflB2PGk","outputId":"96fc1fff-c861-426c-bba9-50273709e91e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m51.2/51.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting configSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.26.4)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.13.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from configSpace) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from configSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from configSpace) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, configSpace, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=9e3ba89e7f7bc243235e8e82462cb42ceebb946198b095eb5b4b2809a81c76a9\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for configSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configSpace: filename=configspace-1.2.1-py3-none-any.whl size=115950 sha256=19fe42784f284eef2965008240a2de6a5dd34ff643ec3b6fca8e08ec4de79629\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35184 sha256=9b07cafac0fd9444b3d952b85a4eba469ee0ad732837ee5e6895a3b219f935a2\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster configSpace netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, configSpace, hpbandster\n","Successfully installed Pyro4-4.82 configSpace-1.2.1 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import lightgbm as lgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-LSTM doubles hidden size\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# LSTM Configurations\n","lstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to store LSTM feature representations\n","lstm_features = []\n","\n","for num_layers in lstm_layers:\n","    print(f\"Training Bi-LSTM with {num_layers} layers...\")\n","    start_time = time.time()\n","\n","    lstm_model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        lstm_model.train()\n","        optimizer.zero_grad()\n","        outputs = lstm_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Extract Feature Representations\n","    lstm_model.eval()\n","    with torch.no_grad():\n","        train_features = lstm_model(X_train_torch).numpy()\n","        val_features = lstm_model(X_val_torch).numpy()\n","        test_features = lstm_model(X_test_torch).numpy()\n","\n","    lstm_features.append((train_features, val_features, test_features))\n","    print(f\"Bi-LSTM ({num_layers} layers) training time: {time.time() - start_time:.2f} seconds\")\n","\n","# Stack extracted features\n","train_features_stacked = np.hstack([feat[0] for feat in lstm_features])\n","val_features_stacked = np.hstack([feat[1] for feat in lstm_features])\n","test_features_stacked = np.hstack([feat[2] for feat in lstm_features])\n","\n","# Define ConfigSpace for BOHB (LightGBM)\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_leaves\", 20, 300, default_value=50))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 12, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"feature_fraction\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Worker for LightGBM\n","class LightGBMWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = lgb.LGBMRegressor(\n","            num_leaves=config[\"num_leaves\"],\n","            max_depth=config[\"max_depth\"],\n","            learning_rate=config[\"learning_rate\"],\n","            feature_fraction=config[\"feature_fraction\"],\n","            random_state=42\n","        )\n","        model.fit(self.train_features, Y_train)\n","        Y_val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, Y_val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","# Run BOHB for stacked LSTM features\n","print(\"\\nRunning BOHB for Stacked Bi-LSTM + LightGBM...\")\n","start_time = time.time()\n","\n","NS = hpns.NameServer(run_id=\"stacked_bi_lstm_lgb_bohb\", host=\"127.0.0.1\", port=None)\n","NS.start()\n","\n","worker = LightGBMWorker(\n","    train_features=train_features_stacked,\n","    val_features=val_features_stacked,\n","    nameserver=\"127.0.0.1\",\n","    run_id=\"stacked_bi_lstm_lgb_bohb\"\n",")\n","worker.run(background=True)\n","\n","bohb = BOHB(\n","    configspace=get_config_space(),\n","    run_id=\"stacked_bi_lstm_lgb_bohb\",\n","    nameserver=\"127.0.0.1\",\n","    min_budget=1,\n","    max_budget=3\n",")\n","\n","res = bohb.run(n_iterations=50)\n","\n","bohb.shutdown()\n","NS.shutdown()\n","\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","\n","best_lgb_model = lgb.LGBMRegressor(\n","    num_leaves=best_params[\"num_leaves\"],\n","    max_depth=best_params[\"max_depth\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    feature_fraction=best_params[\"feature_fraction\"],\n","    random_state=42\n",")\n","\n","best_lgb_model.fit(train_features_stacked, Y_train)\n","\n","Y_train_pred = best_lgb_model.predict(train_features_stacked)\n","Y_val_pred = best_lgb_model.predict(val_features_stacked)\n","Y_test_pred = best_lgb_model.predict(test_features_stacked)\n","\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","print(f\"\\nBOHB optimization time: {time.time() - start_time:.2f} seconds\")\n","print(\"\\nBest Parameters for Stacked Bi-LSTM + LightGBM:\", best_params)\n","print(\"\\nTraining set metrics:\", train_metrics)\n","print(\"\\nValidation set metrics:\", val_metrics)\n","print(\"\\nTest set metrics:\", test_metrics)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WueB7E3X5krY","outputId":"b094890f-b1c0-4e83-a2fb-a73d9ae00580"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Bi-LSTM with 2 layers...\n","Bi-LSTM (2 layers) training time: 76.92 seconds\n","Training Bi-LSTM with 3 layers...\n","Bi-LSTM (3 layers) training time: 74.55 seconds\n","Training Bi-LSTM with 5 layers...\n","Bi-LSTM (5 layers) training time: 130.02 seconds\n","\n","Running BOHB for Stacked Bi-LSTM + LightGBM...\n","[LightGBM] [Warning] feature_fraction is set=0.720358981684, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.720358981684\n","[LightGBM] [Warning] feature_fraction is set=0.720358981684, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.720358981684\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000300 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 3\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.720358981684, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.720358981684\n","[LightGBM] [Warning] feature_fraction is set=0.720358981684, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.720358981684\n","[LightGBM] [Warning] feature_fraction is set=0.720358981684, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.720358981684\n","\n","BOHB optimization time: 40.99 seconds\n","\n","Best Parameters for Stacked Bi-LSTM + LightGBM: {'feature_fraction': 0.720358981684, 'learning_rate': 0.2409381052371, 'max_depth': 10, 'num_leaves': 274}\n","\n","Training set metrics: (0.003389469286487523, 2.5847266065009077e-05, 0.005084020659380632, 0.9998523190638002, 0.9836685343532379)\n","\n","Validation set metrics: (0.1563454976216439, 0.030301484508357006, 0.17407321594190475, -4.151961358499455, 8.774350395285067)\n","\n","Test set metrics: (0.4253058963284006, 0.18692017584270226, 0.43234266021606316, -29.97232736862743, 20.98058186807983)\n"]}]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import lightgbm as lgb\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","\n","# Define Bi-LSTM Model\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiLSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Function to calculate metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors\n","\n","X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)  # Add sequence length dimension\n","X_val_torch = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)\n","X_test_torch = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","Y_train_torch = torch.tensor(Y_train.values, dtype=torch.float32).unsqueeze(1)\n","Y_val_torch = torch.tensor(Y_val.values, dtype=torch.float32).unsqueeze(1)\n","Y_test_torch = torch.tensor(Y_test.values, dtype=torch.float32).unsqueeze(1)\n","\n","# LSTM Training\n","lstm_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","lstm_features = []\n","train_time_start = time.time()\n","\n","for num_layers in lstm_layers:\n","    lstm_model = BiLSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","    num_epochs = 100\n","\n","    for epoch in range(num_epochs):\n","        lstm_model.train()\n","        optimizer.zero_grad()\n","        outputs = lstm_model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    lstm_model.eval()\n","    with torch.no_grad():\n","        train_features = lstm_model(X_train_torch).numpy()\n","        val_features = lstm_model(X_val_torch).numpy()\n","        test_features = lstm_model(X_test_torch).numpy()\n","\n","    lstm_features.append((train_features, val_features, test_features))\n","\n","train_time = time.time() - train_time_start\n","\n","# Stack extracted features\n","train_features_stacked = np.hstack([feat[0] for feat in lstm_features])\n","val_features_stacked = np.hstack([feat[1] for feat in lstm_features])\n","test_features_stacked = np.hstack([feat[2] for feat in lstm_features])\n","\n","# Define ConfigSpace for BOHB (LightGBM)\n","def get_config_space():\n","    cs = CS.ConfigurationSpace()\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"num_leaves\", 20, 300, default_value=50))\n","    cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\"max_depth\", 3, 12, default_value=6))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"learning_rate\", 0.01, 0.3, default_value=0.1))\n","    cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\"feature_fraction\", 0.5, 1.0, default_value=0.8))\n","    return cs\n","\n","# BOHB Optimization\n","bohb_start_time = time.time()\n","\n","NS = hpns.NameServer(run_id=\"stacked_bi_lstm_lgb_bohb\", host=\"127.0.0.1\", port=None)\n","NS.start()\n","\n","class LightGBMWorker(Worker):\n","    def __init__(self, train_features, val_features, **kwargs):\n","        super().__init__(**kwargs)\n","        self.train_features = train_features\n","        self.val_features = val_features\n","\n","    def compute(self, config, budget, **kwargs):\n","        model = lgb.LGBMRegressor(\n","            num_leaves=config[\"num_leaves\"],\n","            max_depth=config[\"max_depth\"],\n","            learning_rate=config[\"learning_rate\"],\n","            feature_fraction=config[\"feature_fraction\"],\n","            random_state=42\n","        )\n","        model.fit(self.train_features, Y_train)\n","        val_pred = model.predict(self.val_features)\n","        mae = mean_absolute_error(Y_val, val_pred)\n","        return {\"loss\": mae, \"info\": config}\n","\n","worker = LightGBMWorker(\n","    train_features=train_features_stacked,\n","    val_features=val_features_stacked,\n","    nameserver=\"127.0.0.1\",\n","    run_id=\"stacked_bi_lstm_lgb_bohb\"\n",")\n","worker.run(background=True)\n","\n","bohb = BOHB(\n","    configspace=get_config_space(),\n","    run_id=\"stacked_bi_lstm_lgb_bohb\",\n","    nameserver=\"127.0.0.1\",\n","    min_budget=1,\n","    max_budget=3\n",")\n","res = bohb.run(n_iterations=50)\n","bohb.shutdown()\n","NS.shutdown()\n","bohb_time = time.time() - bohb_start_time\n","\n","# Train final LightGBM model\n","best_config = res.get_incumbent_id()\n","best_params = res.get_id2config_mapping()[best_config][\"config\"]\n","best_lgb_model = lgb.LGBMRegressor(**best_params, random_state=42)\n","best_lgb_model.fit(train_features_stacked, Y_train)\n","\n","# Predictions and metrics\n","val_time_start = time.time()\n","Y_val_pred = best_lgb_model.predict(val_features_stacked)\n","val_time = time.time() - val_time_start\n","\n","test_time_start = time.time()\n","Y_test_pred = best_lgb_model.predict(test_features_stacked)\n","test_time = time.time() - test_time_start\n","\n","train_metrics = calculate_metrics(Y_train, best_lgb_model.predict(train_features_stacked))\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print results\n","print(\"Train Time:\", train_time, \"BOHB Optimization Time:\", bohb_time, \"Validation Time:\", val_time, \"Test Time:\", test_time)\n","print(\"Train Metrics:\", train_metrics)\n","print(\"Validation Metrics:\", val_metrics)\n","print(\"Test Metrics:\", test_metrics)\n"],"metadata":{"id":"-ka19jst68RB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"30a3e893-0be2-4157-86d8-d8c493609415"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] feature_fraction is set=0.839198088036, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.839198088036\n","[LightGBM] [Warning] feature_fraction is set=0.839198088036, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.839198088036\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000301 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 765\n","[LightGBM] [Info] Number of data points in the train set: 7736, number of used features: 3\n","[LightGBM] [Info] Start training from score 0.454038\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.839198088036, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.839198088036\n","[LightGBM] [Warning] feature_fraction is set=0.839198088036, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.839198088036\n","[LightGBM] [Warning] feature_fraction is set=0.839198088036, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.839198088036\n","Train Time: 236.15686225891113 BOHB Optimization Time: 43.784982442855835 Validation Time: 0.012754678726196289 Test Time: 0.011949777603149414\n","Train Metrics: (0.0034501448176585546, 2.7304983491058177e-05, 0.005225417063838845, 0.9998439902496946, 1.0033262071690625)\n","Validation Metrics: (0.15634642467298798, 0.030301778863596026, 0.17407406143247198, -4.152011405777392, 8.774403495186434)\n","Test Metrics: (0.425306838148945, 0.18692097696725096, 0.43234358670766815, -29.972460113483212, 20.98062866755041)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"C7lu-QgJRtFR"},"execution_count":null,"outputs":[]}]}