{"cells":[{"cell_type":"markdown","metadata":{"id":"CnsEk1fS0iyV"},"source":["#Initial Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lu4T-9m3zQJi"},"outputs":[],"source":["# Importing necessary libraries for data analysis and manipulation\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","# For handling warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JUpagbGDz0V3","outputId":"4fd9987a-43ff-4079-c490-e6a7ce512ffc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mamlRge5z4td"},"outputs":[],"source":["df_aapl = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stocks/AAPL.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDwyIdyez6km"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import boxcox\n","\n","df_aapl['Close_log'] = np.log(df_aapl['Close'] + 1)\n","df_aapl['Close_sqrt'] = np.sqrt(df_aapl['Close'])\n","df_aapl['Close_boxcox'], _ = boxcox(df_aapl['Close'] + 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g4HjyD_S0LL2","outputId":"f32b8147-07b6-4100-bb03-04822e6121df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Skewness: 2.5045276102319933\n","Log Transformation Skewness: 0.8535555176510303\n","Square Root Transformation Skewness: 1.6211545809555206\n","Box-Cox Transformation Skewness: 0.43527466713563334\n"]}],"source":["\n","skew_original = df_aapl['Close'].skew()\n","skew_log = df_aapl['Close_log'].skew()\n","skew_sqrt = df_aapl['Close_sqrt'].skew()\n","skew_boxcox = pd.Series(df_aapl['Close_boxcox']).skew()\n","\n","print(f\"Original Skewness: {skew_original}\")\n","print(f\"Log Transformation Skewness: {skew_log}\")\n","print(f\"Square Root Transformation Skewness: {skew_sqrt}\")\n","print(f\"Box-Cox Transformation Skewness: {skew_boxcox}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CXBpZTd0N-v"},"outputs":[],"source":["\n","df_aapl['Open_log'] = np.log(df_aapl['Open'])\n","df_aapl['High_log'] = np.log(df_aapl['High'])\n","df_aapl['Low_log'] = np.log(df_aapl['Low'])\n","df_aapl['Adj Close_log'] = np.log(df_aapl['Adj Close'])\n","df_aapl['Volume_log'] = np.log(df_aapl['Volume'])\n","\n","\n","df_aapl['Open_sqrt'] = np.sqrt(df_aapl['Open'])\n","df_aapl['High_sqrt'] = np.sqrt(df_aapl['High'])\n","df_aapl['Low_sqrt'] = np.sqrt(df_aapl['Low'])\n","df_aapl['Adj Close_sqrt'] = np.sqrt(df_aapl['Adj Close'])\n","df_aapl['Volume_sqrt'] = np.sqrt(df_aapl['Volume'])\n","\n","from scipy.stats import boxcox\n","df_aapl['Open_boxcox'], _ = boxcox(df_aapl['Open'])\n","df_aapl['High_boxcox'], _ = boxcox(df_aapl['High'])\n","df_aapl['Low_boxcox'], _ = boxcox(df_aapl['Low'])\n","df_aapl['Adj Close_boxcox'], _ = boxcox(df_aapl['Adj Close'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4np9tcs0Q1c","outputId":"66136bac-ac9a-4b8e-9190-fcdad4bb3148"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness Before Transformation:\n"," Open         2.504632\n","High         2.502208\n","Low          2.506714\n","Adj Close    2.550677\n","Volume       3.565699\n","dtype: float64\n","\n","Skewness After Transformation:\n"," Open_log            0.482872\n","High_log            0.481997\n","Low_log             0.484246\n","Adj Close_log       0.494009\n","Open_sqrt           1.620771\n","High_sqrt           1.621456\n","Low_sqrt            1.620661\n","Adj Close_sqrt      1.679402\n","Volume_sqrt         1.299776\n","Open_boxcox         0.181226\n","High_boxcox         0.179749\n","Low_boxcox          0.182882\n","Adj Close_boxcox    0.180085\n","dtype: float64\n"]}],"source":["\n","skewness_before = df_aapl[['Open', 'High', 'Low', 'Adj Close', 'Volume']].skew()\n","skewness_after = df_aapl[['Open_log', 'High_log', 'Low_log', 'Adj Close_log',\n","                          'Open_sqrt', 'High_sqrt', 'Low_sqrt', 'Adj Close_sqrt', 'Volume_sqrt',\n","                          'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox']].skew()\n","\n","print(\"Skewness Before Transformation:\\n\", skewness_before)\n","print(\"\\nSkewness After Transformation:\\n\", skewness_after)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FO7fhx2D0S7g","outputId":"378a848e-1ac0-43fc-e1b9-80df590da957"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skewness After Box-Cox Transformation:\n","Open_boxcox         0.435237\n","High_boxcox         0.433381\n","Low_boxcox          0.437331\n","Adj Close_boxcox    0.458762\n","Close_boxcox        0.435275\n","dtype: float64\n"]}],"source":["from scipy import stats\n","\n","df_aapl['Open_boxcox'], _ = stats.boxcox(df_aapl['Open'] + 1)\n","df_aapl['High_boxcox'], _ = stats.boxcox(df_aapl['High'] + 1)\n","df_aapl['Low_boxcox'], _ = stats.boxcox(df_aapl['Low'] + 1)\n","df_aapl['Adj Close_boxcox'], _ = stats.boxcox(df_aapl['Adj Close'] + 1)\n","df_aapl['Close_boxcox'], _ = stats.boxcox(df_aapl['Close'] + 1)\n","\n","skewness_after_boxcox = df_aapl[['Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox', 'Close_boxcox']].skew()\n","\n","print(\"Skewness After Box-Cox Transformation:\")\n","print(skewness_after_boxcox)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zWTa3Ony0VjQ","outputId":"1f7c174a-4efc-40a3-f13c-3795ea218091"},"outputs":[{"output_type":"stream","name":"stdout","text":["         Date      Open      High       Low  Adj Close     Close     Volume  \\\n","0  1980-12-12  0.128348  0.128906  0.128348   0.098943  0.128348  469033600   \n","1  1980-12-15  0.122210  0.122210  0.121652   0.093781  0.121652  175884800   \n","2  1980-12-16  0.113281  0.113281  0.112723   0.086898  0.112723  105728000   \n","3  1980-12-17  0.115513  0.116071  0.115513   0.089049  0.115513   86441600   \n","4  1980-12-18  0.118862  0.119420  0.118862   0.091630  0.118862   73449600   \n","\n","   Open_boxcox  High_boxcox  Low_boxcox  Adj Close_boxcox  Close_boxcox  \n","0     0.117689     0.118173    0.117674          0.092374      0.117689  \n","1     0.112503     0.112516    0.112016          0.087857      0.112030  \n","2     0.104886     0.104897    0.104395          0.081785      0.104407  \n","3     0.106798     0.107287    0.106786          0.083688      0.106798  \n","4     0.109657     0.110145    0.109644          0.085966      0.109657  \n"]}],"source":["\n","df_aapl_cleaned = df_aapl[['Date', 'Open', 'High', 'Low', 'Adj Close', 'Close', 'Volume',\n","                           'Open_boxcox', 'High_boxcox', 'Low_boxcox', 'Adj Close_boxcox',\n","                           'Close_boxcox']]\n","\n","print(df_aapl_cleaned.head())\n"]},{"cell_type":"markdown","metadata":{"id":"Vcl4PSQ90b0t"},"source":["# Train Validation Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJ9DHEUQ0Xxj","outputId":"9b7ee1d3-c4bb-49d0-ba19-4aa10ab7ffca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training set: (7736, 3), Validation set: (1658, 3), Test set: (1658, 3)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","X = df_aapl_cleaned[['Open_boxcox', 'High_boxcox', 'Low_boxcox']]\n","Y = df_aapl_cleaned['Close_boxcox']\n","\n","X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, shuffle=False)\n","X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, shuffle=False)\n","\n","print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n"]},{"cell_type":"markdown","metadata":{"id":"CNituZbAbpCR"},"source":["#Random Forest"]},{"cell_type":"markdown","metadata":{"id":"6Z1_uKYO5JWl"},"source":["###Initial"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77greOZa0p3s","outputId":"55cba0a1-fb8e-4d48-a1c7-45a66276309a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n","Train Metrics: MAE=0.0021, MSE=0.0000, RMSE=0.0029, R²=1.0000, MAPE=0.68%\n","Validation Metrics: MAE=0.1373, MSE=0.0245, RMSE=0.1566, R²=-3.1719, MAPE=7.69%\n","Test Metrics: MAE=0.4057, MSE=0.1706, RMSE=0.4130, R²=-27.2669, MAPE=20.00%\n","Execution Times:\n","Total Bi-GRU Train Time: 716.87 seconds\n","Total Train Time: 719.55 seconds\n","Total Validation Time: 0.10 seconds\n","Total Test Time: 0.03 seconds\n"]}],"source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, GRU, Dense\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Enable GPU for TensorFlow\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train a Bi-GRU model on GPU\n","def train_bi_gru(X_train, Y_train, X_val, Y_val, layers):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(Bidirectional(GRU(64, return_sequences=(layers > 1)), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(Bidirectional(GRU(64, return_sequences=(_ < layers - 2))))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        start_time = time.time()\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","        train_time = time.time() - start_time\n","        return model, train_time\n","\n","# Reshaping input for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train Bi-GRU models\n","total_train_time = 0\n","bi_gru_models = {}\n","bi_gru_predictions = {}\n","\n","times = {}\n","\n","for layers in [2, 3, 5]:\n","    model, train_time = train_bi_gru(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    total_train_time += train_time\n","    Y_train_pred = model.predict(X_train_r)\n","    Y_val_pred = model.predict(X_val_r)\n","    Y_test_pred = model.predict(X_test_r)\n","\n","    bi_gru_models[layers] = model\n","    bi_gru_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","\n","times['Total Bi-GRU Train Time'] = total_train_time\n","\n","# Prepare input for Random Forest\n","X_train_rf = np.column_stack([bi_gru_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_rf = np.column_stack([bi_gru_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_rf = np.column_stack([bi_gru_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train Random Forest model\n","start_time = time.time()\n","rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n","rf_model.fit(X_train_rf, Y_train)\n","total_train_time += time.time() - start_time\n","\n","times['Total Train Time'] = total_train_time\n","\n","# Predictions from Random Forest\n","start_time = time.time()\n","Y_train_pred_rf = rf_model.predict(X_train_rf)\n","total_val_time = time.time() - start_time\n","\n","start_time = time.time()\n","Y_val_pred_rf = rf_model.predict(X_val_rf)\n","total_val_time += time.time() - start_time\n","\n","times['Total Validation Time'] = total_val_time\n","\n","start_time = time.time()\n","Y_test_pred_rf = rf_model.predict(X_test_rf)\n","total_test_time = time.time() - start_time\n","\n","times['Total Test Time'] = total_test_time\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_rf)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_rf)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_rf)\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print training times\n","print(\"Execution Times:\")\n","for model, t in times.items():\n","    print(f\"{model}: {t:.2f} seconds\")"]},{"cell_type":"markdown","metadata":{"id":"amY77qVbMX8C"},"source":["###Optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a61tNe-e8JdZ","outputId":"9081eecb-94d8-4a70-93f0-734a7a2db2c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.39)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, alembic, optuna\n","Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.2.1\n"]}],"source":["!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sz2_OMz28HdT","outputId":"1fb2f65b-dc3a-46f3-ca68-cb9fc8405db7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","Bi-GRU with 2 layers trained in 135.30 seconds.\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","Bi-GRU with 3 layers trained in 199.70 seconds.\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-29 08:52:11,031] A new study created in memory with name: no-name-628ce4bd-1d84-408d-a478-35faf04aef25\n"]},{"name":"stdout","output_type":"stream","text":["Bi-GRU with 5 layers trained in 381.47 seconds.\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-03-29 08:52:12,893] Trial 0 finished with value: 0.1448476440720555 and parameters: {'n_estimators': 262, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.1448476440720555.\n","[I 2025-03-29 08:52:14,445] Trial 1 finished with value: 0.22588101145349068 and parameters: {'n_estimators': 451, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 0 with value: 0.1448476440720555.\n","[I 2025-03-29 08:52:15,539] Trial 2 finished with value: 0.14318966879861666 and parameters: {'n_estimators': 154, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.14318966879861666.\n","[I 2025-03-29 08:52:16,709] Trial 3 finished with value: 0.1410046767913257 and parameters: {'n_estimators': 150, 'max_depth': 22, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.1410046767913257.\n","[I 2025-03-29 08:52:18,040] Trial 4 finished with value: 0.1381709151833608 and parameters: {'n_estimators': 218, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 4 with value: 0.1381709151833608.\n","[I 2025-03-29 08:52:24,747] Trial 5 finished with value: 0.13764065289109906 and parameters: {'n_estimators': 228, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 5 with value: 0.13764065289109906.\n","[I 2025-03-29 08:52:27,002] Trial 6 finished with value: 0.1482426295433206 and parameters: {'n_estimators': 340, 'max_depth': 16, 'min_samples_split': 10, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 5 with value: 0.13764065289109906.\n","[I 2025-03-29 08:52:27,261] Trial 7 finished with value: 0.18851693808017486 and parameters: {'n_estimators': 58, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 5 with value: 0.13764065289109906.\n","[I 2025-03-29 08:52:27,664] Trial 8 finished with value: 0.22598218988341384 and parameters: {'n_estimators': 110, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 5 with value: 0.13764065289109906.\n","[I 2025-03-29 08:52:29,321] Trial 9 finished with value: 0.2258311105709242 and parameters: {'n_estimators': 224, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 5 with value: 0.13764065289109906.\n","[I 2025-03-29 08:52:39,270] Trial 10 finished with value: 0.13753551259702346 and parameters: {'n_estimators': 361, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 10 with value: 0.13753551259702346.\n","[I 2025-03-29 08:52:47,363] Trial 11 finished with value: 0.13750058273049462 and parameters: {'n_estimators': 348, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 11 with value: 0.13750058273049462.\n","[I 2025-03-29 08:52:55,694] Trial 12 finished with value: 0.14082622197843736 and parameters: {'n_estimators': 369, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 11 with value: 0.13750058273049462.\n","[I 2025-03-29 08:53:06,237] Trial 13 finished with value: 0.14080701509317445 and parameters: {'n_estimators': 488, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 11 with value: 0.13750058273049462.\n","[I 2025-03-29 08:53:14,506] Trial 14 finished with value: 0.1375422221753757 and parameters: {'n_estimators': 360, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 11 with value: 0.13750058273049462.\n","[I 2025-03-29 08:53:23,422] Trial 15 finished with value: 0.14189452462105162 and parameters: {'n_estimators': 423, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 11 with value: 0.13750058273049462.\n","[I 2025-03-29 08:53:29,528] Trial 16 finished with value: 0.13987975286360776 and parameters: {'n_estimators': 307, 'max_depth': 27, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 11 with value: 0.13750058273049462.\n","[I 2025-03-29 08:53:39,138] Trial 17 finished with value: 0.13987367344521365 and parameters: {'n_estimators': 402, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 11 with value: 0.13750058273049462.\n","[I 2025-03-29 08:53:44,563] Trial 18 finished with value: 0.14304889040623803 and parameters: {'n_estimators': 300, 'max_depth': 27, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 11 with value: 0.13750058273049462.\n","[I 2025-03-29 08:53:55,017] Trial 19 finished with value: 0.1407418464056355 and parameters: {'n_estimators': 491, 'max_depth': 23, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 11 with value: 0.13750058273049462.\n","[I 2025-03-29 08:54:03,834] Trial 20 finished with value: 0.14189683406021142 and parameters: {'n_estimators': 411, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 11 with value: 0.13750058273049462.\n","[I 2025-03-29 08:54:11,800] Trial 21 finished with value: 0.1374936132186221 and parameters: {'n_estimators': 351, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 21 with value: 0.1374936132186221.\n","[I 2025-03-29 08:54:20,954] Trial 22 finished with value: 0.13742833088029344 and parameters: {'n_estimators': 327, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 22 with value: 0.13742833088029344.\n","[I 2025-03-29 08:54:28,084] Trial 23 finished with value: 0.13987652420434452 and parameters: {'n_estimators': 322, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 22 with value: 0.13742833088029344.\n","[I 2025-03-29 08:54:34,958] Trial 24 finished with value: 0.13747222767166087 and parameters: {'n_estimators': 265, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 22 with value: 0.13742833088029344.\n"]},{"name":"stdout","output_type":"stream","text":["Optuna tuning completed in 143.93 seconds.\n","Best Hyperparameters: {'n_estimators': 327, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n","Random Forest trained in 8.70 seconds.\n","Total Execution Time: 869.10 seconds.\n","Train Metrics: MAE=0.0012, MSE=0.0000, RMSE=0.0018, R²=1.0000, MAPE=0.36%\n","Validation Metrics: MAE=0.1374, MSE=0.0246, RMSE=0.1567, R²=-3.1771, MAPE=7.69%\n","Test Metrics: MAE=0.4058, MSE=0.1707, RMSE=0.4131, R²=-27.2818, MAPE=20.01%\n"]}],"source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, GRU, Dense\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import optuna\n","\n","# Enable GPU for TensorFlow\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train a Bi-GRU model on GPU\n","def train_bi_gru(X_train, Y_train, X_val, Y_val, layers):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(Bidirectional(GRU(64, return_sequences=(layers > 1)), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(Bidirectional(GRU(64, return_sequences=(_ < layers - 2))))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        start_time = time.time()\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","        train_time = time.time() - start_time\n","        return model, train_time\n","\n","# Reshaping input for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train Bi-GRU models\n","bi_gru_predictions = {}\n","training_times = {}\n","\n","for layers in [2, 3, 5]:\n","    start_time = time.time()\n","    model, train_time = train_bi_gru(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    Y_train_pred = model.predict(X_train_r)\n","    Y_val_pred = model.predict(X_val_r)\n","    Y_test_pred = model.predict(X_test_r)\n","    bi_gru_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","    training_times[layers] = train_time\n","    print(f\"Bi-GRU with {layers} layers trained in {train_time:.2f} seconds.\")\n","\n","# Prepare input for Random Forest\n","X_train_rf = np.column_stack([bi_gru_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_rf = np.column_stack([bi_gru_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_rf = np.column_stack([bi_gru_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# **OPTUNA Hyperparameter Optimization for Random Forest**\n","def objective(trial):\n","    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n","    max_depth = trial.suggest_int(\"max_depth\", 3, 30)\n","    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n","    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n","    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n","\n","    # Train Random Forest\n","    rf_model = RandomForestRegressor(\n","        n_estimators=n_estimators,\n","        max_depth=max_depth,\n","        min_samples_split=min_samples_split,\n","        min_samples_leaf=min_samples_leaf,\n","        max_features=max_features,\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","    rf_model.fit(X_train_rf, Y_train)\n","\n","    # Validation Prediction\n","    Y_val_pred_rf = rf_model.predict(X_val_rf)\n","\n","    # Return MAE as the optimization metric (lower is better)\n","    return mean_absolute_error(Y_val, Y_val_pred_rf)\n","\n","# Run Optuna Optimization\n","start_optuna_time = time.time()\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=25)  # Reduced trials to 25\n","optuna_time = time.time() - start_optuna_time\n","print(f\"Optuna tuning completed in {optuna_time:.2f} seconds.\")\n","\n","# Get Best Hyperparameters\n","best_params = study.best_params\n","print(\"Best Hyperparameters:\", best_params)\n","\n","# Train the best Random Forest model with tuned hyperparameters\n","start_rf_train_time = time.time()\n","best_rf_model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n","best_rf_model.fit(X_train_rf, Y_train)\n","rf_train_time = time.time() - start_rf_train_time\n","print(f\"Random Forest trained in {rf_train_time:.2f} seconds.\")\n","\n","# Predictions from the optimized Random Forest\n","Y_train_pred_rf = best_rf_model.predict(X_train_rf)\n","Y_val_pred_rf = best_rf_model.predict(X_val_rf)\n","Y_test_pred_rf = best_rf_model.predict(X_test_rf)\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, Y_train_pred_rf)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_rf)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_rf)\n","\n","total_execution_time = sum(training_times.values()) + optuna_time + rf_train_time\n","print(f\"Total Execution Time: {total_execution_time:.2f} seconds.\")\n","\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"GRzBkYFRb_qc"},"source":["### BOHB"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ta7ijvKINN5q","outputId":"3481fb12-ca8b-460c-84b3-d87ba18e0713"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (2.0.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.14.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (10.6.0)\n","Building wheels for collected packages: ConfigSpace\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=configspace-1.2.1-py3-none-any.whl size=115990 sha256=6a507edecc2d9c5363e84408067da73ce50b219f32f044351fff5b0db2193fae\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","Successfully built ConfigSpace\n","Installing collected packages: ConfigSpace\n","Successfully installed ConfigSpace-1.2.1\n"]}],"source":["!pip install ConfigSpace"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fw_dxLhoNZxw","outputId":"5769e653-6abf-4723-e1fd-398f59b85dc6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: ConfigSpace in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (2.0.2)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.14.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=f6500d7e645173aa72a073b5a3862e4e05fde8e6784b06372d5895183e790e90\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35182 sha256=92752dd7b0ebbad5652b6ad7fc9d37fbe7a453dc8009f9990a3601bed4dcba10\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, hpbandster\n","Successfully installed Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}],"source":["!pip install hpbandster\n"]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiGRUModel, self).__init__()\n","        self.bigru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-directional outputs\n","\n","    def forward(self, x):\n","        out, _ = self.bigru(x)\n","        out = self.fc(out[:, -1, :])  # Use last hidden state\n","        return out\n","\n","# Function to Calculate Metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","def to_torch_tensor(data):\n","    return torch.tensor(data.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","Y_train_torch, Y_val_torch, Y_test_torch = map(to_torch_tensor, [Y_train, Y_val, Y_test])\n","X_train_torch, X_val_torch, X_test_torch = map(to_torch_tensor, [X_train, X_val, X_test])\n","\n","# Bi-GRU Configurations\n","bi_gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to Store Bi-GRU Feature Representations\n","bi_gru_features = []\n","\n","for num_layers in bi_gru_layers:\n","    print(f\"\\nTraining Bi-GRU with {num_layers} layers...\")\n","\n","    model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    num_epochs = 30\n","\n","    # Training Loop\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    # Extract Feature Representations\n","    model.eval()\n","    with torch.no_grad():\n","        val_start = time.time()\n","        train_features = model(X_train_torch).cpu().numpy()\n","        val_features = model(X_val_torch).cpu().numpy()\n","        val_time = time.time() - val_start\n","\n","        test_start = time.time()\n","        test_features = model(X_test_torch).cpu().numpy()\n","        test_time = time.time() - test_start\n","\n","    bi_gru_features.append((train_features, val_features, test_features, train_time, val_time, test_time))\n","\n","# Concatenate Features from All Layers\n","final_train_features = np.hstack([feat[0] for feat in bi_gru_features])\n","final_val_features = np.hstack([feat[1] for feat in bi_gru_features])\n","final_test_features = np.hstack([feat[2] for feat in bi_gru_features])\n","\n","# Total Time Records\n","total_train_time = sum([feat[3] for feat in bi_gru_features])\n","total_val_time = sum([feat[4] for feat in bi_gru_features])\n","total_test_time = sum([feat[5] for feat in bi_gru_features])\n","\n","print(f\"\\nTotal Train Time: {total_train_time:.2f}s | Val Time: {total_val_time:.2f}s | Test Time: {total_test_time:.2f}s\")\n","\n","# Train RandomForest\n","rf_model = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42)\n","rf_model.fit(final_train_features, Y_train)\n","\n","# Predictions\n","Y_train_pred = rf_model.predict(final_train_features)\n","Y_val_pred = rf_model.predict(final_val_features)\n","Y_test_pred = rf_model.predict(final_test_features)\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"\\nTrain Metrics:\", train_metrics, \"| Time:\", total_train_time)\n","print(\"Validation Metrics:\", val_metrics, \"| Time:\", total_val_time)\n","print(\"Test Metrics:\", test_metrics, \"| Time:\", total_test_time)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63B8cy0qftBu","outputId":"82acf516-3ae4-4c74-a92f-948036dac821"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","\n","Training Bi-GRU with 2 layers...\n","\n","Training Bi-GRU with 3 layers...\n","\n","Training Bi-GRU with 5 layers...\n","\n","Total Train Time: 30.25s | Val Time: 0.45s | Test Time: 0.07s\n","\n","Train Metrics: (0.003921404004600693, 2.751615565207013e-05, np.float64(0.005245584395667477), 0.9998427836964615, 1.402713608131611) | Time: 30.24535059928894\n","Validation Metrics: (0.14799651665559532, 0.027693688043290424, np.float64(0.16641420625442535), -3.708574942393369, 8.296616688333128) | Time: 0.44842028617858887\n","Test Metrics: (0.4167300727337636, 0.17969902391168596, np.float64(0.4239092165920505), -28.775795851482673, 20.554445481083615) | Time: 0.07289886474609375\n"]}]},{"cell_type":"markdown","source":["# Extra Tree"],"metadata":{"id":"6sIgbTnQs-Xq"}},{"cell_type":"markdown","source":["##Initial"],"metadata":{"id":"vmHN3eRMtAMh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVkFpDat_8f6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5791a94-79e4-4759-f08f-b34c1e09447f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\n","Performance Metrics:\n","Train Metrics: MAE=0.0001, MSE=0.0000, RMSE=0.0003, R²=1.0000, MAPE=0.02%\n","Validation Metrics: MAE=0.1352, MSE=0.0239, RMSE=0.1546, R²=-3.0652, MAPE=7.56%\n","Test Metrics: MAE=0.4034, MSE=0.1687, RMSE=0.4108, R²=-26.9577, MAPE=19.89%\n"]}],"source":["\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, GRU, Dense\n","from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import time\n","\n","# Check GPU availability\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to train Bi-GRU model\n","def train_bi_gru(X_train, Y_train, X_val, Y_val, layers):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(Bidirectional(GRU(64, return_sequences=(layers > 1)), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(Bidirectional(GRU(64, return_sequences=(_ < layers - 2))))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","        return model\n","\n","# Reshape input data for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Train multiple Bi-GRU models\n","bi_gru_models = {}\n","bi_gru_predictions = {}\n","\n","for layers in [2, 3, 5]:\n","    model = train_bi_gru(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    bi_gru_models[layers] = model\n","\n","# Extract predictions from Bi-GRU models\n","for layers in [2, 3, 5]:\n","    Y_train_pred = bi_gru_models[layers].predict(X_train_r)\n","    Y_val_pred = bi_gru_models[layers].predict(X_val_r)\n","    Y_test_pred = bi_gru_models[layers].predict(X_test_r)\n","    bi_gru_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","\n","# Prepare input for Extra Trees Regressor\n","X_train_et = np.column_stack([bi_gru_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_et = np.column_stack([bi_gru_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_et = np.column_stack([bi_gru_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Train Extra Trees Regressor\n","et_model = ExtraTreesRegressor(n_estimators=100, max_depth=None, min_samples_split=2,\n","                              min_samples_leaf=1, random_state=42, n_jobs=-1)\n","et_model.fit(X_train_et, Y_train)\n","\n","# Predict with Extra Trees Regressor\n","Y_train_pred_et = et_model.predict(X_train_et)\n","Y_val_pred_et = et_model.predict(X_val_et)\n","Y_test_pred_et = et_model.predict(X_test_et)\n","\n","# Compute performance metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","metrics_train = compute_metrics(Y_train, Y_train_pred_et)\n","metrics_val = compute_metrics(Y_val, Y_val_pred_et)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_et)\n","\n","# Print performance metrics\n","print(\"\\nPerformance Metrics:\")\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n"]},{"cell_type":"markdown","source":["## Optuna"],"metadata":{"id":"bQnXlX-Lxi4o"}},{"cell_type":"code","source":["!pip install Optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kopg49aWxnpP","outputId":"a3d1512a-9e65-48ac-d597-5deaf62d1753"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from Optuna) (1.15.2)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from Optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from Optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from Optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from Optuna) (2.0.39)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from Optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from Optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->Optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->Optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->Optuna) (3.1.1)\n"]}]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import optuna\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, GRU, Dense\n","from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Enable GPU for TensorFlow\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        print(\"GPU activated for TensorFlow!\")\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Function to define and train a Bi-GRU model on GPU\n","def train_bi_gru(X_train, Y_train, X_val, Y_val, layers):\n","    with tf.device('/GPU:0'):\n","        model = Sequential()\n","        model.add(Bidirectional(GRU(64, return_sequences=(layers > 1)), input_shape=(X_train.shape[1], 1)))\n","        for _ in range(layers - 1):\n","            model.add(Bidirectional(GRU(64, return_sequences=(_ < layers - 2))))\n","        model.add(Dense(1))\n","\n","        model.compile(optimizer='adam', loss='mse')\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=16, verbose=0)\n","        return model\n","\n","# Reshaping input for GRU\n","X_train_r = np.expand_dims(X_train, axis=-1)\n","X_val_r = np.expand_dims(X_val, axis=-1)\n","X_test_r = np.expand_dims(X_test, axis=-1)\n","\n","# Initialize timing dictionary\n","times = {}\n","\n","# Train Phase (includes training all Bi-GRUs and Extra Trees)\n","start_train_time = time.time()\n","\n","# Train 2, 3, and 5-layer Bi-GRU models\n","bi_gru_models = {}\n","bi_gru_predictions = {}\n","\n","for layers in [2, 3, 5]:\n","    model = train_bi_gru(X_train_r, Y_train, X_val_r, Y_val, layers)\n","    bi_gru_models[layers] = model\n","\n","# Generate predictions from all Bi-GRU models\n","for layers in [2, 3, 5]:\n","    Y_train_pred = bi_gru_models[layers].predict(X_train_r)\n","    Y_val_pred = bi_gru_models[layers].predict(X_val_r)\n","    Y_test_pred = bi_gru_models[layers].predict(X_test_r)\n","    bi_gru_predictions[layers] = (Y_train_pred, Y_val_pred, Y_test_pred)\n","\n","# Prepare input for Extra Trees\n","X_train_et = np.column_stack([bi_gru_predictions[layers][0] for layers in [2, 3, 5]])\n","X_val_et = np.column_stack([bi_gru_predictions[layers][1] for layers in [2, 3, 5]])\n","X_test_et = np.column_stack([bi_gru_predictions[layers][2] for layers in [2, 3, 5]])\n","\n","# Optuna Hyperparameter Tuning\n","def objective(trial):\n","    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n","    max_depth = trial.suggest_int('max_depth', 3, 15)\n","    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n","    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n","\n","    et_model = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=max_depth,\n","                                   min_samples_split=min_samples_split,\n","                                   min_samples_leaf=min_samples_leaf, random_state=42, n_jobs=-1)\n","    et_model.fit(X_train_et, Y_train)\n","    Y_val_pred_et = et_model.predict(X_val_et)\n","    return mean_absolute_error(Y_val, Y_val_pred_et)\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=20)\n","\n","# Train Best Extra Trees Model\n","best_params = study.best_params\n","best_et_model = ExtraTreesRegressor(**best_params, random_state=42, n_jobs=-1)\n","best_et_model.fit(X_train_et, Y_train)\n","\n","times['Total Train Time'] = time.time() - start_train_time\n","\n","# Validation Phase\n","start_val_time = time.time()\n","Y_val_pred_et = best_et_model.predict(X_val_et)\n","times['Total Validate Time'] = time.time() - start_val_time\n","\n","# Test Phase\n","start_test_time = time.time()\n","Y_test_pred_et = best_et_model.predict(X_test_et)\n","times['Total Test Time'] = time.time() - start_test_time\n","\n","# Function to calculate metrics\n","def compute_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Compute and print metrics\n","metrics_train = compute_metrics(Y_train, best_et_model.predict(X_train_et))\n","metrics_val = compute_metrics(Y_val, Y_val_pred_et)\n","metrics_test = compute_metrics(Y_test, Y_test_pred_et)\n","\n","print(\"\\nPerformance Metrics:\")\n","print(f\"Train Metrics: MAE={metrics_train[0]:.4f}, MSE={metrics_train[1]:.4f}, RMSE={metrics_train[2]:.4f}, R²={metrics_train[3]:.4f}, MAPE={metrics_train[4]:.2f}%\")\n","print(f\"Validation Metrics: MAE={metrics_val[0]:.4f}, MSE={metrics_val[1]:.4f}, RMSE={metrics_val[2]:.4f}, R²={metrics_val[3]:.4f}, MAPE={metrics_val[4]:.2f}%\")\n","print(f\"Test Metrics: MAE={metrics_test[0]:.4f}, MSE={metrics_test[1]:.4f}, RMSE={metrics_test[2]:.4f}, R²={metrics_test[3]:.4f}, MAPE={metrics_test[4]:.2f}%\")\n","\n","# Print timing information\n","print(\"\\nTiming Information:\")\n","for phase, t in times.items():\n","    print(f\"{phase}: {t:.2f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1o02B2cxs65","outputId":"abda74f2-09a4-43fd-de6d-4e4771d2cbe8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n","\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-03-29 16:51:38,573] A new study created in memory with name: no-name-193592ad-b534-451e-9273-f8b6f18b704f\n","[I 2025-03-29 16:51:39,165] Trial 0 finished with value: 0.19623070937780243 and parameters: {'n_estimators': 281, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.19623070937780243.\n","[I 2025-03-29 16:51:39,346] Trial 1 finished with value: 0.2028513448046366 and parameters: {'n_estimators': 73, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.19623070937780243.\n","[I 2025-03-29 16:51:40,347] Trial 2 finished with value: 0.13635199035045734 and parameters: {'n_estimators': 186, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.13635199035045734.\n","[I 2025-03-29 16:51:41,453] Trial 3 finished with value: 0.1429724669722488 and parameters: {'n_estimators': 247, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 5}. Best is trial 2 with value: 0.13635199035045734.\n","[I 2025-03-29 16:51:41,888] Trial 4 finished with value: 0.1406609754221815 and parameters: {'n_estimators': 81, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.13635199035045734.\n","[I 2025-03-29 16:51:42,504] Trial 5 finished with value: 0.14399217002241296 and parameters: {'n_estimators': 250, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.13635199035045734.\n","[I 2025-03-29 16:51:43,570] Trial 6 finished with value: 0.14057075735391167 and parameters: {'n_estimators': 219, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.13635199035045734.\n","[I 2025-03-29 16:51:44,061] Trial 7 finished with value: 0.1403706396043868 and parameters: {'n_estimators': 123, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.13635199035045734.\n","[I 2025-03-29 16:51:44,551] Trial 8 finished with value: 0.16772786433251408 and parameters: {'n_estimators': 241, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.13635199035045734.\n","[I 2025-03-29 16:51:45,103] Trial 9 finished with value: 0.19637578759460775 and parameters: {'n_estimators': 284, 'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.13635199035045734.\n","[I 2025-03-29 16:51:45,814] Trial 10 finished with value: 0.13657540937195234 and parameters: {'n_estimators': 177, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.13635199035045734.\n","[I 2025-03-29 16:51:46,478] Trial 11 finished with value: 0.13659158371519062 and parameters: {'n_estimators': 175, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.13635199035045734.\n","[I 2025-03-29 16:51:47,539] Trial 12 finished with value: 0.13608974345074745 and parameters: {'n_estimators': 169, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 12 with value: 0.13608974345074745.\n","[I 2025-03-29 16:51:48,583] Trial 13 finished with value: 0.13628306845272467 and parameters: {'n_estimators': 123, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 12 with value: 0.13608974345074745.\n","[I 2025-03-29 16:51:49,546] Trial 14 finished with value: 0.1394518505163602 and parameters: {'n_estimators': 119, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 12 with value: 0.13608974345074745.\n","[I 2025-03-29 16:51:50,174] Trial 15 finished with value: 0.14098614914213056 and parameters: {'n_estimators': 117, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 12 with value: 0.13608974345074745.\n","[I 2025-03-29 16:51:51,219] Trial 16 finished with value: 0.14214416124170534 and parameters: {'n_estimators': 140, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 12 with value: 0.13608974345074745.\n","[I 2025-03-29 16:51:51,895] Trial 17 finished with value: 0.13949563518881308 and parameters: {'n_estimators': 151, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 12 with value: 0.13608974345074745.\n","[I 2025-03-29 16:51:52,184] Trial 18 finished with value: 0.14299473006525976 and parameters: {'n_estimators': 89, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 12 with value: 0.13608974345074745.\n","[I 2025-03-29 16:51:52,468] Trial 19 finished with value: 0.13675393536251243 and parameters: {'n_estimators': 51, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 12 with value: 0.13608974345074745.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Performance Metrics:\n","Train Metrics: MAE=0.0022, MSE=0.0000, RMSE=0.0030, R²=0.9999, MAPE=0.69%\n","Validation Metrics: MAE=0.1361, MSE=0.0242, RMSE=0.1555, R²=-3.1115, MAPE=7.62%\n","Test Metrics: MAE=0.4044, MSE=0.1695, RMSE=0.4118, R²=-27.0925, MAPE=19.94%\n","\n","Timing Information:\n","Total Train Time: 821.25 seconds\n","Total Validate Time: 0.07 seconds\n","Total Test Time: 0.08 seconds\n"]}]},{"cell_type":"markdown","source":["## BOHB"],"metadata":{"id":"JUfvNjYp1yGK"}},{"cell_type":"markdown","source":["to execute"],"metadata":{"id":"zIk-6vZSJ17q"}},{"cell_type":"code","source":["!pip install ConfigSpace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4mhW-KWeZV8","outputId":"fde7a57f-fe64-4ac3-e7b4-b987e2bbb3d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ConfigSpace\n","  Downloading configspace-1.2.1.tar.gz (130 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m122.9/131.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (2.0.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (3.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (1.14.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace) (10.6.0)\n","Building wheels for collected packages: ConfigSpace\n","  Building wheel for ConfigSpace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ConfigSpace: filename=configspace-1.2.1-py3-none-any.whl size=115990 sha256=de15b2c2d9de2805854d08389ca81da34bd24db6106c943f38527095ceb9d47b\n","  Stored in directory: /root/.cache/pip/wheels/11/0f/36/d5027c3eeb038827889830f7efbe6a1bad8956b3eb44ab2f44\n","Successfully built ConfigSpace\n","Installing collected packages: ConfigSpace\n","Successfully installed ConfigSpace-1.2.1\n"]}]},{"cell_type":"code","source":["!pip install hpbandster\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-wvIaQiefo9","outputId":"e2ad3c73-983d-45a4-f408-a277bc501c79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hpbandster\n","  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pyro4 (from hpbandster)\n","  Downloading Pyro4-4.82-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting serpent (from hpbandster)\n","  Downloading serpent-1.41-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: ConfigSpace in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.2.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (2.0.2)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from hpbandster) (0.14.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hpbandster) (1.14.1)\n","Collecting netifaces (from hpbandster)\n","  Downloading netifaces-0.11.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (3.2.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (4.12.2)\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from ConfigSpace->hpbandster) (10.6.0)\n","Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->hpbandster) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (1.17.0)\n","Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading serpent-1.41-py3-none-any.whl (9.6 kB)\n","Building wheels for collected packages: hpbandster, netifaces\n","  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=79986 sha256=e6351187361ee792877a944d81ee89fb2ce784d38e170beb106a2e9054a89740\n","  Stored in directory: /root/.cache/pip/wheels/fb/da/7d/af80a6b0a6898aaf2e1e93ab00cdf03251624e67f0641e9f0b\n","  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for netifaces: filename=netifaces-0.11.0-cp311-cp311-linux_x86_64.whl size=35184 sha256=21459e938e5641ccc6495353323d1183d2f5e113d43d4f42a94414fd75b5eff8\n","  Stored in directory: /root/.cache/pip/wheels/40/85/29/648c19bbbb5f1d30e33bfb343fd7fb54296b402f7205d8e46f\n","Successfully built hpbandster netifaces\n","Installing collected packages: netifaces, serpent, Pyro4, hpbandster\n","Successfully installed Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.41\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ConfigSpace as CS\n","import ConfigSpace.hyperparameters as CSH\n","import hpbandster.core.nameserver as hpns\n","from hpbandster.optimizers import BOHB\n","from hpbandster.core.worker import Worker\n","from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n","import time\n","\n","# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define Bi-GRU Model\n","class BiGRUModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(BiGRUModel, self).__init__()\n","        self.bigru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bi-directional outputs\n","\n","    def forward(self, x):\n","        out, _ = self.bigru(x)\n","        out = self.fc(out[:, -1, :])  # Use last hidden state\n","        return out\n","\n","# Function to Calculate Metrics\n","def calculate_metrics(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n","    return mae, mse, rmse, r2, mape\n","\n","# Convert datasets to PyTorch tensors and move to GPU\n","def to_torch_tensor(data):\n","    return torch.tensor(data.values, dtype=torch.float32).unsqueeze(1).to(device)\n","\n","Y_train_torch, Y_val_torch, Y_test_torch = map(to_torch_tensor, [Y_train, Y_val, Y_test])\n","X_train_torch, X_val_torch, X_test_torch = map(to_torch_tensor, [X_train, X_val, X_test])\n","\n","# Bi-GRU Configurations\n","bi_gru_layers = [2, 3, 5]\n","hidden_dim = 64\n","output_dim = 1\n","input_dim = X_train.shape[1]\n","\n","# Dictionary to Store Bi-GRU Feature Representations\n","bi_gru_features = []\n","\n","for num_layers in bi_gru_layers:\n","    print(f\"\\nTraining Bi-GRU with {num_layers} layers...\")\n","\n","    model = BiGRUModel(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    num_epochs = 30\n","\n","    # Training Loop\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train_torch)\n","        loss = criterion(outputs, Y_train_torch)\n","        loss.backward()\n","        optimizer.step()\n","    train_time = time.time() - start_time\n","\n","    # Extract Feature Representations\n","    model.eval()\n","    with torch.no_grad():\n","        val_start = time.time()\n","        train_features = model(X_train_torch).cpu().numpy()\n","        val_features = model(X_val_torch).cpu().numpy()\n","        val_time = time.time() - val_start\n","\n","        test_start = time.time()\n","        test_features = model(X_test_torch).cpu().numpy()\n","        test_time = time.time() - test_start\n","\n","    bi_gru_features.append((train_features, val_features, test_features, train_time, val_time, test_time))\n","\n","# Concatenate Features from All Layers\n","final_train_features = np.hstack([feat[0] for feat in bi_gru_features])\n","final_val_features = np.hstack([feat[1] for feat in bi_gru_features])\n","final_test_features = np.hstack([feat[2] for feat in bi_gru_features])\n","\n","# Total Time Records\n","total_train_time = sum([feat[3] for feat in bi_gru_features])\n","total_val_time = sum([feat[4] for feat in bi_gru_features])\n","total_test_time = sum([feat[5] for feat in bi_gru_features])\n","\n","print(f\"\\nTotal Train Time: {total_train_time:.2f}s | Val Time: {total_val_time:.2f}s | Test Time: {total_test_time:.2f}s\")\n","\n","# Train ExtraTrees\n","et_model = ExtraTreesRegressor(n_estimators=100, max_depth=6, random_state=42)\n","et_model.fit(final_train_features, Y_train)\n","\n","# Predictions\n","Y_train_pred = et_model.predict(final_train_features)\n","Y_val_pred = et_model.predict(final_val_features)\n","Y_test_pred = et_model.predict(final_test_features)\n","\n","# Calculate Metrics\n","train_metrics = calculate_metrics(Y_train, Y_train_pred)\n","val_metrics = calculate_metrics(Y_val, Y_val_pred)\n","test_metrics = calculate_metrics(Y_test, Y_test_pred)\n","\n","# Print Results\n","print(\"\\nTrain Metrics:\", train_metrics, \"| Time:\", total_train_time)\n","print(\"Validation Metrics:\", val_metrics, \"| Time:\", total_val_time)\n","print(\"Test Metrics:\", test_metrics, \"| Time:\", total_test_time)\n"],"metadata":{"id":"p2fWn94jJ0Hv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0fd6acfe-0cc6-4878-c45f-dd1f476e75ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","\n","Training Bi-GRU with 2 layers...\n","\n","Training Bi-GRU with 3 layers...\n","\n","Training Bi-GRU with 5 layers...\n","\n","Total Train Time: 29.80s | Val Time: 0.57s | Test Time: 0.09s\n","\n","Train Metrics: (0.0037038780205375437, 2.596549963364185e-05, np.float64(0.005095635351321938), 0.9998516435244971, 1.354387342403099) | Time: 29.795137643814087\n","Validation Metrics: (0.15348086347050036, 0.029354145801182174, np.float64(0.17133051625785226), -3.990891612512934, 8.611081842636793) | Time: 0.5749325752258301\n","Test Metrics: (0.42223850900387505, 0.18432042887681355, np.float64(0.42932555115764254), -29.541554105441218, 20.828162064014744) | Time: 0.08554744720458984\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ivjWzWt0e6Pu"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1eUcgqDssPxe_Bg-Ta4o-kee69JpLtuXw","timestamp":1743329957394}],"collapsed_sections":["CnsEk1fS0iyV","Vcl4PSQ90b0t","CNituZbAbpCR","6Z1_uKYO5JWl","amY77qVbMX8C","GRzBkYFRb_qc","6sIgbTnQs-Xq","vmHN3eRMtAMh","bQnXlX-Lxi4o","JUfvNjYp1yGK"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}